{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb2b1ee-d39a-45e9-a16a-c3505a92e870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 23:54:40,241 - INFO - Transformers library available for BERT embeddings\n",
      "2025-04-07 23:54:40,243 - WARNING - pyahocorasick not available. Using fallback implementation.\n",
      "2025-04-07 23:54:40,836 - INFO - Using device: cpu\n",
      "2025-04-07 23:54:40,842 - INFO - All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1 Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import distance metrics\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from Levenshtein import jaro_winkler, ratio as levenshtein_ratio\n",
    "import textdistance\n",
    "from fuzzywuzzy import fuzz\n",
    "import jellyfish\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Try importing transformers with graceful fallback\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    transformers_available = True\n",
    "    logger.info(\"Transformers library available for BERT embeddings\")\n",
    "except ImportError:\n",
    "    transformers_available = False\n",
    "    logger.warning(\"Transformers library not available. Will use TF-IDF fallback.\")\n",
    "\n",
    "# Try importing pyahocorasick with fallback\n",
    "try:\n",
    "    import pyahocorasick\n",
    "    aho_corasick_available = True\n",
    "    logger.info(\"pyahocorasick is available\")\n",
    "except ImportError:\n",
    "    logger.warning(\"pyahocorasick not available. Using fallback implementation.\")\n",
    "    aho_corasick_available = False\n",
    "\n",
    "# Import visualization libraries if available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    visualization_available = True\n",
    "except ImportError:\n",
    "    visualization_available = False\n",
    "    logger.warning(\"Visualization libraries not available. Plots will be skipped.\")\n",
    "\n",
    "# Set up device for pytorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants for the algorithm\n",
    "SEMANTIC_SIMILARITY_THRESHOLD = 0.85  # Threshold for considering semantic similarity high\n",
    "STRING_SIMILARITY_THRESHOLD = 0.80    # Threshold for string similarity\n",
    "ACRONYM_SIMILARITY_THRESHOLD = 0.75   # Threshold for acronym formation similarity\n",
    "DEFAULT_BATCH_SIZE = 64               # Default batch size for processing\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "logger.info(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84de7309-35a6-4fe3-a568-5d8ff3178d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 23:54:45,331 - INFO - Loading advanced BERT model 'sentence-transformers/all-mpnet-base-v2'...\n",
      "2025-04-07 23:54:45,332 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-04-07 23:54:46,443 - INFO - Use pytorch device: cpu\n",
      "2025-04-07 23:54:46,783 - INFO - Advanced BERT model loaded successfully on cpu\n",
      "2025-04-07 23:54:46,785 - INFO - Advanced BERT embedder initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Advanced BERT Embedder\n",
    "\n",
    "class AdvancedBERTEmbedder:\n",
    "    \"\"\"\n",
    "    Advanced BERT embedder using state-of-the-art models with improved pooling strategies,\n",
    "    caching, and domain adaptation specifically optimized for merchant name matching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=None, pooling_strategy='mean', cache_size=5000, device=None):\n",
    "        \"\"\"\n",
    "        Initialize advanced BERT embedder with specified model and pooling strategy.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained model to use\n",
    "            pooling_strategy (str): Pooling strategy ('mean', 'cls', 'max', or 'attention')\n",
    "            cache_size (int): Size of the LRU cache for embeddings\n",
    "            device: Device to run the model on (cuda or cpu)\n",
    "        \"\"\"\n",
    "        # Select the best model based on availability and performance\n",
    "        self.model_candidates = [\n",
    "            'sentence-transformers/all-mpnet-base-v2',         # Best performance but slower\n",
    "            'sentence-transformers/all-distilroberta-v1',      # Good balance of performance/speed\n",
    "            'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',  # Good for international merchants\n",
    "            'sentence-transformers/all-MiniLM-L12-v2'          # Fast but less accurate\n",
    "        ]\n",
    "        \n",
    "        # If no model specified, use the first available one\n",
    "        if model_name is None:\n",
    "            self.model_name = self.model_candidates[0]\n",
    "        else:\n",
    "            self.model_name = model_name\n",
    "            \n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.max_sequence_length = 512  # BERT's limit\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.initialized = False\n",
    "        self.domain_adapted = False\n",
    "        self.domain_data = None\n",
    "        \n",
    "        # Initialize pre-trained model if transformers available\n",
    "        if transformers_available:\n",
    "            try:\n",
    "                logger.info(f\"Loading advanced BERT model '{self.model_name}'...\")\n",
    "                \n",
    "                # Use SentenceTransformer for better performance\n",
    "                self.model = SentenceTransformer(self.model_name).to(self.device)\n",
    "                \n",
    "                # Also initialize a tokenizer for fine-tuning\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                \n",
    "                self.initialized = True\n",
    "                logger.info(f\"Advanced BERT model loaded successfully on {self.device}\")\n",
    "                \n",
    "                # Set up LRU cache for embeddings\n",
    "                self.encode = lru_cache(maxsize=self.cache_size)(self._encode_uncached)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error initializing BERT model: {e}\")\n",
    "                self.initialized = False\n",
    "        \n",
    "        # Initialize TF-IDF fallback with improved settings\n",
    "        if not self.initialized:\n",
    "            # Using character n-grams for better handling of typos and abbreviations\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(\n",
    "                analyzer='char_wb', \n",
    "                ngram_range=(2, 5),  # Increased n-gram range\n",
    "                min_df=2,            # Ignore very rare n-grams\n",
    "                max_df=0.95          # Ignore very common n-grams\n",
    "            )\n",
    "            self.tfidf_fitted = False\n",
    "            logger.info(\"Using enhanced TF-IDF fallback for embeddings\")\n",
    "    \n",
    "    def adapt_to_domain(self, examples_df, epochs=5, learning_rate=2e-5):\n",
    "        \"\"\"\n",
    "        Perform extensive domain adaptation to improve merchant name understanding.\n",
    "        \n",
    "        Args:\n",
    "            examples_df (DataFrame): DataFrame with matched merchant names\n",
    "            epochs (int): Number of adaptation epochs\n",
    "            learning_rate (float): Learning rate for adaptation\n",
    "        \"\"\"\n",
    "        if not self.initialized or not transformers_available:\n",
    "            logger.warning(\"Domain adaptation skipped - model not initialized with transformers\")\n",
    "            return\n",
    "        \n",
    "        if self.domain_adapted and self.domain_data is not None:\n",
    "            logger.info(\"Model already adapted to domain. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # Store domain data for potential reuse\n",
    "        self.domain_data = examples_df.copy()\n",
    "        \n",
    "        # Extract positive pairs (matching merchant names)\n",
    "        positive_pairs = []\n",
    "        \n",
    "        # Check different column combinations to find matched pairs\n",
    "        if 'Enhanced_Score' in examples_df.columns:\n",
    "            for _, row in examples_df.iterrows():\n",
    "                if row['Enhanced_Score'] >= 0.7:  # Reduced threshold to get more examples\n",
    "                    positive_pairs.append((row['Acronym'], row['Full_Name']))\n",
    "        elif 'Expected_Match' in examples_df.columns:\n",
    "            for _, row in examples_df.iterrows():\n",
    "                if row['Expected_Match']:\n",
    "                    positive_pairs.append((row['Acronym'], row['Full_Name']))\n",
    "        else:\n",
    "            # Just use all pairs as examples if no scoring columns exist\n",
    "            for _, row in examples_df.iterrows():\n",
    "                positive_pairs.append((row['Acronym'], row['Full_Name']))\n",
    "        \n",
    "        # Skip if not enough examples\n",
    "        if len(positive_pairs) < 5:\n",
    "            logger.warning(\"Not enough high-quality examples for adaptation\")\n",
    "            return\n",
    "        \n",
    "        # Create negative pairs for contrastive learning\n",
    "        # (randomly pair non-matching merchant names)\n",
    "        all_names = examples_df['Acronym'].tolist() + examples_df['Full_Name'].tolist()\n",
    "        negative_pairs = []\n",
    "        \n",
    "        for i in range(min(len(positive_pairs), 100)):  # Limit to 100 negative pairs\n",
    "            while True:\n",
    "                name1 = np.random.choice(all_names)\n",
    "                name2 = np.random.choice(all_names)\n",
    "                pair = (name1, name2)\n",
    "                # Check that this isn't actually a positive pair\n",
    "                if pair not in positive_pairs and (name2, name1) not in positive_pairs and name1 != name2:\n",
    "                    negative_pairs.append(pair)\n",
    "                    break\n",
    "        \n",
    "        # Implement enhanced adaptation with contrastive learning\n",
    "        logger.info(f\"Starting domain adaptation with {len(positive_pairs)} positive and {len(negative_pairs)} negative pairs...\")\n",
    "        \n",
    "        # Create a fine-tunable version of the model\n",
    "        if hasattr(self.model, 'auto_model'):\n",
    "            # For SentenceTransformer models\n",
    "            model_for_tuning = self.model.auto_model\n",
    "        else:\n",
    "            # Fallback to directly using the model\n",
    "            model_for_tuning = self.model\n",
    "        \n",
    "        model_for_tuning.train()\n",
    "        optimizer = torch.optim.AdamW(model_for_tuning.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        \n",
    "        batch_size = 8  # Small batch size for fine-tuning\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            # Process positive pairs (push closer together)\n",
    "            for i in range(0, len(positive_pairs), batch_size):\n",
    "                batch_pairs = positive_pairs[i:i+batch_size]\n",
    "                \n",
    "                # Prepare anchor and positive texts\n",
    "                anchors = [pair[0] for pair in batch_pairs]\n",
    "                positives = [pair[1] for pair in batch_pairs]\n",
    "                \n",
    "                # Get embeddings using the model\n",
    "                if hasattr(self.model, 'encode'):\n",
    "                    # For SentenceTransformer models\n",
    "                    anchor_embeddings = self.model.encode(anchors, convert_to_tensor=True)\n",
    "                    positive_embeddings = self.model.encode(positives, convert_to_tensor=True)\n",
    "                else:\n",
    "                    # If using raw transformer model\n",
    "                    # Tokenize\n",
    "                    anchor_inputs = self.tokenizer(anchors, return_tensors='pt', padding=True, \n",
    "                                            truncation=True, max_length=self.max_sequence_length).to(self.device)\n",
    "                    positive_inputs = self.tokenizer(positives, return_tensors='pt', padding=True, \n",
    "                                            truncation=True, max_length=self.max_sequence_length).to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    with torch.no_grad():\n",
    "                        anchor_outputs = model_for_tuning(**anchor_inputs)\n",
    "                        positive_outputs = model_for_tuning(**positive_inputs)\n",
    "                    \n",
    "                    # Get embeddings (CLS token)\n",
    "                    anchor_embeddings = anchor_outputs.last_hidden_state[:, 0, :]\n",
    "                    positive_embeddings = positive_outputs.last_hidden_state[:, 0, :]\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                anchor_embeddings = F.normalize(anchor_embeddings, p=2, dim=1)\n",
    "                positive_embeddings = F.normalize(positive_embeddings, p=2, dim=1)\n",
    "                \n",
    "                # Contrastive loss (push matching names closer)\n",
    "                similarity = F.cosine_similarity(anchor_embeddings, positive_embeddings)\n",
    "                positive_loss = (1.0 - similarity).mean()\n",
    "                \n",
    "                # Process negative pairs if available (push further apart)\n",
    "                negative_loss = 0\n",
    "                if negative_pairs:\n",
    "                    j = i % max(1, len(negative_pairs) - batch_size)\n",
    "                    neg_batch_pairs = negative_pairs[j:j+batch_size]\n",
    "                    \n",
    "                    neg_anchors = [pair[0] for pair in neg_batch_pairs]\n",
    "                    neg_samples = [pair[1] for pair in neg_batch_pairs]\n",
    "                    \n",
    "                    # Get embeddings\n",
    "                    if hasattr(self.model, 'encode'):\n",
    "                        neg_anchor_embeddings = self.model.encode(neg_anchors, convert_to_tensor=True)\n",
    "                        neg_sample_embeddings = self.model.encode(neg_samples, convert_to_tensor=True)\n",
    "                    else:\n",
    "                        # Tokenize\n",
    "                        neg_anchor_inputs = self.tokenizer(neg_anchors, return_tensors='pt', padding=True, \n",
    "                                                truncation=True, max_length=self.max_sequence_length).to(self.device)\n",
    "                        neg_sample_inputs = self.tokenizer(neg_samples, return_tensors='pt', padding=True, \n",
    "                                                truncation=True, max_length=self.max_sequence_length).to(self.device)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        with torch.no_grad():\n",
    "                            neg_anchor_outputs = model_for_tuning(**neg_anchor_inputs)\n",
    "                            neg_sample_outputs = model_for_tuning(**neg_sample_inputs)\n",
    "                        \n",
    "                        # Get embeddings\n",
    "                        neg_anchor_embeddings = neg_anchor_outputs.last_hidden_state[:, 0, :]\n",
    "                        neg_sample_embeddings = neg_sample_outputs.last_hidden_state[:, 0, :]\n",
    "                    \n",
    "                    # Normalize embeddings\n",
    "                    neg_anchor_embeddings = F.normalize(neg_anchor_embeddings, p=2, dim=1)\n",
    "                    neg_sample_embeddings = F.normalize(neg_sample_embeddings, p=2, dim=1)\n",
    "                    \n",
    "                    # Push negative pairs apart (with margin)\n",
    "                    neg_similarity = F.cosine_similarity(neg_anchor_embeddings, neg_sample_embeddings)\n",
    "                    negative_loss = F.relu(neg_similarity - 0.3).mean()  # Push apart with margin of 0.3\n",
    "                \n",
    "                # Combined loss\n",
    "                loss = positive_loss + negative_loss\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model_for_tuning.parameters(), 1.0)  # Gradient clipping\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Calculate average loss\n",
    "            avg_loss = total_loss / (len(positive_pairs) // batch_size + 1)\n",
    "            logger.info(f\"  Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Reset model to evaluation mode\n",
    "        model_for_tuning.eval()\n",
    "        \n",
    "        # Update internal state\n",
    "        self.domain_adapted = True\n",
    "        \n",
    "        # Clear the embedding cache after adaptation\n",
    "        if hasattr(self, 'encode') and hasattr(self.encode, 'cache_clear'):\n",
    "            self.encode.cache_clear()\n",
    "            \n",
    "        logger.info(f\"Domain adaptation completed successfully\")\n",
    "    \n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Mean pooling - take average of all token embeddings\"\"\"\n",
    "        token_embeddings = model_output[0]  # First element contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def _cls_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"CLS pooling - use the [CLS] token embedding\"\"\"\n",
    "        return model_output[0][:, 0]\n",
    "    \n",
    "    def _max_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Max pooling - take max of all token embeddings\"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "        return torch.max(token_embeddings, 1)[0]\n",
    "    \n",
    "    def _attention_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Attention pooling - use attention weights to create weighted average\"\"\"\n",
    "        token_embeddings = model_output[0]  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Create a simple attention mechanism\n",
    "        attention_scores = torch.matmul(\n",
    "            token_embeddings, \n",
    "            token_embeddings.mean(dim=1).unsqueeze(-1)\n",
    "        ).squeeze(-1)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Apply mask\n",
    "        attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to token embeddings\n",
    "        weighted_sum = torch.bmm(\n",
    "            attention_weights.unsqueeze(1), \n",
    "            token_embeddings\n",
    "        ).squeeze(1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        return weighted_sum\n",
    "    \n",
    "    def _get_pooled_embeddings(self, model_output, attention_mask):\n",
    "        \"\"\"Apply the selected pooling strategy\"\"\"\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            return self._mean_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'cls':\n",
    "            return self._cls_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            return self._max_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'attention':\n",
    "            return self._attention_pooling(model_output, attention_mask)\n",
    "        else:\n",
    "            # Default to mean pooling\n",
    "            return self._mean_pooling(model_output, attention_mask)\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Fit the TF-IDF vectorizer on a corpus of texts (only needed for TF-IDF fallback)\"\"\"\n",
    "        if not self.initialized:\n",
    "            # Fit TF-IDF vectorizer\n",
    "            self.tfidf_vectorizer.fit(texts)\n",
    "            self.tfidf_fitted = True\n",
    "            logger.info(\"TF-IDF vectorizer fitted on corpus\")\n",
    "    \n",
    "    def _encode_uncached(self, texts, batch_size=32, show_progress=False):\n",
    "        \"\"\"\n",
    "        Internal method to encode texts into embeddings without caching.\n",
    "        This is wrapped by the LRU cache decorator in the initializer.\n",
    "        \"\"\"\n",
    "        # Handle single text input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Return empty array for empty input\n",
    "        if len(texts) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Use pre-trained BERT if available\n",
    "        if self.initialized:\n",
    "            try:\n",
    "                # Use SentenceTransformer's encode method directly if available\n",
    "                if hasattr(self.model, 'encode'):\n",
    "                    # Faster encoding with batching\n",
    "                    embeddings = self.model.encode(\n",
    "                        texts, \n",
    "                        batch_size=batch_size, \n",
    "                        show_progress_bar=show_progress,\n",
    "                        convert_to_numpy=True\n",
    "                    )\n",
    "                    return embeddings\n",
    "                else:\n",
    "                    # Manual batching with raw transformer model\n",
    "                    all_embeddings = []\n",
    "                    \n",
    "                    for i in range(0, len(texts), batch_size):\n",
    "                        if show_progress and i % (batch_size * 10) == 0:\n",
    "                            logger.info(f\"Processing batch {i//batch_size + 1}/{(len(texts)//batch_size) + 1}\")\n",
    "                        \n",
    "                        batch_texts = texts[i:i+batch_size]\n",
    "                        \n",
    "                        # Tokenize\n",
    "                        encoded_input = self.tokenizer(\n",
    "                            batch_texts, \n",
    "                            padding=True, \n",
    "                            truncation=True, \n",
    "                            max_length=self.max_sequence_length,\n",
    "                            return_tensors='pt'\n",
    "                        ).to(self.device)\n",
    "                        \n",
    "                        # Compute token embeddings\n",
    "                        with torch.no_grad():\n",
    "                            model_output = self.model(**encoded_input)\n",
    "                            batch_embeddings = self._get_pooled_embeddings(model_output, encoded_input['attention_mask'])\n",
    "                            all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "                    \n",
    "                    return np.vstack(all_embeddings)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in BERT encoding: {e}\")\n",
    "                logger.info(\"Falling back to TF-IDF encoding\")\n",
    "                # Fall back to TF-IDF\n",
    "                self.initialized = False\n",
    "        \n",
    "        # Use TF-IDF fallback\n",
    "        if not self.tfidf_fitted:\n",
    "            self.fit(texts)\n",
    "        \n",
    "        return self.tfidf_vectorizer.transform(texts).toarray()\n",
    "    \n",
    "    def compute_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts using the pre-trained model\n",
    "        \n",
    "        Args:\n",
    "            text1: First text\n",
    "            text2: Second text\n",
    "            \n",
    "        Returns:\n",
    "            float: Cosine similarity score\n",
    "        \"\"\"\n",
    "        # Empty text check\n",
    "        if not text1 or not text2:\n",
    "            return 0.0\n",
    "            \n",
    "        # Get embeddings for both texts\n",
    "        emb1 = self.encode([text1])[0]\n",
    "        emb2 = self.encode([text2])[0]\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        norm1 = np.linalg.norm(emb1)\n",
    "        norm2 = np.linalg.norm(emb2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        return np.dot(emb1, emb2) / (norm1 * norm2)\n",
    "\n",
    "# Initialize advanced BERT embedder with state-of-the-art model\n",
    "bert_embedder = AdvancedBERTEmbedder(\n",
    "    model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "    pooling_strategy='mean',\n",
    "    cache_size=10000,  # Increased cache size\n",
    "    device=device\n",
    ")\n",
    "logger.info(\"Advanced BERT embedder initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ffe90ed-0194-485c-a1ea-8a10c8acedd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 00:24:27,121 - INFO - Enhanced merchant preprocessor initialized!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Merchant Name Preprocessing\n",
    "class MerchantPreprocessor:\n",
    "    \"\"\"\n",
    "    Enhanced merchant name preprocessor with industry-specific pattern handling \n",
    "    and extensive abbreviation dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize merchant preprocessor with dictionaries and patterns\"\"\"\n",
    "        # Load comprehensive abbreviation dictionaries\n",
    "        self.abbreviations = self._load_abbreviation_dictionary()\n",
    "        self.domain_abbreviations = self._load_domain_abbreviations()\n",
    "        self.stopwords = self._load_stopwords()\n",
    "        self.domain_stopwords = self._load_domain_stopwords()\n",
    "        \n",
    "        # Compile common business name patterns\n",
    "        self.business_suffixes = self._compile_business_suffixes()\n",
    "        self.business_patterns = self._compile_business_patterns()\n",
    "    \n",
    "    def _load_abbreviation_dictionary(self):\n",
    "        \"\"\"Load comprehensive abbreviation dictionary with merchant-specific terms\"\"\"\n",
    "        return {\n",
    "            # Banking & Financial Institutions (expanded)\n",
    "            'bofa': 'bank of america', 'b of a': 'bank of america',\n",
    "            'boa': 'bank of america', 'bac': 'bank of america',\n",
    "            'jpm': 'jpmorgan chase', 'jpm chase': 'jpmorgan chase',\n",
    "            'jpmc': 'jpmorgan chase', 'chase': 'jpmorgan chase',\n",
    "            'wf': 'wells fargo', 'wfb': 'wells fargo bank',\n",
    "            'citi': 'citibank', 'citi bank': 'citibank',\n",
    "            'gs': 'goldman sachs', 'ms': 'morgan stanley',\n",
    "            'db': 'deutsche bank', 'hsbc': 'hongkong and shanghai banking corporation',\n",
    "            'amex': 'american express', 'amx': 'american express',\n",
    "            'usb': 'us bank', 'rbc': 'royal bank of canada',\n",
    "            'pnc': 'pnc financial services', 'td': 'toronto dominion bank',\n",
    "            'bny': 'bank of new york', 'bnyc': 'bank of new york mellon',\n",
    "            'cba': 'commonwealth bank of australia', 'nab': 'national australia bank',\n",
    "            'rba': 'reserve bank of australia', 'westpac': 'western pacific bank',\n",
    "            'fargo': 'wells fargo', 'chase bank': 'jpmorgan chase',\n",
    "            'usaa': 'united services automobile association',\n",
    "            'disc': 'discover', 'discover card': 'discover',\n",
    "            'cap1': 'capital one', 'cap one': 'capital one',\n",
    "            'citi': 'citibank', 'fidelity': 'fidelity investments',\n",
    "            'schwab': 'charles schwab', 'etrade': 'e-trade financial',\n",
    "            \n",
    "            # Fast Food & Restaurant Chains (expanded)\n",
    "            'mcd': 'mcdonalds', 'mcds': 'mcdonalds', 'md': 'mcdonalds',\n",
    "            'mickey ds': 'mcdonalds', 'maccas': 'mcdonalds',\n",
    "            'bk': 'burger king', 'kfc': 'kentucky fried chicken',\n",
    "            'sbux': 'starbucks', 'sb': 'starbucks', 'starbks': 'starbucks',\n",
    "            'tb': 'taco bell', 'wen': 'wendys', 'wendys': 'wendys restaurant',\n",
    "            'dq': 'dairy queen', 'ph': 'pizza hut', 'pzh': 'pizza hut',\n",
    "            'dnkn': 'dunkin donuts', 'cfa': 'chick fil a',\n",
    "            'cmg': 'chipotle mexican grill', 'chipotle': 'chipotle mexican grill',\n",
    "            'ihop': 'international house of pancakes',\n",
    "            'tgi': 'tgi fridays', 'tgif': 'tgi fridays',\n",
    "            'bww': 'buffalo wild wings', 'pjs': 'papa johns',\n",
    "            'ljs': 'long john silvers', 'popeyes': 'popeyes louisiana kitchen',\n",
    "            'subway': 'subway restaurant', 'dominos': 'dominos pizza',\n",
    "            'panera': 'panera bread', 'five guys': 'five guys burgers and fries',\n",
    "            'applebees': 'applebees neighborhood grill',\n",
    "            'arbys': 'arbys restaurant', 'hooters': 'hooters restaurant',\n",
    "            'cracker barrel': 'cracker barrel old country store',\n",
    "            \n",
    "            # Retail & Supermarkets (expanded)\n",
    "            'wmt': 'walmart', 'wal mart': 'walmart', 'walm': 'walmart',\n",
    "            'tgt': 'target', 'targ': 'target', 'tarjay': 'target',\n",
    "            'costco': 'costco wholesale', 'cost': 'costco',\n",
    "            'kroger': 'the kroger co', 'krg': 'kroger',\n",
    "            'cvs': 'cvs pharmacy', 'cvs rx': 'cvs pharmacy',\n",
    "            'wag': 'walgreens', 'walg': 'walgreens',\n",
    "            'hd': 'home depot', 'thd': 'the home depot',\n",
    "            'low': 'lowes', 'lowes': 'lowes home improvement',\n",
    "            'bby': 'best buy', 'bb': 'best buy',\n",
    "            'amzn': 'amazon', 'amazon': 'amazon.com',\n",
    "            '711': '7-eleven', '7-11': '7-eleven',\n",
    "            'tjx': 'tj maxx', 'tjmaxx': 'tj maxx',\n",
    "            'wfm': 'whole foods market', 'whole foods': 'whole foods market',\n",
    "            'sams': 'sams club', 'sams club': 'sams club',\n",
    "            'macys': 'macys department store', 'nordstrom': 'nordstrom',\n",
    "            'kohls': 'kohls department store', 'publix': 'publix super markets',\n",
    "            'safeway': 'safeway supermarket', 'albertsons': 'albertsons supermarket',\n",
    "            'aldi': 'aldi supermarket', 'lidl': 'lidl supermarket',\n",
    "            'trader joes': 'trader joes', 'tj': 'trader joes',\n",
    "            \n",
    "            # Tech Companies (expanded)\n",
    "            'msft': 'microsoft', 'ms': 'microsoft', 'aapl': 'apple',\n",
    "            'goog': 'google', 'googl': 'google', 'alphabet': 'google',\n",
    "            'amzn': 'amazon', 'fb': 'facebook', 'meta': 'meta platforms',\n",
    "            'nflx': 'netflix', 'tsla': 'tesla motors', 'tsla': 'tesla',\n",
    "            'ibm': 'international business machines', 'csco': 'cisco systems',\n",
    "            'orcl': 'oracle', 'intc': 'intel', 'amd': 'advanced micro devices',\n",
    "            'nvda': 'nvidia', 'adbe': 'adobe', 'crm': 'salesforce',\n",
    "            'dell': 'dell technologies', 'hpe': 'hewlett packard enterprise',\n",
    "            'hp': 'hewlett packard', 'vmw': 'vmware', 'twtr': 'twitter',\n",
    "            'ebay': 'ebay', 'pypl': 'paypal', 'sqsp': 'squarespace',\n",
    "            'lyft': 'lyft', 'uber': 'uber technologies',\n",
    "            \n",
    "            # Common address components (expanded)\n",
    "            'rd': 'road', 'st': 'street', 'ave': 'avenue', \n",
    "            'blvd': 'boulevard', 'ctr': 'center', 'ln': 'lane', \n",
    "            'dr': 'drive', 'pl': 'place', 'ct': 'court',\n",
    "            'hwy': 'highway', 'pkwy': 'parkway', 'sq': 'square',\n",
    "            'cir': 'circle', 'ter': 'terrace', 'expy': 'expressway',\n",
    "            'fwy': 'freeway', 'tpke': 'turnpike', 'crk': 'creek',\n",
    "            'hvn': 'haven', 'xing': 'crossing', 'vlg': 'village',\n",
    "            'spgs': 'springs', 'mtn': 'mountain', 'lk': 'lake',\n",
    "            'n': 'north', 's': 'south', 'e': 'east', 'w': 'west',\n",
    "            'ne': 'northeast', 'nw': 'northwest', 'se': 'southeast',\n",
    "            'sw': 'southwest', 'apt': 'apartment', 'ste': 'suite',\n",
    "            'bldg': 'building', 'fl': 'floor', 'rm': 'room',\n",
    "            \n",
    "            # Expanded location-based prefixes\n",
    "            'norcal': 'northern california', 'socal': 'southern california',\n",
    "            'nyc': 'new york city', 'la': 'los angeles', 'sf': 'san francisco',\n",
    "            'chi': 'chicago', 'atl': 'atlanta', 'hou': 'houston',\n",
    "            'dfw': 'dallas fort worth', 'mia': 'miami', 'bos': 'boston',\n",
    "            'dc': 'washington dc', 'sea': 'seattle', 'den': 'denver',\n",
    "            'phx': 'phoenix', 'pdx': 'portland', 'msp': 'minneapolis saint paul',\n",
    "            'stl': 'saint louis', 'pit': 'pittsburgh', 'cin': 'cincinnati',\n",
    "            'cle': 'cleveland', 'slc': 'salt lake city', 'nola': 'new orleans',\n",
    "            'kcmo': 'kansas city', 'nash': 'nashville', 'ind': 'indianapolis',\n",
    "            \n",
    "            # Gas stations and convenience stores\n",
    "            'bp': 'british petroleum', 'chevron': 'chevron gas station',\n",
    "            'shell': 'shell gas station', 'exxon': 'exxon gas station',\n",
    "            'mobil': 'mobil gas station', 'texaco': 'texaco gas station',\n",
    "            'conoco': 'conoco gas station', 'marathon': 'marathon gas station',\n",
    "            'sunoco': 'sunoco gas station', 'valero': 'valero gas station',\n",
    "            'speedway': 'speedway gas station', 'circle k': 'circle k convenience store',\n",
    "            'quiktrip': 'quiktrip convenience store', 'qt': 'quiktrip',\n",
    "            'cumbys': 'cumberland farms', 'wawa': 'wawa convenience store',\n",
    "            'sheetz': 'sheetz convenience store', 'racetrac': 'racetrac gas station',\n",
    "            'caseys': 'caseys general store', 'murphy': 'murphy usa gas station',\n",
    "            \n",
    "            # Airlines\n",
    "            'aa': 'american airlines', 'aal': 'american airlines', \n",
    "            'dal': 'delta airlines', 'dl': 'delta air lines',\n",
    "            'ual': 'united airlines', 'ua': 'united airlines',\n",
    "            'luv': 'southwest airlines', 'wn': 'southwest airlines',\n",
    "            'jblu': 'jetblue airways', 'b6': 'jetblue airways',\n",
    "            'alk': 'alaska airlines', 'as': 'alaska airlines',\n",
    "            'save': 'spirit airlines', 'nk': 'spirit airlines',\n",
    "            'ulcc': 'frontier airlines', 'f9': 'frontier airlines',\n",
    "            \n",
    "            # Hospitality\n",
    "            'mar': 'marriott', 'hilton': 'hilton hotels',\n",
    "            'hyatt': 'hyatt hotels', 'wyndham': 'wyndham hotels',\n",
    "            'ihg': 'intercontinental hotels group', 'choice': 'choice hotels',\n",
    "            'bwi': 'best western', 'accor': 'accor hotels',\n",
    "            'mhgc': 'mgm resorts', 'mgm': 'mgm resorts',\n",
    "            'lvs': 'las vegas sands', 'wynn': 'wynn resorts',\n",
    "            'h': 'hyatt', 'hot': 'starwood hotels'\n",
    "        }\n",
    "    \n",
    "    def _load_domain_abbreviations(self):\n",
    "        \"\"\"Load domain-specific abbreviation dictionaries\"\"\"\n",
    "        return {\n",
    "            'Medical': {\n",
    "                'dr': 'doctor', 'hosp': 'hospital', 'med': 'medical',\n",
    "                'clin': 'clinic', 'pharm': 'pharmacy', 'lab': 'laboratory',\n",
    "                'dept': 'department', 'ctr': 'center', 'inst': 'institute',\n",
    "                'er': 'emergency room', 'icu': 'intensive care unit',\n",
    "                'ob': 'obstetrics', 'gyn': 'gynecology', 'peds': 'pediatrics',\n",
    "                'ortho': 'orthopedics', 'onc': 'oncology', 'neuro': 'neurology',\n",
    "                'radiol': 'radiology', 'card': 'cardiology', 'derm': 'dermatology',\n",
    "                'ent': 'ear nose throat', 'opht': 'ophthalmology', 'uro': 'urology',\n",
    "                'gastro': 'gastroenterology', 'hem': 'hematology', 'oncol': 'oncology',\n",
    "                'rheum': 'rheumatology', 'endo': 'endocrinology', 'pulm': 'pulmonology',\n",
    "                'rx': 'prescription', 'surg': 'surgical', 'rehab': 'rehabilitation',\n",
    "                'path': 'pathology', 'anesth': 'anesthesiology', 'psych': 'psychiatry'\n",
    "            },\n",
    "            'Government': {\n",
    "                'govt': 'government', 'dept': 'department', 'admin': 'administration',\n",
    "                'auth': 'authority', 'fed': 'federal', 'natl': 'national',\n",
    "                'comm': 'commission', 'sec': 'secretary', 'org': 'organization',\n",
    "                'div': 'division', 'bur': 'bureau', 'off': 'office',\n",
    "                'min': 'ministry', 'reg': 'regional', 'dist': 'district',\n",
    "                'cncl': 'council', 'cmte': 'committee', 'subcmte': 'subcommittee',\n",
    "                'doj': 'department of justice', 'dod': 'department of defense',\n",
    "                'dos': 'department of state', 'dot': 'department of transportation',\n",
    "                'doe': 'department of energy', 'dol': 'department of labor',\n",
    "                'dhs': 'department of homeland security', 'epa': 'environmental protection agency',\n",
    "                'fbi': 'federal bureau of investigation', 'cia': 'central intelligence agency',\n",
    "                'irs': 'internal revenue service', 'usps': 'united states postal service',\n",
    "                'sba': 'small business administration', 'va': 'veterans affairs',\n",
    "                'omb': 'office of management and budget', 'gao': 'government accountability office'\n",
    "            },\n",
    "            'Education': {\n",
    "                'univ': 'university', 'coll': 'college', 'acad': 'academy',\n",
    "                'elem': 'elementary', 'sch': 'school', 'inst': 'institute',\n",
    "                'dept': 'department', 'lib': 'library', 'lab': 'laboratory',\n",
    "                'fac': 'faculty', 'prof': 'professor', 'assoc': 'associate',\n",
    "                'asst': 'assistant', 'adm': 'administration', 'stdnt': 'student',\n",
    "                'grad': 'graduate', 'undergrad': 'undergraduate', 'alum': 'alumni',\n",
    "                'edu': 'education', 'hs': 'high school', 'ms': 'middle school',\n",
    "                'jr': 'junior', 'sr': 'senior', 'tech': 'technical',\n",
    "                'comm': 'community', 'uni': 'university', 'poly': 'polytechnic',\n",
    "                'prsch': 'preschool', 'k12': 'kindergarten through 12th grade'\n",
    "            },\n",
    "            'Financial': {\n",
    "                'fin': 'financial', 'svcs': 'services', 'mgmt': 'management',\n",
    "                'assoc': 'associates', 'intl': 'international', 'grp': 'group',\n",
    "                'corp': 'corporation', 'cap': 'capital', 'inv': 'investment',\n",
    "                'asset': 'asset management', 'sec': 'securities', 'adv': 'advisors',\n",
    "                'tr': 'trust', 'port': 'portfolio', 'acct': 'account',\n",
    "                'bal': 'balance', 'stmt': 'statement', 'equ': 'equity',\n",
    "                'div': 'dividend', 'ret': 'retirement', 'pens': 'pension',\n",
    "                'ira': 'individual retirement account', '401k': '401k retirement plan',\n",
    "                'hdg': 'holdings', 'fx': 'foreign exchange', 'mm': 'money market',\n",
    "                'pe': 'private equity', 'vc': 'venture capital', 'am': 'asset management',\n",
    "                'wm': 'wealth management', 'pm': 'portfolio management',\n",
    "                'cc': 'credit card', 'mtg': 'mortgage', 'ins': 'insurance'\n",
    "            },\n",
    "            'Restaurant': {\n",
    "                'rest': 'restaurant', 'cafe': 'cafeteria', 'grill': 'grillery',\n",
    "                'brew': 'brewery', 'bar': 'bar and grill', 'bbq': 'barbecue',\n",
    "                'deli': 'delicatessen', 'stk': 'steakhouse', 'bf': 'breakfast',\n",
    "                'din': 'dinner', 'chs': 'cheese', 'ckn': 'chicken',\n",
    "                'brgr': 'burger', 'piz': 'pizza', 'mex': 'mexican',\n",
    "                'ital': 'italian', 'chin': 'chinese', 'jpn': 'japanese',\n",
    "                'thai': 'thai', 'ind': 'indian', 'korea': 'korean',\n",
    "                'viet': 'vietnamese', 'med': 'mediterranean', 'seafd': 'seafood',\n",
    "                'sushi': 'sushi restaurant', 'taco': 'taco restaurant',\n",
    "                'sand': 'sandwich', 'bagel': 'bagel shop', 'bkry': 'bakery',\n",
    "                'coff': 'coffee', 'juice': 'juice bar', 'smth': 'smoothie'\n",
    "            },\n",
    "            'Retail': {\n",
    "                'dept': 'department', 'str': 'store', 'suprmkt': 'supermarket',\n",
    "                'groc': 'grocery', 'pharm': 'pharmacy', 'disc': 'discount',\n",
    "                'whs': 'warehouse', 'outlet': 'outlet store', 'fash': 'fashion',\n",
    "                'appl': 'appliance', 'elect': 'electronics', 'furn': 'furniture',\n",
    "                'hdw': 'hardware', 'jwlr': 'jewelry', 'btq': 'boutique',\n",
    "                'cloth': 'clothing', 'shoes': 'shoe store', 'sport': 'sporting goods',\n",
    "                'toy': 'toy store', 'book': 'book store', 'music': 'music store',\n",
    "                'auto': 'auto parts', 'pet': 'pet supplies', 'craft': 'craft store',\n",
    "                'opt': 'optical', 'cosm': 'cosmetics', 'lux': 'luxury',\n",
    "                'bargn': 'bargain', 'homegd': 'home goods', 'gard': 'garden',\n",
    "                'off': 'office supplies', 'tech': 'technology'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _load_stopwords(self):\n",
    "        \"\"\"Load general stopwords for preprocessing\"\"\"\n",
    "        return {\n",
    "            'inc', 'llc', 'co', 'ltd', 'corp', 'plc', 'na', 'the', \n",
    "            'and', 'of', 'for', 'in', 'a', 'an', 'by', 'to', 'at',\n",
    "            'corporation', 'incorporated', 'company', 'limited',\n",
    "            'with', 'from', 'as', 'on', 'group', 'services', 'international',\n",
    "            'enterprises', 'holdings', 'global', 'worldwide', 'partners',\n",
    "            'associates', 'industries', 'solutions', 'systems', 'technologies'\n",
    "        }\n",
    "    \n",
    "    def _load_domain_stopwords(self):\n",
    "        \"\"\"Load domain-specific stopwords\"\"\"\n",
    "        return {\n",
    "            'Medical': {\n",
    "                'center', 'healthcare', 'medical', 'health', 'care', 'services', \n",
    "                'clinic', 'hospital', 'center', 'associates', 'physicians',\n",
    "                'specialists', 'group', 'practice', 'medicine', 'diagnostic',\n",
    "                'treatment', 'therapy', 'rehabilitation', 'wellness', 'urgent',\n",
    "                'emergency', 'family', 'primary', 'specialty', 'surgical',\n",
    "                'memorial', 'regional', 'community', 'university', 'medical center'\n",
    "            },\n",
    "            'Government': {\n",
    "                'department', 'office', 'agency', 'bureau', 'division', 'authority', \n",
    "                'administration', 'commission', 'board', 'committee', 'council',\n",
    "                'district', 'federal', 'state', 'county', 'city', 'municipal',\n",
    "                'regional', 'national', 'international', 'public', 'affairs',\n",
    "                'services', 'development', 'program', 'project', 'initiative',\n",
    "                'regulatory', 'oversight', 'enforcement', 'compliance'\n",
    "            },\n",
    "            'Education': {\n",
    "                'university', 'college', 'school', 'institute', 'academy', 'education', \n",
    "                'learning', 'campus', 'department', 'faculty', 'studies',\n",
    "                'research', 'sciences', 'arts', 'technology', 'engineering',\n",
    "                'mathematics', 'business', 'law', 'medicine', 'public',\n",
    "                'community', 'state', 'technical', 'vocational', 'elementary',\n",
    "                'middle', 'high', 'district', 'board', 'trustees'\n",
    "            },\n",
    "            'Financial': {\n",
    "                'financial', 'services', 'management', 'capital', 'investment', \n",
    "                'banking', 'advisor', 'wealth', 'asset', 'fund', 'equity',\n",
    "                'securities', 'brokerage', 'exchange', 'traders', 'trading',\n",
    "                'markets', 'partners', 'associates', 'advisors', 'planners',\n",
    "                'retirement', 'insurance', 'trust', 'savings', 'loan',\n",
    "                'credit', 'mortgage', 'real estate', 'properties'\n",
    "            },\n",
    "            'Restaurant': {\n",
    "                'restaurant', 'cafe', 'diner', 'eatery', 'grill', 'kitchen', \n",
    "                'bar', 'house', 'bistro', 'pizzeria', 'steakhouse', 'bakery',\n",
    "                'coffeehouse', 'pub', 'tavern', 'chophouse', 'seafood',\n",
    "                'cuisine', 'dining', 'food', 'catering', 'express', 'fast food',\n",
    "                'buffet', 'garden', 'parlor', 'house', 'shack', 'joint'\n",
    "            },\n",
    "            'Retail': {\n",
    "                'store', 'shop', 'market', 'mart', 'outlet', 'center', 'warehouse',\n",
    "                'superstore', 'supermarket', 'department', 'boutique', 'emporium',\n",
    "                'gallery', 'corner', 'supply', 'supplies', 'goods', 'products',\n",
    "                'retail', 'discount', 'factory', 'co-op', 'cooperative', 'shoppe',\n",
    "                'super', 'mega', 'mini', 'express', 'neighborhood', 'local'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _compile_business_suffixes(self):\n",
    "        \"\"\"\n",
    "        Compile comprehensive regex patterns for common business name suffixes\n",
    "        with more variations and international coverage\n",
    "        \"\"\"\n",
    "        suffix_patterns = {\n",
    "            r'\\bco(?:mpany)?\\b': '',             # Company/Co.\n",
    "            r'\\binc(?:orporated)?\\b': '',        # Incorporated/Inc.\n",
    "            r'\\bltd\\b': '',                      # Ltd\n",
    "            r'\\bllc\\b': '',                      # LLC\n",
    "            r'\\bcorp(?:oration)?\\b': '',         # Corporation/Corp\n",
    "            r'\\blimited\\b': '',                  # Limited\n",
    "            r'\\bplc\\b': '',                      # PLC\n",
    "            r'\\bltda\\b': '',                     # LTDA (Latin America)\n",
    "            r'\\bgmbh\\b': '',                     # GmbH (German)\n",
    "            r'\\bs\\.?a\\.?\\b': '',                 # S.A. (international)\n",
    "            r'\\bs\\.?p\\.?a\\.?\\b': '',             # S.p.A. (Italian)\n",
    "            r'\\bs\\.?a\\.?r\\.?l\\.?\\b': '',         # S.A.R.L. (French)\n",
    "            r'\\bs\\.?l\\.?\\b': '',                 # S.L. (Spanish)\n",
    "            r'\\ba\\.?g\\.?\\b': '',                 # A.G. (German)\n",
    "            r'\\ba\\.?s\\.?\\b': '',                 # A.S. (Scandinavian)\n",
    "            r'\\bpty\\b': '',                      # PTY (Australia)\n",
    "            r'\\bpvt\\b': '',                      # PVT (private)\n",
    "            r'\\bp\\.?t\\.?\\b': '',                 # P.T. (Indonesian)\n",
    "            r'\\bc\\.?v\\.?\\b': '',                 # C.V. (Dutch)\n",
    "            r'\\bgroup\\b': '',                    # Group\n",
    "            r'\\bholdings?\\b': '',                # Holdings/Holding\n",
    "            r'\\binternational\\b': '',            # International\n",
    "            r'\\bworldwide\\b': '',                # Worldwide\n",
    "            r'\\bglobal\\b': '',                   # Global\n",
    "            r'\\benterprises?\\b': '',             # Enterprises/Enterprise\n",
    "            r'\\bassociates?\\b': '',              # Associates/Associate\n",
    "            r'\\bpartners?\\b': '',                # Partners/Partner\n",
    "            r'\\bindustries?\\b': '',              # Industries/Industry\n",
    "            r'\\bsolutions?\\b': '',               # Solutions/Solution\n",
    "            r'\\bservices?\\b': '',                # Services/Service\n",
    "            r'\\btechnologies?\\b': '',            # Technologies/Technology\n",
    "            r'\\bsystems?\\b': ''                  # Systems/System\n",
    "        }\n",
    "        \n",
    "        # Compile all regex patterns\n",
    "        compiled_patterns = {re.compile(pattern, re.IGNORECASE): replacement \n",
    "                             for pattern, replacement in suffix_patterns.items()}\n",
    "        \n",
    "        return compiled_patterns\n",
    "    \n",
    "    def _compile_business_patterns(self):\n",
    "        \"\"\"Compile regex patterns for common business name structures\"\"\"\n",
    "        business_patterns = {\n",
    "            # Location prefix patterns (e.g., \"North Shore Hospital\")\n",
    "            r'(north|south|east|west|central|downtown|uptown|midtown)\\s+(.+)': \n",
    "                lambda m: m.group(2),  # Keep main business name\n",
    "                \n",
    "            # Location suffix patterns (e.g., \"Walmart Downtown\")\n",
    "            r'(.+?)\\s+(north|south|east|west|central|downtown|uptown|midtown)$': \n",
    "                lambda m: m.group(1),  # Keep main business name\n",
    "                \n",
    "            # Branch/location patterns (e.g., \"Chase Bank - Brooklyn Branch\")\n",
    "            r'(.+?)\\s*[\\-\\\\\\#]\\s*(.+?)\\s+(branch|location|store|outlet)$': \n",
    "                lambda m: m.group(1),  # Keep main entity\n",
    "                \n",
    "            # Store number patterns (e.g., \"Target #1234\")\n",
    "            r'(.+?)\\s*#\\s*\\d+$': \n",
    "                lambda m: m.group(1),  # Keep store name, remove number\n",
    "                \n",
    "            # Franchise patterns (e.g., \"McDonald's Franchise\")\n",
    "            r'(.+?)\\s+(franchise|franchisee)$': \n",
    "                lambda m: m.group(1),  # Keep main brand\n",
    "                \n",
    "            # Multiple locations pattern (e.g., \"Starbucks - New York & Boston\")\n",
    "            r'(.+?)\\s*[\\-\\\\]\\s*(.+?\\s+(?:&|and)\\s+.+?)$': \n",
    "                lambda m: m.group(1)   # Keep main entity\n",
    "        }\n",
    "        \n",
    "        # Compile all regex patterns\n",
    "        compiled_patterns = {re.compile(pattern, re.IGNORECASE): replacement \n",
    "                             for pattern, replacement in business_patterns.items()}\n",
    "        \n",
    "        return compiled_patterns\n",
    "    \n",
    "    def preprocess(self, text, domain=None, remove_suffixes=True, normalize_mcdonald=True):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with better merchant-specific handling\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to preprocess\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            remove_suffixes (bool): Whether to remove business suffixes\n",
    "            normalize_mcdonald (bool): Whether to normalize McDonald's variants\n",
    "            \n",
    "        Returns:\n",
    "            str: Preprocessed text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Save original for special case handling\n",
    "        original_text = text\n",
    "        \n",
    "        # Handle special cases before general processing\n",
    "        \n",
    "        # McDonald's special handling\n",
    "        if normalize_mcdonald and ('mcdonald' in text or 'mcd' in text or 'mcdonalds' in text):\n",
    "            # Normalize all McDonald's variants\n",
    "            for pattern in ['mcdonald\\'s', 'mcdonalds', 'mcdonald', 'mcd ', 'mcd\\'s']:\n",
    "                if pattern in text:\n",
    "                    text = 'mcdonalds'\n",
    "                    break\n",
    "        \n",
    "        # Starbucks special handling\n",
    "        if 'starbucks' in text or 'sbux' in text:\n",
    "            for pattern in ['starbucks coffee', 'starbucks coffee company', 'starbucks corp']:\n",
    "                if pattern in text:\n",
    "                    text = 'starbucks'\n",
    "                    break\n",
    "        \n",
    "        # Walmart special handling\n",
    "        if 'walmart' in text or 'wal' in text or 'wmt' in text:\n",
    "            for pattern in ['wal-mart', 'wal mart', 'walmart supercenter', 'walmart neighborhood market']:\n",
    "                if pattern in text:\n",
    "                    text = 'walmart'\n",
    "                    break\n",
    "        \n",
    "        # If we've made a special case substitution, return it directly\n",
    "        if text != original_text.lower().strip() and len(text) > 0:\n",
    "            return text\n",
    "        \n",
    "        # Better handling of punctuation - preserve apostrophes in business names\n",
    "        text = re.sub(r'([^a-z0-9\\'\\.\\&\\-])', ' ', text)\n",
    "        \n",
    "        # Special handling for business name apostrophes with multiple patterns\n",
    "        text = re.sub(r'\\'s\\b', 's', text)  # Convert McDonald's to McDonalds\n",
    "        text = re.sub(r'\\'', '', text)      # Remove remaining apostrophes\n",
    "        \n",
    "        # Handle special characters in company names\n",
    "        text = re.sub(r'&amp;', '&', text)  # Fix HTML entities\n",
    "        text = re.sub(r'@', 'at', text)     # Replace @ with 'at'\n",
    "        \n",
    "        # Normalize spaces and remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Apply business suffix removal if requested\n",
    "        if remove_suffixes:\n",
    "            for pattern, replacement in self.business_suffixes.items():\n",
    "                text = pattern.sub(replacement, text)\n",
    "        \n",
    "        # Apply business pattern normalization\n",
    "        for pattern, replacement_func in self.business_patterns.items():\n",
    "            match = pattern.search(text)\n",
    "            if match:\n",
    "                text = replacement_func(match).strip()\n",
    "        \n",
    "        # Split words for further processing\n",
    "        words = text.split()\n",
    "        \n",
    "        # Apply general abbreviation expansion\n",
    "        words = [self.abbreviations.get(word, word) for word in words]\n",
    "        \n",
    "        # Apply domain-specific abbreviation expansion if domain is provided\n",
    "        if domain and domain in self.domain_abbreviations:\n",
    "            words = [self.domain_abbreviations[domain].get(word, word) for word in words]\n",
    "        \n",
    "        # Create expanded stopwords for this domain\n",
    "        expanded_stopwords = self.stopwords.copy()\n",
    "        if domain and domain in self.domain_stopwords:\n",
    "            expanded_stopwords.update(self.domain_stopwords[domain])\n",
    "        \n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in expanded_stopwords]\n",
    "        \n",
    "        # Rejoin words and remove extra spaces\n",
    "        text = ' '.join(words)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_pair(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Preprocess acronym and full name pair with domain-specific handling\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): Acronym or short name\n",
    "            full_name (str): Full name\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (preprocessed_acronym, preprocessed_full_name)\n",
    "        \"\"\"\n",
    "        acronym_clean = self.preprocess(acronym, domain)\n",
    "        full_name_clean = self.preprocess(full_name, domain)\n",
    "        return acronym_clean, full_name_clean\n",
    "\n",
    "# Initialize merchant preprocessor\n",
    "merchant_preprocessor = MerchantPreprocessor()\n",
    "logger.info(\"Enhanced merchant preprocessor initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59417fde-c805-4521-a6f2-079f64aea33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 00:24:30,125 - INFO - Enhanced similarity algorithms initialized!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Similarity Algorithms\n",
    "class SimilarityAlgorithms:\n",
    "    \"\"\"\n",
    "    Enhanced similarity algorithms specifically optimized for merchant name matching.\n",
    "    Includes traditional string-based methods and specialized merchant-specific algorithms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor=None, bert_embedder=None):\n",
    "        \"\"\"\n",
    "        Initialize similarity algorithms with preprocessor and BERT embedder\n",
    "        \n",
    "        Args:\n",
    "            preprocessor: Merchant name preprocessor\n",
    "            bert_embedder: BERT embedder for semantic similarity\n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor or MerchantPreprocessor()\n",
    "        self.bert_embedder = bert_embedder\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer for fallback\n",
    "        self.tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 5))\n",
    "        self.tfidf_fitted = False\n",
    "        \n",
    "        # Thresholds for different similarity types\n",
    "        self.exact_match_threshold = 0.95\n",
    "        self.high_match_threshold = 0.85\n",
    "        self.medium_match_threshold = 0.70\n",
    "        self.low_match_threshold = 0.50\n",
    "    \n",
    "    def jaro_winkler_similarity(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate Jaro-Winkler similarity with enhanced preprocessing\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaro-Winkler similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Apply Jaro-Winkler algorithm\n",
    "        return jaro_winkler(s1_clean, s2_clean)\n",
    "    \n",
    "    def damerau_levenshtein_similarity(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate Damerau-Levenshtein similarity, better for handling transpositions\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Damerau-Levenshtein similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Damerau-Levenshtein distance\n",
    "        max_len = max(len(s1_clean), len(s2_clean))\n",
    "        if max_len == 0:\n",
    "            return 0\n",
    "        \n",
    "        distance = textdistance.damerau_levenshtein.distance(s1_clean, s2_clean)\n",
    "        similarity = 1 - (distance / max_len)\n",
    "        return max(0, similarity)  # Ensure non-negative\n",
    "    \n",
    "    def tfidf_cosine_similarity(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate TF-IDF Cosine similarity for keyword matching\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: TF-IDF cosine similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Fit and transform with TF-IDF\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf.fit_transform([s1_clean, s2_clean])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return float(max(0, similarity))  # Ensure non-negative\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def jaccard_ngram_similarity(self, s1, s2, domain=None, n=2):\n",
    "        \"\"\"\n",
    "        Calculate Jaccard n-gram similarity for character overlaps\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            n (int): Size of n-grams\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard n-gram similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Create n-grams\n",
    "        def get_ngrams(text, n):\n",
    "            return [text[i:i+n] for i in range(len(text)-(n-1))]\n",
    "        \n",
    "        s1_ngrams = set(get_ngrams(s1_clean, n))\n",
    "        s2_ngrams = set(get_ngrams(s2_clean, n))\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        union_size = len(s1_ngrams.union(s2_ngrams))\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        \n",
    "        intersection_size = len(s1_ngrams.intersection(s2_ngrams))\n",
    "        return intersection_size / union_size\n",
    "    \n",
    "    def token_set_ratio(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate token set ratio using fuzzywuzzy\n",
    "        Handles word order and partial matching well\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Token set ratio between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Token Set Ratio\n",
    "        return fuzz.token_set_ratio(s1_clean, s2_clean) / 100\n",
    "    \n",
    "    def token_sort_ratio(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate token sort ratio using fuzzywuzzy\n",
    "        Handles word order differences well\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Token sort ratio between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Token Sort Ratio\n",
    "        return fuzz.token_sort_ratio(s1_clean, s2_clean) / 100\n",
    "    \n",
    "    def contains_ratio(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Check if one string contains the other, useful for acronym-full name matching\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Containment ratio between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Special case for McDonald's\n",
    "        if ('mcdonald' in s1_clean and 'mcdonald' in s2_clean) or ('mcd' in s1_clean and 'mcdonald' in s2_clean) or ('mcdonald' in s1_clean and 'mcd' in s2_clean):\n",
    "            return 1.0\n",
    "            \n",
    "        # Special case for Starbucks\n",
    "        if ('starbuck' in s1_clean and 'starbuck' in s2_clean) or ('sbux' in s1_clean and 'starbuck' in s2_clean) or ('starbuck' in s1_clean and 'sbux' in s2_clean):\n",
    "            return 1.0\n",
    "            \n",
    "        # Special case for Walmart\n",
    "        if ('walmart' in s1_clean and 'walmart' in s2_clean) or ('wmt' in s1_clean and 'walmart' in s2_clean) or ('walmart' in s1_clean and 'wmt' in s2_clean):\n",
    "            return 1.0\n",
    "        \n",
    "        # Check if one string fully contains the other\n",
    "        if s1_clean in s2_clean:\n",
    "            return 1.0\n",
    "        elif s2_clean in s1_clean:\n",
    "            return 1.0\n",
    "            \n",
    "        # Check for partial containment at word level\n",
    "        s1_words = set(s1_clean.split())\n",
    "        s2_words = set(s2_clean.split())\n",
    "        \n",
    "        # If all words in the shorter string are in the longer string\n",
    "        if s1_words.issubset(s2_words):\n",
    "            return 0.9\n",
    "        elif s2_words.issubset(s1_words):\n",
    "            return 0.9\n",
    "        \n",
    "        # Check overlap at the word level\n",
    "        intersection = s1_words.intersection(s2_words)\n",
    "        shorter_len = min(len(s1_words), len(s2_words))\n",
    "        \n",
    "        if shorter_len == 0:\n",
    "            # Fall back to character-level check for short strings\n",
    "            return self._char_contains_ratio(s1_clean, s2_clean)\n",
    "        \n",
    "        return len(intersection) / shorter_len\n",
    "    \n",
    "    def _char_contains_ratio(self, s1, s2):\n",
    "        \"\"\"Helper method for character-level containment ratio\"\"\"\n",
    "        s1_chars = list(s1)\n",
    "        s2_chars = list(s2)\n",
    "        \n",
    "        matches = 0\n",
    "        for char in s1_chars:\n",
    "            if char in s2_chars:\n",
    "                matches += 1\n",
    "                s2_chars.remove(char)  # Remove matched char\n",
    "        \n",
    "        return matches / len(s1_chars) if len(s1_chars) > 0 else 0\n",
    "    \n",
    "    def acronym_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate how well the acronym is formed from the full name\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym\n",
    "            full_name (str): The full name\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Acronym similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Special case handling for common acronyms\n",
    "        acronym_lower = acronym.lower() if isinstance(acronym, str) else \"\"\n",
    "        full_name_lower = full_name.lower() if isinstance(full_name, str) else \"\"\n",
    "        \n",
    "        # Check known acronym-full name pairs\n",
    "        known_pairs = {\n",
    "            'bofa': 'bank of america',\n",
    "            'jp': 'jpmorgan',\n",
    "            'jpm': 'jpmorgan',\n",
    "            'wf': 'wells fargo',\n",
    "            'gs': 'goldman sachs',\n",
    "            'ms': 'morgan stanley',\n",
    "            'citi': 'citibank',\n",
    "            'amex': 'american express',\n",
    "            'mcd': 'mcdonalds',\n",
    "            'sbux': 'starbucks',\n",
    "            'wmt': 'walmart',\n",
    "            'hd': 'home depot',\n",
    "            'low': 'lowes',\n",
    "            'msft': 'microsoft',\n",
    "            'goog': 'google',\n",
    "            'amzn': 'amazon',\n",
    "            'aapl': 'apple'\n",
    "        }\n",
    "        \n",
    "        # Check for known acronym pairs\n",
    "        for known_acr, known_full in known_pairs.items():\n",
    "            if known_acr in acronym_lower and known_full in full_name_lower:\n",
    "                return 1.0  # Perfect match for known pairs\n",
    "        \n",
    "        # Preprocess inputs\n",
    "        acronym_clean, full_name_clean = self.preprocessor.preprocess_pair(acronym, full_name, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Extract first letters from each word in full name\n",
    "        words = full_name_clean.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        # Standard acronym formation - first letter of each word\n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # Check if acronym matches first letters pattern\n",
    "        acronym_lower = acronym_clean.lower()\n",
    "        first_letters_lower = first_letters.lower()\n",
    "        \n",
    "        # Perfect first-letter acronym match\n",
    "        if acronym_lower == first_letters_lower:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check if acronym contains first letters in sequence\n",
    "        acronym_pos = 0\n",
    "        for char in first_letters_lower:\n",
    "            if acronym_pos < len(acronym_lower) and char == acronym_lower[acronym_pos]:\n",
    "                acronym_pos += 1\n",
    "                \n",
    "        if acronym_pos == len(acronym_lower):\n",
    "            # All acronym chars found in sequence\n",
    "            return 0.9\n",
    "        \n",
    "        # Check for matching first letters (in any order)\n",
    "        acronym_chars = set(acronym_lower)\n",
    "        first_letters_chars = set(first_letters_lower)\n",
    "        \n",
    "        if acronym_chars.issubset(first_letters_chars):\n",
    "            # All acronym chars are first letters\n",
    "            return 0.8\n",
    "        \n",
    "        # Check if acronym formed from first letters of some words (partial match)\n",
    "        matches = 0\n",
    "        remaining_acronym = acronym_lower\n",
    "        \n",
    "        for word in words:\n",
    "            if not word or not remaining_acronym:\n",
    "                continue\n",
    "                \n",
    "            if word[0] == remaining_acronym[0]:\n",
    "                matches += 1\n",
    "                remaining_acronym = remaining_acronym[1:]\n",
    "        \n",
    "        if len(acronym_lower) > 0:\n",
    "            return matches / len(acronym_lower)\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def specialized_acronym_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Enhanced acronym similarity with special handling for various business naming patterns\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym\n",
    "            full_name (str): The full name\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Specialized acronym similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Get base acronym similarity\n",
    "        base_score = self.acronym_similarity(acronym, full_name, domain)\n",
    "        \n",
    "        # If base score is high, no need for specialized handling\n",
    "        if base_score > 0.8:\n",
    "            return base_score\n",
    "        \n",
    "        acronym_clean, full_name_clean = self.preprocessor.preprocess_pair(acronym, full_name, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Special case for \"Mc\" prefixes (common in restaurant names)\n",
    "        if full_name_clean.startswith('mc') and len(acronym_clean) >= 1 and acronym_clean[0] == 'm':\n",
    "            # McDonalds -> MCD pattern\n",
    "            modified_full_name = full_name_clean[2:]  # Remove \"mc\"\n",
    "            remaining_chars = acronym_clean[1:]  # Remove \"m\"\n",
    "            \n",
    "            # For \"MCD\" -> \"McDonalds\" pattern\n",
    "            if remaining_chars and len(modified_full_name) > 0:\n",
    "                # Check if remaining chars match consonants in the name\n",
    "                consonants = ''.join([c for c in modified_full_name if c not in 'aeiou'])\n",
    "                if remaining_chars in consonants:\n",
    "                    return 0.95\n",
    "                \n",
    "                # Check first letters after \"Mc\"\n",
    "                words = modified_full_name.split()\n",
    "                if words:\n",
    "                    first_letters = ''.join([word[0] for word in words if word])\n",
    "                    if remaining_chars in first_letters:\n",
    "                        return 0.90\n",
    "        \n",
    "        # Handle initialism patterns (e.g., \"IBM\" -> \"International Business Machines\")\n",
    "        if len(acronym_clean) >= 2 and len(full_name_clean.split()) >= len(acronym_clean):\n",
    "            words = full_name_clean.split()\n",
    "            \n",
    "            # Check if acronym consists of first letters of consecutive words\n",
    "            for i in range(len(words) - len(acronym_clean) + 1):\n",
    "                subset = words[i:i+len(acronym_clean)]\n",
    "                initials = ''.join([word[0] for word in subset if word])\n",
    "                \n",
    "                if initials.lower() == acronym_clean.lower():\n",
    "                    return 0.95\n",
    "        \n",
    "        # Handle \"Bank of X\" vs \"X Bank\" pattern\n",
    "        if ('bank' in acronym_clean and 'bank' in full_name_clean) or ('banking' in acronym_clean and 'bank' in full_name_clean):\n",
    "            # Extract the main entity name (before/after \"Bank\")\n",
    "            if 'bank of' in full_name_clean:\n",
    "                entity = full_name_clean.split('bank of')[1].strip()\n",
    "                if entity and entity[0].lower() in acronym_clean.lower():\n",
    "                    return 0.85\n",
    "            \n",
    "            # Check for \"X Bank\" pattern\n",
    "            for word in acronym_clean.split():\n",
    "                if word != 'bank' and word in full_name_clean:\n",
    "                    return 0.8\n",
    "        \n",
    "        # Check for abbreviated words (like \"Intl\" -> \"International\")\n",
    "        abbr_dict = {\n",
    "            'intl': 'international',\n",
    "            'natl': 'national',\n",
    "            'amer': 'america',\n",
    "            'assn': 'association',\n",
    "            'assoc': 'associates',\n",
    "            'corp': 'corporation',\n",
    "            'univ': 'university',\n",
    "            'tech': 'technology',\n",
    "            'inst': 'institute'\n",
    "        }\n",
    "        \n",
    "        for abbr, full in abbr_dict.items():\n",
    "            if abbr in acronym_clean and full in full_name_clean:\n",
    "                return 0.85\n",
    "        \n",
    "        # Return base score if no specialized patterns matched\n",
    "        return base_score\n",
    "    \n",
    "    def phonetic_similarity(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate phonetic similarity using Soundex algorithm.\n",
    "        Especially useful for similar-sounding business names.\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Phonetic similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # If either string is empty, return 0\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get the soundex codes for words\n",
    "        try:\n",
    "            # For multi-word strings, get soundex for each word\n",
    "            s1_words = s1_clean.split()\n",
    "            s2_words = s2_clean.split()\n",
    "            \n",
    "            # Get soundex codes for each word\n",
    "            s1_codes = [jellyfish.soundex(word) for word in s1_words if len(word) > 1]\n",
    "            s2_codes = [jellyfish.soundex(word) for word in s2_words if len(word) > 1]\n",
    "            \n",
    "            # Calculate matches between codes\n",
    "            matches = 0\n",
    "            total = max(len(s1_codes), len(s2_codes))\n",
    "            \n",
    "            if total == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # Count matched codes\n",
    "            for code in s1_codes:\n",
    "                if code in s2_codes:\n",
    "                    matches += 1\n",
    "                    # Remove the matched code to avoid double counting\n",
    "                    s2_codes.remove(code)\n",
    "            \n",
    "            return matches / total\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in phonetic similarity: {e}\")\n",
    "            # Fallback if there's an error with the soundex calculation\n",
    "            return 0.0\n",
    "    \n",
    "    def metaphone_similarity(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate metaphone similarity (better than Soundex for English).\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Metaphone similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # If either string is empty, return 0\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # For multi-word strings, get metaphone for each word\n",
    "            s1_words = s1_clean.split()\n",
    "            s2_words = s2_clean.split()\n",
    "            \n",
    "            # Get metaphone codes for each word\n",
    "            s1_codes = [jellyfish.metaphone(word) for word in s1_words if len(word) > 1]\n",
    "            s2_codes = [jellyfish.metaphone(word) for word in s2_words if len(word) > 1]\n",
    "            \n",
    "            # Calculate matches between codes\n",
    "            matches = 0\n",
    "            total = max(len(s1_codes), len(s2_codes))\n",
    "            \n",
    "            if total == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # Count matched codes\n",
    "            for code in s1_codes:\n",
    "                if code in s2_codes:\n",
    "                    matches += 1\n",
    "                    # Remove the matched code to avoid double counting\n",
    "                    s2_codes.remove(code)\n",
    "            \n",
    "            return matches / total\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in metaphone similarity: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def semantic_similarity(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate semantic similarity using BERT embeddings\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Semantic similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        if not self.bert_embedder:\n",
    "            return 0.0\n",
    "            \n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            return self.bert_embedder.compute_similarity(s1_clean, s2_clean)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in semantic similarity: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def multi_algorithm_similarity(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate a weighted average of multiple similarity algorithms\n",
    "        with weights optimized for merchant name matching\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Multi-algorithm similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Preprocess inputs\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Base weights\n",
    "        weights = {\n",
    "            'jaro_winkler': 0.2,\n",
    "            'token_set_ratio': 0.15,\n",
    "            'contains_ratio': 0.15,\n",
    "            'acronym': 0.1,\n",
    "            'semantic': 0.25,\n",
    "            'phonetic': 0.05,\n",
    "            'metaphone': 0.05,\n",
    "            'jaccard_ngram': 0.05\n",
    "        }\n",
    "        \n",
    "        # Calculate each similarity score\n",
    "        scores = {\n",
    "            'jaro_winkler': self.jaro_winkler_similarity(s1, s2, domain),\n",
    "            'token_set_ratio': self.token_set_ratio(s1, s2, domain),\n",
    "            'contains_ratio': self.contains_ratio(s1, s2, domain),\n",
    "            'acronym': self.specialized_acronym_similarity(s1, s2, domain),\n",
    "            'semantic': self.semantic_similarity(s1, s2, domain) if self.bert_embedder else 0,\n",
    "            'phonetic': self.phonetic_similarity(s1, s2, domain),\n",
    "            'metaphone': self.metaphone_similarity(s1, s2, domain),\n",
    "            'jaccard_ngram': self.jaccard_ngram_similarity(s1, s2, domain, n=3)\n",
    "        }\n",
    "        \n",
    "        # Adjust weights based on domain if provided\n",
    "        if domain:\n",
    "            if domain == 'Banking' or domain == 'Financial':\n",
    "                weights['acronym'] = 0.20\n",
    "                weights['contains_ratio'] = 0.20\n",
    "                weights['jaro_winkler'] = 0.15\n",
    "                weights['semantic'] = 0.25\n",
    "                weights['token_set_ratio'] = 0.10\n",
    "            elif domain == 'Restaurant':\n",
    "                weights['jaro_winkler'] = 0.25\n",
    "                weights['semantic'] = 0.30\n",
    "                weights['phonetic'] = 0.15\n",
    "                weights['token_set_ratio'] = 0.20\n",
    "                weights['acronym'] = 0.05\n",
    "            elif domain == 'Retail':\n",
    "                weights['contains_ratio'] = 0.25\n",
    "                weights['semantic'] = 0.30\n",
    "                weights['jaro_winkler'] = 0.20\n",
    "                weights['token_set_ratio'] = 0.15\n",
    "                weights['acronym'] = 0.05\n",
    "        \n",
    "        # Calculate weighted average\n",
    "        weighted_sum = sum(weights[algo] * score for algo, score in scores.items())\n",
    "        total_weight = sum(weights.values())\n",
    "        \n",
    "        return weighted_sum / total_weight if total_weight > 0 else 0\n",
    "\n",
    "# Initialize similarity algorithms with the preprocessor and BERT embedder\n",
    "similarity_algorithms = SimilarityAlgorithms(\n",
    "    preprocessor=merchant_preprocessor, \n",
    "    bert_embedder=bert_embedder\n",
    ")\n",
    "logger.info(\"Enhanced similarity algorithms initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ddeb8f-3f16-4b4f-a5b7-fc1617c30173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Advanced Pattern Recognition\n",
    "\n",
    "class PatternRecognition:\n",
    "    \"\"\"\n",
    "    Advanced pattern recognition for merchant names with industry-specific rules.\n",
    "    Detects common patterns in merchant naming conventions across different industries\n",
    "    with enhanced accuracy and specialized handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor=None, similarity_algorithms=None):\n",
    "        \"\"\"\n",
    "        Initialize pattern recognition with preprocessor and similarity algorithms\n",
    "        \n",
    "        Args:\n",
    "            preprocessor: Merchant name preprocessor\n",
    "            similarity_algorithms: Similarity algorithms for merchant matching\n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor or MerchantPreprocessor()\n",
    "        self.similarity_algorithms = similarity_algorithms or SimilarityAlgorithms(self.preprocessor)\n",
    "        \n",
    "        # Load all pattern dictionaries\n",
    "        self.common_merchant_patterns = self._load_merchant_patterns()\n",
    "        self.known_equivalents = self._load_known_equivalents()\n",
    "        self.abbreviation_patterns = self._load_abbreviation_patterns()\n",
    "        \n",
    "        # Initialize regex pattern compilation\n",
    "        self._compile_regex_patterns()\n",
    "        \n",
    "        # Initialize Aho-Corasick automaton for fast pattern matching if available\n",
    "        self.aho_corasick_finder = None\n",
    "        if aho_corasick_available:\n",
    "            self._initialize_aho_corasick()\n",
    "    \n",
    "    def _load_merchant_patterns(self):\n",
    "        \"\"\"Load comprehensive merchant name patterns by industry\"\"\"\n",
    "        return {\n",
    "            'banking': {\n",
    "                'bank_name_patterns': [\n",
    "                    # Basic bank naming patterns\n",
    "                    (r'(\\w+)\\s+bank', r'bank\\s+of\\s+(\\w+)'),  # \"Chase Bank\" vs \"Bank of America\"\n",
    "                    (r'(\\w+)\\s+banking', r'(\\w+)\\s+financial'),  # \"Western Banking\" vs \"Western Financial\"\n",
    "                    (r'(\\w+)\\s+credit\\s+union', r'(\\w+)\\s+cu'),  # \"State Credit Union\" vs \"State CU\"\n",
    "                    (r'(\\w+)\\s+savings\\s+bank', r'(\\w+)\\s+savings'),  # \"Community Savings Bank\" vs \"Community Savings\"\n",
    "                    (r'(\\w+)\\s+trust\\s+company', r'(\\w+)\\s+trust'),  # \"First Trust Company\" vs \"First Trust\"\n",
    "                    (r'(\\w+)\\s+national\\s+bank', r'(\\w+)\\s+natl'),  # \"First National Bank\" vs \"First Natl\"\n",
    "                    \n",
    "                    # International bank patterns\n",
    "                    (r'banco\\s+(\\w+)', r'bank\\s+of\\s+(\\w+)'),  # \"Banco Popular\" vs \"Bank of Popular\"\n",
    "                    (r'banque\\s+(\\w+)', r'bank\\s+of\\s+(\\w+)'),  # \"Banque Nationale\" vs \"National Bank\"\n",
    "                    (r'deutsche\\s+(\\w+)', r'german\\s+(\\w+)'),  # \"Deutsche Bank\" vs \"German Bank\"\n",
    "                ],\n",
    "                'branch_patterns': [\n",
    "                    (r'(\\w+)\\s+branch', r'\\1'),  # \"Downtown Branch\" -> \"Downtown\"\n",
    "                    (r'(\\w+)\\s+location', r'\\1'),  # \"Main Location\" -> \"Main\"\n",
    "                    (r'(\\w+)\\s+-\\s+(\\w+)', r'\\1'),  # \"Chase - Brooklyn\" -> \"Chase\"\n",
    "                    (r'(\\w+)\\s+banking\\s+center', r'\\1'),  # \"Chase Banking Center\" -> \"Chase\"\n",
    "                    (r'(\\w+)\\s+financial\\s+center', r'\\1'),  # \"Wells Fargo Financial Center\" -> \"Wells Fargo\"\n",
    "                    (r'(\\w+)\\s+atm', r'\\1'),  # \"Chase ATM\" -> \"Chase\"\n",
    "                ]\n",
    "            },\n",
    "            'retail': {\n",
    "                'store_patterns': [\n",
    "                    (r'(\\w+)\\s+#\\d+', r'\\1'),  # \"Walmart #1234\" -> \"Walmart\"\n",
    "                    (r'(\\w+)\\s+store', r'\\1'),  # \"Target Store\" -> \"Target\"\n",
    "                    (r'(\\w+)\\s+superstore', r'\\1'),  # \"Walmart Superstore\" -> \"Walmart\"\n",
    "                    (r'(\\w+)\\s+supermarket', r'\\1'),  # \"Kroger Supermarket\" -> \"Kroger\"\n",
    "                    (r'(\\w+)\\s+market', r'\\1'),  # \"Whole Foods Market\" -> \"Whole Foods\"\n",
    "                    (r'(\\w+)\\s+express', r'\\1'),  # \"Safeway Express\" -> \"Safeway\"\n",
    "                    (r'(\\w+)\\s+neighborhood\\s+market', r'\\1'),  # \"Walmart Neighborhood Market\" -> \"Walmart\"\n",
    "                    (r'(\\w+)\\s+supercenter', r'\\1'),  # \"Walmart Supercenter\" -> \"Walmart\"\n",
    "                    (r'(\\w+)\\s+warehouse', r'\\1'),  # \"Costco Warehouse\" -> \"Costco\"\n",
    "                    (r'(\\w+)\\s+dept\\s+store', r'\\1'),  # \"Macy's Dept Store\" -> \"Macy's\"\n",
    "                    (r'(\\w+)\\s+department\\s+store', r'\\1'),  # \"Macy's Department Store\" -> \"Macy's\"\n",
    "                    (r'(\\w+)\\s+outlet', r'\\1'),  # \"Nike Outlet\" -> \"Nike\"\n",
    "                ],\n",
    "                'location_patterns': [\n",
    "                    (r'(\\w+)\\s+at\\s+(\\w+\\s*\\w*)', r'\\1'),  # \"Target at Springfield Mall\" -> \"Target\"\n",
    "                    (r'(\\w+)\\s+in\\s+(\\w+\\s*\\w*)', r'\\1'),  # \"Walmart in Chicago\" -> \"Walmart\"\n",
    "                    (r'(\\w+)\\s+on\\s+(\\w+\\s*\\w*)', r'\\1'),  # \"Kroger on Main Street\" -> \"Kroger\"\n",
    "                    (r'(\\w+)\\s+-\\s+(\\w+\\s*\\w*)', r'\\1'),  # \"Best Buy - Downtown\" -> \"Best Buy\"\n",
    "                ],\n",
    "                'specific_store_types': [\n",
    "                    (r'(\\w+)\\s+pharmacy', r'\\1'),  # \"CVS Pharmacy\" -> \"CVS\"\n",
    "                    (r'(\\w+)\\s+drug\\s+store', r'\\1'),  # \"Walgreens Drug Store\" -> \"Walgreens\"\n",
    "                    (r'(\\w+)\\s+hardware', r'\\1'),  # \"Ace Hardware\" -> \"Ace\"\n",
    "                    (r'(\\w+)\\s+home\\s+improvement', r'\\1'),  # \"Lowe's Home Improvement\" -> \"Lowe's\"\n",
    "                    (r'(\\w+)\\s+home\\s+depot', r'home\\s+depot'),  # Any \"Home Depot\" variation\n",
    "                ],\n",
    "            },\n",
    "            'restaurant': {\n",
    "                'location_patterns': [\n",
    "                    (r'(\\w+)\\s+restaurant', r'\\1'),  # \"McDonald's Restaurant\" -> \"McDonald's\"\n",
    "                    (r'(\\w+)\\s+cafe', r'\\1'),  # \"Starbucks Cafe\" -> \"Starbucks\"\n",
    "                    (r'(\\w+)\\s+grill', r'\\1'),  # \"Applebee's Grill\" -> \"Applebee's\"\n",
    "                    (r'(\\w+)\\s+-\\s+(\\w+)', r'\\1'),  # \"McDonald's - Downtown\" -> \"McDonald's\"\n",
    "                    (r'(\\w+)\\s+kitchen', r'\\1'),  # \"Chipotle Mexican Kitchen\" -> \"Chipotle Mexican\"\n",
    "                    (r'(\\w+)\\s+bar\\s+&?\\s*grill', r'\\1'),  # \"Chili's Bar & Grill\" -> \"Chili's\"\n",
    "                    (r'(\\w+)\\s+eatery', r'\\1'),  # \"Panera Eatery\" -> \"Panera\"\n",
    "                    (r'(\\w+)\\s+diner', r'\\1'),  # \"Denny's Diner\" -> \"Denny's\"\n",
    "                ],\n",
    "                'chain_name_patterns': [\n",
    "                    (r'mcdonald\\'?s', r'mcdonalds'),  # Normalize McDonald's variations\n",
    "                    (r'dunkin\\'?\\s*donuts?', r'dunkin'),  # \"Dunkin' Donuts\" -> \"Dunkin\"\n",
    "                    (r'starbucks\\s+coffee', r'starbucks'),  # \"Starbucks Coffee\" -> \"Starbucks\"\n",
    "                    (r'kfc|kentucky\\s+fried\\s+chicken', r'kfc'),  # KFC variations\n",
    "                    (r'burger\\s+king', r'bk'),  # \"Burger King\" -> \"BK\"\n",
    "                    (r'pizza\\s+hut', r'pizzahut'),  # \"Pizza Hut\" -> \"PizzaHut\"\n",
    "                    (r'taco\\s+bell', r'tacobell'),  # \"Taco Bell\" -> \"TacoBell\"\n",
    "                ],\n",
    "                'food_type_patterns': [\n",
    "                    (r'(\\w+)\\s+pizzeria', r'\\1'),  # \"Domino's Pizzeria\" -> \"Domino's\"\n",
    "                    (r'(\\w+)\\s+steakhouse', r'\\1'),  # \"Outback Steakhouse\" -> \"Outback\"\n",
    "                    (r'(\\w+)\\s+sushi', r'\\1'),  # \"Tokyo Sushi\" -> \"Tokyo\"\n",
    "                    (r'(\\w+)\\s+bakery', r'\\1'),  # \"Panera Bakery\" -> \"Panera\"\n",
    "                    (r'(\\w+)\\s+bbq', r'\\1'),  # \"Famous Dave's BBQ\" -> \"Famous Dave's\"\n",
    "                    (r'(\\w+)\\s+taco', r'\\1'),  # \"Chronic Taco\" -> \"Chronic\"\n",
    "                ],\n",
    "            },\n",
    "            'hotel': {\n",
    "                'property_patterns': [\n",
    "                    (r'(\\w+)\\s+hotel', r'\\1'),  # \"Marriott Hotel\" -> \"Marriott\"\n",
    "                    (r'(\\w+)\\s+inn', r'\\1'),  # \"Holiday Inn\" -> \"Holiday\"\n",
    "                    (r'(\\w+)\\s+suites', r'\\1'),  # \"Comfort Suites\" -> \"Comfort\"\n",
    "                    (r'(\\w+)\\s+resort', r'\\1'),  # \"Wynn Resort\" -> \"Wynn\"\n",
    "                    (r'(\\w+)\\s+lodge', r'\\1'),  # \"Pine Lodge\" -> \"Pine\"\n",
    "                    (r'(\\w+)\\s+motel', r'\\1'),  # \"Super 8 Motel\" -> \"Super 8\"\n",
    "                    (r'(\\w+)\\s+&\\s+suites', r'\\1'),  # \"Hampton & Suites\" -> \"Hampton\"\n",
    "                    (r'(\\w+)\\s+hotel\\s+&\\s+resort', r'\\1'),  # \"Hilton Hotel & Resort\" -> \"Hilton\"\n",
    "                    (r'(\\w+)\\s+by\\s+(\\w+)', r'\\2'),  # \"Fairfield by Marriott\" -> \"Marriott\"\n",
    "                ],\n",
    "                'chain_patterns': [\n",
    "                    (r'hyatt\\s+regency', r'hyatt'),  # \"Hyatt Regency\" -> \"Hyatt\"\n",
    "                    (r'marriott\\s+courtyard', r'marriott'),  # \"Marriott Courtyard\" -> \"Marriott\"\n",
    "                    (r'hilton\\s+garden\\s+inn', r'hilton'),  # \"Hilton Garden Inn\" -> \"Hilton\"\n",
    "                    (r'holiday\\s+inn\\s+express', r'holiday\\s+inn'),  # \"Holiday Inn Express\" -> \"Holiday Inn\"\n",
    "                    (r'four\\s+seasons', r'four\\s+seasons'),  # Preserve \"Four Seasons\"\n",
    "                    (r'best\\s+western\\s+plus', r'best\\s+western'),  # \"Best Western Plus\" -> \"Best Western\"\n",
    "                ],\n",
    "            },\n",
    "            'gas_station': {\n",
    "                'station_patterns': [\n",
    "                    (r'(\\w+)\\s+gas', r'\\1'),  # \"Shell Gas\" -> \"Shell\"\n",
    "                    (r'(\\w+)\\s+gas\\s+station', r'\\1'),  # \"Chevron Gas Station\" -> \"Chevron\"\n",
    "                    (r'(\\w+)\\s+fuel', r'\\1'),  # \"BP Fuel\" -> \"BP\"\n",
    "                    (r'(\\w+)\\s+oil', r'\\1'),  # \"Mobil Oil\" -> \"Mobil\"\n",
    "                    (r'(\\w+)\\s+service\\s+station', r'\\1'),  # \"Texaco Service Station\" -> \"Texaco\"\n",
    "                    (r'(\\w+)\\s+petroleum', r'\\1'),  # \"Phillips Petroleum\" -> \"Phillips\"\n",
    "                ],\n",
    "                'convenience_patterns': [\n",
    "                    (r'(\\w+)\\s+convenience', r'\\1'),  # \"7-Eleven Convenience\" -> \"7-Eleven\"\n",
    "                    (r'(\\w+)\\s+mart', r'\\1'),  # \"Speedway Mart\" -> \"Speedway\"\n",
    "                    (r'(\\w+)\\s+corner\\s+store', r'\\1'),  # \"Shell Corner Store\" -> \"Shell\"\n",
    "                ],\n",
    "            },\n",
    "            'general': {\n",
    "                'location_prefixes': [\n",
    "                    (r'(north|south|east|west|downtown|midtown|uptown|central)\\s+(\\w+)', r'\\2'),\n",
    "                    (r'(\\w+)\\s+(north|south|east|west|downtown|midtown|uptown|central)', r'\\1'),\n",
    "                    (r'(n|s|e|w)\\s+(\\w+)', r'\\2'),  # \"N Target\" -> \"Target\"\n",
    "                    (r'(\\w+)\\s+(n|s|e|w)', r'\\1'),  # \"Target S\" -> \"Target\"\n",
    "                ],\n",
    "                'franchise_patterns': [\n",
    "                    (r'(\\w+)\\s+franchise', r'\\1'),\n",
    "                    (r'(\\w+)\\s+franchisee', r'\\1'),\n",
    "                    (r'(\\w+)\\s+franchisor', r'\\1'),\n",
    "                    (r'(\\w+)\\s+licensed', r'\\1'),\n",
    "                ],\n",
    "                'subsidiary_patterns': [\n",
    "                    (r'(\\w+),\\s+a\\s+(\\w+)\\s+company', r'\\1'),\n",
    "                    (r'(\\w+),\\s+subsidiary\\s+of\\s+(\\w+)', r'\\1'),\n",
    "                    (r'(\\w+)\\s+division\\s+of\\s+(\\w+)', r'\\1'),\n",
    "                    (r'(\\w+)\\s+by\\s+(\\w+)', r'\\1'),\n",
    "                    (r'(\\w+)\\s+owned\\s+by\\s+(\\w+)', r'\\1'),\n",
    "                ],\n",
    "                'numbered_locations': [\n",
    "                    (r'(\\w+)\\s+\\d+', r'\\1'),  # \"Starbucks 123\" -> \"Starbucks\"\n",
    "                    (r'(\\w+)\\s+no\\.?\\s*\\d+', r'\\1'),  # \"McDonald's No. 456\" -> \"McDonald's\"\n",
    "                    (r'(\\w+)\\s+number\\s*\\d+', r'\\1'),  # \"Subway Number 789\" -> \"Subway\"\n",
    "                    (r'(\\w+)\\s+store\\s*\\d+', r'\\1'),  # \"Target Store 101\" -> \"Target\"\n",
    "                ],\n",
    "                'special_character_patterns': [\n",
    "                    (r'(\\w+)\\s*[&@]\\s*(\\w+)', r'\\1 and \\2'),  # \"AT&T\" -> \"AT and T\", \"M@cys\" -> \"M and cys\"\n",
    "                    (r'(\\w+)\\s*\\+\\s*(\\w+)', r'\\1 plus \\2'),  # \"Bed+Bath\" -> \"Bed plus Bath\"\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def _load_known_equivalents(self):\n",
    "        \"\"\"Load known equivalent merchant names\"\"\"\n",
    "        return {\n",
    "            # Banking and financial\n",
    "            'bofa': ['bank of america', 'bankofamerica', 'bank america', 'b of a'],\n",
    "            'chase': ['jpmorgan chase', 'jp morgan', 'chase bank', 'jpmorgan'],\n",
    "            'wells fargo': ['wf', 'wellsfargo', 'wells'],\n",
    "            'citi': ['citibank', 'citigroup', 'citicorp'],\n",
    "            'amex': ['american express', 'americanexpress'],\n",
    "            'discover': ['discover card', 'discover financial'],\n",
    "            'capital one': ['capitalone', 'cap1', 'cap one'],\n",
    "            \n",
    "            # Retail\n",
    "            'walmart': ['wal-mart', 'wal mart', 'wmt', 'walmart supercenter', 'walmart neighborhood market'],\n",
    "            'target': ['target store', 'super target', 'target superstore'],\n",
    "            'costco': ['costco wholesale', 'costco warehouse', 'price costco'],\n",
    "            'amazon': ['amazon.com', 'amazon marketplace', 'amzn'],\n",
    "            'home depot': ['thd', 'the home depot', 'homedepot'],\n",
    "            'lowes': ['lowes home improvement', 'lowe\\'s'],\n",
    "            'best buy': ['bestbuy', 'bby'],\n",
    "            'cvs': ['cvs pharmacy', 'cvs caremark', 'cvs health'],\n",
    "            'walgreens': ['walgreen', 'wag'],\n",
    "            \n",
    "            # Fast food and restaurants\n",
    "            'mcdonalds': ['mcdonald\\'s', 'mcd', 'mcds', 'micky ds'],\n",
    "            'starbucks': ['starbucks coffee', 'sbux', 'starbucks coffee company'],\n",
    "            'subway': ['subway restaurant', 'subway sandwiches'],\n",
    "            'taco bell': ['tacobell', 'bell'],\n",
    "            'burger king': ['bk', 'burgerking'],\n",
    "            'wendys': ['wendy\\'s', 'wendys old fashioned hamburgers'],\n",
    "            'kfc': ['kentucky fried chicken', 'kentucky fried'],\n",
    "            'chipotle': ['chipotle mexican grill', 'cmg'],\n",
    "            'dominos': ['domino\\'s pizza', 'dominos pizza'],\n",
    "            'pizza hut': ['pizzahut', 'the hut'],\n",
    "            \n",
    "            # Gas stations\n",
    "            'shell': ['shell oil', 'shell gas', 'shell gas station'],\n",
    "            'exxon': ['exxonmobil', 'exxon mobil', 'esso'],\n",
    "            'bp': ['british petroleum', 'bp gas', 'bp gas station'],\n",
    "            'chevron': ['chevron gas', 'chevron gas station', 'chevron texaco'],\n",
    "            'mobil': ['mobil gas', 'mobil gas station', 'exxon mobil'],\n",
    "            \n",
    "            # Hotels\n",
    "            'marriott': ['marriott hotel', 'marriott international', 'jw marriott'],\n",
    "            'hilton': ['hilton hotel', 'hilton worldwide', 'hilton hotels'],\n",
    "            'hyatt': ['hyatt hotel', 'hyatt regency', 'grand hyatt'],\n",
    "            'holiday inn': ['holiday inn express', 'ihg', 'holiday'],\n",
    "            'sheraton': ['sheraton hotel', 'sheraton resort'],\n",
    "            \n",
    "            # Tech companies\n",
    "            'apple': ['apple store', 'apple inc', 'apple computer'],\n",
    "            'microsoft': ['msft', 'microsoft store', 'microsoft corporation'],\n",
    "            'google': ['google inc', 'alphabet', 'google play'],\n",
    "            'facebook': ['fb', 'meta', 'facebook inc'],\n",
    "            'amazon': ['amazon web services', 'aws', 'amazon.com'],\n",
    "        }\n",
    "    \n",
    "    def _load_abbreviation_patterns(self):\n",
    "        \"\"\"Load patterns for handling common abbreviations in merchant names\"\"\"\n",
    "        return {\n",
    "            # Business type abbreviations\n",
    "            'inc': ['incorporated', 'incorporation'],\n",
    "            'corp': ['corporation'],\n",
    "            'co': ['company'],\n",
    "            'ltd': ['limited'],\n",
    "            'llc': ['limited liability company'],\n",
    "            'lp': ['limited partnership'],\n",
    "            'intl': ['international'],\n",
    "            'assoc': ['associates', 'association'],\n",
    "            'svcs': ['services'],\n",
    "            'mgmt': ['management'],\n",
    "            'tech': ['technology', 'technologies'],\n",
    "            'sys': ['systems', 'system'],\n",
    "            'sol': ['solutions', 'solution'],\n",
    "            \n",
    "            # Industry abbreviations\n",
    "            'ctr': ['center'],\n",
    "            'dept': ['department'],\n",
    "            'pharm': ['pharmacy', 'pharmaceutical'],\n",
    "            'rest': ['restaurant'],\n",
    "            'cafe': ['cafeteria', 'caf'],\n",
    "            'mkt': ['market'],\n",
    "            'groc': ['grocery', 'groceries'],\n",
    "            'disc': ['discount'],\n",
    "            'whs': ['warehouse'],\n",
    "            \n",
    "            # Location abbreviations\n",
    "            'st': ['street'],\n",
    "            'ave': ['avenue'],\n",
    "            'blvd': ['boulevard'],\n",
    "            'rd': ['road'],\n",
    "            'hwy': ['highway'],\n",
    "            'cty': ['city'],\n",
    "            'twn': ['town'],\n",
    "            'vlg': ['village'],\n",
    "            'plz': ['plaza'],\n",
    "            'sq': ['square'],\n",
    "            'stn': ['station'],\n",
    "            \n",
    "            # Direction abbreviations\n",
    "            'n': ['north'],\n",
    "            's': ['south'],\n",
    "            'e': ['east'],\n",
    "            'w': ['west'],\n",
    "            'ne': ['northeast'],\n",
    "            'nw': ['northwest'],\n",
    "            'se': ['southeast'],\n",
    "            'sw': ['southwest'],\n",
    "        }\n",
    "    \n",
    "    def _compile_regex_patterns(self):\n",
    "        \"\"\"Compile all regex patterns for faster matching\"\"\"\n",
    "        self.compiled_patterns = {}\n",
    "        \n",
    "        for domain, pattern_groups in self.common_merchant_patterns.items():\n",
    "            self.compiled_patterns[domain] = {}\n",
    "            \n",
    "            for pattern_type, patterns in pattern_groups.items():\n",
    "                self.compiled_patterns[domain][pattern_type] = [\n",
    "                    (re.compile(pattern, re.IGNORECASE), replacement)\n",
    "                    for pattern, replacement in patterns\n",
    "                ]\n",
    "    \n",
    "    def _initialize_aho_corasick(self):\n",
    "        \"\"\"Initialize Aho-Corasick automaton for fast pattern matching of known equivalents\"\"\"\n",
    "        if not aho_corasick_available:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            self.aho_corasick_finder = pyahocorasick.Automaton()\n",
    "            \n",
    "            # Add all merchant names and their equivalents\n",
    "            for key, equivalents in self.known_equivalents.items():\n",
    "                for eq in equivalents:\n",
    "                    self.aho_corasick_finder.add_str(eq, (key, eq))\n",
    "                self.aho_corasick_finder.add_str(key, (key, key))\n",
    "                \n",
    "            # Finalize the automaton\n",
    "            self.aho_corasick_finder.make_automaton()\n",
    "            logger.info(\"Aho-Corasick automaton initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Aho-Corasick automaton: {e}\")\n",
    "            self.aho_corasick_finder = None\n",
    "    \n",
    "    def detect_merchant_patterns(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Detect merchant name patterns in a pair of strings\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First string\n",
    "            s2 (str): Second string\n",
    "            domain (str, optional): Domain for specialized pattern detection\n",
    "            \n",
    "        Returns:\n",
    "            dict: Detected patterns and their confidence scores\n",
    "        \"\"\"\n",
    "        if not s1 or not s2:\n",
    "            return {}\n",
    "            \n",
    "        # Preprocess the strings\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty after preprocessing\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return {}\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # First check for known equivalents - highest confidence\n",
    "        equiv_match = self._detect_known_equivalents(s1_clean, s2_clean)\n",
    "        if equiv_match:\n",
    "            patterns['known_equivalent'] = {\n",
    "                'pattern': equiv_match,\n",
    "                'confidence': 0.95,\n",
    "                'explanation': f\"Known equivalent pair found: '{equiv_match[0]}' and '{equiv_match[1]}'\"\n",
    "            }\n",
    "            # If known equivalents found, we can return immediately as it's high confidence\n",
    "            return patterns\n",
    "        \n",
    "        # Check for abbreviation patterns\n",
    "        abbrev_patterns = self._detect_abbreviation_patterns(s1_clean, s2_clean)\n",
    "        if abbrev_patterns:\n",
    "            patterns['abbreviation'] = {\n",
    "                'pattern': abbrev_patterns,\n",
    "                'confidence': 0.85,\n",
    "                'explanation': f\"Abbreviation pattern detected: {abbrev_patterns}\"\n",
    "            }\n",
    "        \n",
    "        # Detect domain-specific patterns\n",
    "        if domain:\n",
    "            # Check specific domain patterns if provided\n",
    "            if domain.lower() in self.common_merchant_patterns:\n",
    "                domain_patterns = self._detect_domain_specific_patterns(s1_clean, s2_clean, domain.lower())\n",
    "                if domain_patterns:\n",
    "                    patterns[f'{domain.lower()}_specific'] = {\n",
    "                        'pattern': domain_patterns,\n",
    "                        'confidence': 0.9,\n",
    "                        'explanation': f\"{domain} specific pattern detected: {domain_patterns}\"\n",
    "                    }\n",
    "        else:\n",
    "            # Try all domains if not specified\n",
    "            for domain_name in self.common_merchant_patterns:\n",
    "                if domain_name == 'general':\n",
    "                    continue  # Skip general, we'll check it separately\n",
    "                \n",
    "                domain_patterns = self._detect_domain_specific_patterns(s1_clean, s2_clean, domain_name)\n",
    "                if domain_patterns:\n",
    "                    patterns[f'{domain_name}_pattern'] = {\n",
    "                        'pattern': domain_patterns,\n",
    "                        'confidence': 0.8,\n",
    "                        'explanation': f\"{domain_name} pattern detected: {domain_patterns}\"\n",
    "                    }\n",
    "        \n",
    "        # Always check general patterns\n",
    "        general_patterns = self._detect_domain_specific_patterns(s1_clean, s2_clean, 'general')\n",
    "        if general_patterns:\n",
    "            patterns['general_pattern'] = {\n",
    "                'pattern': general_patterns,\n",
    "                'confidence': 0.7,\n",
    "                'explanation': f\"General pattern detected: {general_patterns}\"\n",
    "            }\n",
    "        \n",
    "        # Detect character-level patterns\n",
    "        char_patterns = self._detect_character_patterns(s1_clean, s2_clean)\n",
    "        if char_patterns:\n",
    "            patterns['character_pattern'] = {\n",
    "                'pattern': char_patterns,\n",
    "                'confidence': 0.65,\n",
    "                'explanation': f\"Character-level pattern detected: {char_patterns}\"\n",
    "            }\n",
    "        \n",
    "        # Detect grammatical structure patterns\n",
    "        structure_patterns = self._detect_structure_patterns(s1_clean, s2_clean)\n",
    "        if structure_patterns:\n",
    "            patterns['structure_pattern'] = {\n",
    "                'pattern': structure_patterns,\n",
    "                'confidence': 0.75,\n",
    "                'explanation': f\"Grammatical structure pattern detected: {structure_patterns}\"\n",
    "            }\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _detect_known_equivalents(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Check if the merchant names are known equivalents using optimized matching\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First preprocessed string\n",
    "            s2 (str): Second preprocessed string\n",
    "            \n",
    "        Returns:\n",
    "            tuple or None: Tuple of (canonical_name, equivalent) if found, otherwise None\n",
    "        \"\"\"\n",
    "        # Use Aho-Corasick for fast matching if available\n",
    "        if self.aho_corasick_finder:\n",
    "            # Find matches in both strings\n",
    "            s1_matches = list(self.aho_corasick_finder.iter(s1))\n",
    "            s2_matches = list(self.aho_corasick_finder.iter(s2))\n",
    "            \n",
    "            # Check if we have matches in both strings\n",
    "            if s1_matches and s2_matches:\n",
    "                # Get canonical names from matches\n",
    "                s1_canonicals = set(match[1][0] for match in s1_matches)\n",
    "                s2_canonicals = set(match[1][0] for match in s2_matches)\n",
    "                \n",
    "                # Check for intersection\n",
    "                common_canonicals = s1_canonicals.intersection(s2_canonicals)\n",
    "                if common_canonicals:\n",
    "                    # Return the first common canonical name and the original strings\n",
    "                    return (next(iter(common_canonicals)), (s1, s2))\n",
    "        \n",
    "        # Fallback to dictionary lookup if Aho-Corasick is not available\n",
    "        for canonical, equivalents in self.known_equivalents.items():\n",
    "            # Check if both strings are related to the same canonical name\n",
    "            s1_matches = canonical == s1 or s1 in equivalents\n",
    "            s2_matches = canonical == s2 or s2 in equivalents\n",
    "            \n",
    "            if s1_matches and s2_matches:\n",
    "                return (canonical, (s1, s2))\n",
    "            \n",
    "            # Try fuzzy matching for each equivalent\n",
    "            all_variants = equivalents + [canonical]\n",
    "            for variant in all_variants:\n",
    "                jw_s1 = self.similarity_algorithms.jaro_winkler_similarity(variant, s1)\n",
    "                jw_s2 = self.similarity_algorithms.jaro_winkler_similarity(variant, s2)\n",
    "                \n",
    "                # If high similarity with both strings, they're likely equivalent\n",
    "                if jw_s1 > 0.85 and jw_s2 > 0.85:\n",
    "                    return (canonical, (s1, s2))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _detect_abbreviation_patterns(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Detect if one string is an abbreviation of the other\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First preprocessed string\n",
    "            s2 (str): Second preprocessed string\n",
    "            \n",
    "        Returns:\n",
    "            dict or None: Abbreviation pattern details if found, otherwise None\n",
    "        \"\"\"\n",
    "        # Check for common abbreviation patterns\n",
    "        s1_words = s1.split()\n",
    "        s2_words = s2.split()\n",
    "        \n",
    "        abbrev_found = False\n",
    "        expanded_found = False\n",
    "        abbrev_word = None\n",
    "        expanded_word = None\n",
    "        \n",
    "        # Check each word against known abbreviations\n",
    "        for word in s1_words:\n",
    "            if word in self.abbreviation_patterns:\n",
    "                # Check if any expanded form exists in s2\n",
    "                for expanded in self.abbreviation_patterns[word]:\n",
    "                    if expanded in s2 or any(expanded in w for w in s2_words):\n",
    "                        abbrev_found = True\n",
    "                        abbrev_word = word\n",
    "                        expanded_word = expanded\n",
    "                        break\n",
    "        \n",
    "        # Check in reverse direction too\n",
    "        if not abbrev_found:\n",
    "            for word in s2_words:\n",
    "                if word in self.abbreviation_patterns:\n",
    "                    # Check if any expanded form exists in s1\n",
    "                    for expanded in self.abbreviation_patterns[word]:\n",
    "                        if expanded in s1 or any(expanded in w for w in s1_words):\n",
    "                            abbrev_found = True\n",
    "                            abbrev_word = word\n",
    "                            expanded_word = expanded\n",
    "                            break\n",
    "        \n",
    "        if abbrev_found:\n",
    "            return {\n",
    "                'type': 'abbreviation_expansion',\n",
    "                'abbreviation': abbrev_word,\n",
    "                'expansion': expanded_word,\n",
    "                'direction': 's1_to_s2' if abbrev_word in s1_words else 's2_to_s1'\n",
    "            }\n",
    "        \n",
    "        # Check for acronyms (first letters of words)\n",
    "        # See if one string is an acronym of the other\n",
    "        s1_is_acronym = len(s1_words) == 1 and len(s1.replace('.', '')) <= 5\n",
    "        s2_is_acronym = len(s2_words) == 1 and len(s2.replace('.', '')) <= 5\n",
    "        \n",
    "        if s1_is_acronym and not s2_is_acronym:\n",
    "            # Check if s1 is acronym of s2\n",
    "            first_letters = ''.join([word[0] for word in s2_words if word]).lower()\n",
    "            if s1.lower() == first_letters:\n",
    "                return {\n",
    "                    'type': 'acronym_expansion',\n",
    "                    'acronym': s1,\n",
    "                    'expansion': s2,\n",
    "                    'direction': 's1_to_s2'\n",
    "                }\n",
    "        elif s2_is_acronym and not s1_is_acronym:\n",
    "            # Check if s2 is acronym of s1\n",
    "            first_letters = ''.join([word[0] for word in s1_words if word]).lower()\n",
    "            if s2.lower() == first_letters:\n",
    "                return {\n",
    "                    'type': 'acronym_expansion',\n",
    "                    'acronym': s2,\n",
    "                    'expansion': s1,\n",
    "                    'direction': 's2_to_s1'\n",
    "                }\n",
    "        \n",
    "        # Check for partial abbreviations (like \"Intl\" for \"International\")\n",
    "        for i, word1 in enumerate(s1_words):\n",
    "            for j, word2 in enumerate(s2_words):\n",
    "                # Skip very short words or exact matches\n",
    "                if len(word1) < 3 or len(word2) < 3 or word1 == word2:\n",
    "                    continue\n",
    "                \n",
    "                # Check if one word is a prefix of the other\n",
    "                if (word1.startswith(word2) or word2.startswith(word1)) and abs(len(word1) - len(word2)) > 2:\n",
    "                    shorter = word1 if len(word1) < len(word2) else word2\n",
    "                    longer = word2 if len(word1) < len(word2) else word1\n",
    "                    \n",
    "                    # Check if shorter is at least 3 chars and longer is at least 50% longer\n",
    "                    if len(shorter) >= 3 and len(longer) > len(shorter) * 1.5:\n",
    "                        return {\n",
    "                            'type': 'partial_abbreviation',\n",
    "                            'abbreviation': shorter,\n",
    "                            'expansion': longer,\n",
    "                            'direction': 's1_to_s2' if shorter == word1 else 's2_to_s1'\n",
    "                        }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _detect_domain_specific_patterns(self, s1, s2, domain):\n",
    "        \"\"\"\n",
    "        Detect industry-specific patterns based on the provided domain\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First preprocessed string\n",
    "            s2 (str): Second preprocessed string\n",
    "            domain (str): Domain for pattern detection\n",
    "            \n",
    "        Returns:\n",
    "            list: List of detected patterns for the domain\n",
    "        \"\"\"\n",
    "        if domain not in self.common_merchant_patterns:\n",
    "            return []\n",
    "        \n",
    "        detected_patterns = []\n",
    "        \n",
    "        for pattern_type, patterns in self.compiled_patterns[domain].items():\n",
    "            for pattern, replacement in patterns:\n",
    "                # Check if pattern matches first string\n",
    "                s1_match = pattern.search(s1)\n",
    "                if s1_match:\n",
    "                    s1_normalized = pattern.sub(replacement, s1)\n",
    "                    # Check if normalized version now matches second string (exact or high similarity)\n",
    "                    if s1_normalized == s2 or self.similarity_algorithms.jaro_winkler_similarity(s1_normalized, s2) > 0.85:\n",
    "                        detected_patterns.append({\n",
    "                            'domain': domain,\n",
    "                            'pattern_type': pattern_type,\n",
    "                            'original': s1,\n",
    "                            'normalized': s1_normalized,\n",
    "                            'match': s2,\n",
    "                            'regex': pattern.pattern\n",
    "                        })\n",
    "                \n",
    "                # Check in reverse direction\n",
    "                s2_match = pattern.search(s2)\n",
    "                if s2_match:\n",
    "                    s2_normalized = pattern.sub(replacement, s2)\n",
    "                    # Check if normalized version now matches first string (exact or high similarity)\n",
    "                    if s2_normalized == s1 or self.similarity_algorithms.jaro_winkler_similarity(s2_normalized, s1) > 0.85:\n",
    "                        detected_patterns.append({\n",
    "                            'domain': domain,\n",
    "                            'pattern_type': pattern_type,\n",
    "                            'original': s2,\n",
    "                            'normalized': s2_normalized,\n",
    "                            'match': s1,\n",
    "                            'regex': pattern.pattern\n",
    "                        })\n",
    "        \n",
    "        return detected_patterns\n",
    "    \n",
    "    def _detect_character_patterns(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Detect character-level patterns like typos, character substitutions, etc.\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First preprocessed string\n",
    "            s2 (str): Second preprocessed string\n",
    "            \n",
    "        Returns:\n",
    "            list: List of detected character-level patterns\n",
    "        \"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        # Detect common character substitutions\n",
    "        char_subs = {\n",
    "            'zero_o': ('0', 'o'),\n",
    "            'one_l': ('1', 'l'),\n",
    "            'at_a': ('@', 'a'),\n",
    "            'plus_t': ('+', 't'),\n",
    "            'ampersand_and': ('&', 'and'),\n",
    "            'dollar_s': ('$', 's'),\n",
    "            'exclamation_i': ('!', 'i'),\n",
    "        }\n",
    "        \n",
    "        for sub_name, (char1, char2) in char_subs.items():\n",
    "            # Check if s1 contains char1 and s2 contains char2 or vice versa\n",
    "            if (char1 in s1 and char2 in s2) or (char2 in s1 and char1 in s2):\n",
    "                # Create normalized versions by replacing the characters\n",
    "                s1_norm = s1.replace(char1, char2).replace(char2, char2)\n",
    "                s2_norm = s2.replace(char1, char2).replace(char2, char2)\n",
    "                \n",
    "                # See if normalized versions are more similar\n",
    "                orig_sim = self.similarity_algorithms.jaro_winkler_similarity(s1, s2)\n",
    "                norm_sim = self.similarity_algorithms.jaro_winkler_similarity(s1_norm, s2_norm)\n",
    "                \n",
    "                if norm_sim > orig_sim:\n",
    "                    patterns.append({\n",
    "                        'type': 'character_substitution',\n",
    "                        'subtype': sub_name,\n",
    "                        'original_similarity': orig_sim,\n",
    "                        'normalized_similarity': norm_sim,\n",
    "                        'improvement': norm_sim - orig_sim\n",
    "                    })\n",
    "        \n",
    "        # Detect transpositions (swapped adjacent characters)\n",
    "        s1_chars = list(s1)\n",
    "        for i in range(len(s1_chars) - 1):\n",
    "            # Try swapping adjacent characters\n",
    "            swapped = s1_chars.copy()\n",
    "            swapped[i], swapped[i+1] = swapped[i+1], swapped[i]\n",
    "            s1_swapped = ''.join(swapped)\n",
    "            \n",
    "            # Check if swapped version is more similar to s2\n",
    "            orig_sim = self.similarity_algorithms.jaro_winkler_similarity(s1, s2)\n",
    "            swap_sim = self.similarity_algorithms.jaro_winkler_similarity(s1_swapped, s2)\n",
    "            \n",
    "            if swap_sim > orig_sim + 0.1:  # Significant improvement\n",
    "                patterns.append({\n",
    "                    'type': 'transposition',\n",
    "                    'position': i,\n",
    "                    'original': s1,\n",
    "                    'swapped': s1_swapped,\n",
    "                    'original_similarity': orig_sim,\n",
    "                    'swapped_similarity': swap_sim\n",
    "                })\n",
    "        \n",
    "        # Detect common typos in merchant names\n",
    "        typo_corrections = {\n",
    "            'walmart': ['wallmart', 'walmark', 'walmrt'],\n",
    "            'target': ['targat', 'tarket', 'targt'],\n",
    "            'starbucks': ['starbuks', 'starbuck', 'startbucks'],\n",
    "            'mcdonalds': ['macdonalds', 'mcdonlds', 'medonalds'],\n",
    "            'amazon': ['amazan', 'amizon', 'amazom'],\n",
    "            'costco': ['cosco', 'casco', 'cotsco'],\n",
    "            'pizza': ['piza', 'pizzza', 'pizaa'],\n",
    "            'restaurant': ['resturant', 'restrant', 'restarant']\n",
    "        }\n",
    "        \n",
    "        for correct, typos in typo_corrections.items():\n",
    "            if correct in s1 and any(typo in s2 for typo in typos):\n",
    "                patterns.append({\n",
    "                    'type': 'common_typo',\n",
    "                    'correct': correct,\n",
    "                    'typo': next((typo for typo in typos if typo in s2), None),\n",
    "                    'direction': 's1_to_s2'\n",
    "                })\n",
    "            elif correct in s2 and any(typo in s1 for typo in typos):\n",
    "                patterns.append({\n",
    "                    'type': 'common_typo',\n",
    "                    'correct': correct,\n",
    "                    'typo': next((typo for typo in typos if typo in s1), None),\n",
    "                    'direction': 's2_to_s1'\n",
    "                })\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _detect_structure_patterns(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Detect grammatical structure patterns between merchant names\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First preprocessed string\n",
    "            s2 (str): Second preprocessed string\n",
    "            \n",
    "        Returns:\n",
    "            list: List of detected structural patterns\n",
    "        \"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        # Check for word order differences\n",
    "        s1_words = set(s1.split())\n",
    "        s2_words = set(s2.split())\n",
    "        \n",
    "        # If the sets of words are the same but the strings are different,\n",
    "        # then it's a word order difference\n",
    "        if s1_words == s2_words and s1 != s2:\n",
    "            patterns.append({\n",
    "                'type': 'word_order',\n",
    "                'words': sorted(list(s1_words)),\n",
    "                'original_s1': s1,\n",
    "                'original_s2': s2\n",
    "            })\n",
    "        \n",
    "        # Check for compound vs. separate words\n",
    "        # e.g., \"walmart\" vs \"wal mart\"\n",
    "        s1_nospace = s1.replace(' ', '')\n",
    "        s2_nospace = s2.replace(' ', '')\n",
    "        \n",
    "        if s1_nospace == s2_nospace and s1 != s2:\n",
    "            patterns.append({\n",
    "                'type': 'compound_vs_separate',\n",
    "                'compound': s1 if ' ' not in s1 else s2,\n",
    "                'separate': s2 if ' ' in s2 else s1\n",
    "            })\n",
    "        \n",
    "        # Check for possessive forms\n",
    "        # e.g., \"McDonald's\" vs \"McDonalds\"\n",
    "        s1_noposs = s1.replace('\\'s', 's')\n",
    "        s2_noposs = s2.replace('\\'s', 's')\n",
    "        \n",
    "        if s1_noposs == s2_noposs and s1 != s2:\n",
    "            patterns.append({\n",
    "                'type': 'possessive_form',\n",
    "                'with_apostrophe': s1 if '\\'' in s1 else s2,\n",
    "                'without_apostrophe': s2 if '\\'' not in s2 else s1\n",
    "            })\n",
    "        \n",
    "        # Check for plural vs. singular forms\n",
    "        singular_endings = ['s', 'es', 'ies']\n",
    "        for word1 in s1.split():\n",
    "            for word2 in s2.split():\n",
    "                # Skip short words\n",
    "                if len(word1) < 4 or len(word2) < 4:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if one is a plural of the other\n",
    "                for ending in singular_endings:\n",
    "                    if word1 == word2 + ending:\n",
    "                        patterns.append({\n",
    "                            'type': 'plural_singular',\n",
    "                            'plural': word1,\n",
    "                            'singular': word2,\n",
    "                            'direction': 's1_to_s2'\n",
    "                        })\n",
    "                    elif word2 == word1 + ending:\n",
    "                        patterns.append({\n",
    "                            'type': 'plural_singular',\n",
    "                            'plural': word2,\n",
    "                            'singular': word1,\n",
    "                            'direction': 's2_to_s1'\n",
    "                        })\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def get_canonical_name(self, merchant_name, domain=None):\n",
    "        \"\"\"\n",
    "        Get the canonical name for a merchant name\n",
    "        \n",
    "        Args:\n",
    "            merchant_name (str): Merchant name to normalize\n",
    "            domain (str, optional): Domain for specialized processing\n",
    "            \n",
    "        Returns:\n",
    "            str: Canonical merchant name\n",
    "        \"\"\"\n",
    "        if not merchant_name:\n",
    "            return \"\"\n",
    "            \n",
    "        # Preprocess the input\n",
    "        clean_name = self.preprocessor.preprocess(merchant_name, domain)\n",
    "        \n",
    "        # Check for known equivalents\n",
    "        if self.aho_corasick_finder:\n",
    "            matches = list(self.aho_corasick_finder.iter(clean_name))\n",
    "            if matches:\n",
    "                # Get the canonical name from the longest match\n",
    "                longest_match = max(matches, key=lambda x: x[0] - x[1][1].find(clean_name))\n",
    "                return longest_match[1][0]  # Return canonical name\n",
    "        \n",
    "        # Fallback to dictionary lookup\n",
    "        for canonical, equivalents in self.known_equivalents.items():\n",
    "            if clean_name == canonical or clean_name in equivalents:\n",
    "                return canonical\n",
    "            \n",
    "            # Try fuzzy matching\n",
    "            for variant in equivalents + [canonical]:\n",
    "                jw_sim = self.similarity_algorithms.jaro_winkler_similarity(variant, clean_name)\n",
    "                if jw_sim > 0.9:\n",
    "                    return canonical\n",
    "        \n",
    "        # Apply domain-specific normalization if domain is provided\n",
    "        if domain and domain.lower() in self.common_merchant_patterns:\n",
    "            # Try applying all patterns for this domain\n",
    "            for pattern_type, patterns in self.compiled_patterns[domain.lower()].items():\n",
    "                for pattern, replacement in patterns:\n",
    "                    if pattern.search(clean_name):\n",
    "                        normalized = pattern.sub(replacement, clean_name)\n",
    "                        if normalized != clean_name:\n",
    "                            return normalized\n",
    "        \n",
    "        # Apply general patterns\n",
    "        for pattern_type, patterns in self.compiled_patterns['general'].items():\n",
    "            for pattern, replacement in patterns:\n",
    "                if pattern.search(clean_name):\n",
    "                    normalized = pattern.sub(replacement, clean_name)\n",
    "                    if normalized != clean_name:\n",
    "                        return normalized\n",
    "        \n",
    "        # If no patterns matched, return the preprocessed name\n",
    "        return clean_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6781bcc4-f7a9-4d69-ad8f-03215fb940c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Advanced BERT-Based Semantic Analysis\n",
    "\n",
    "class BertSemanticAnalyzer:\n",
    "    \"\"\"\n",
    "    Advanced semantic analyzer using BERT and domain-specific tuning for\n",
    "    merchant name matching with enhanced accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_embedder=None, similarity_algorithms=None, preprocessor=None):\n",
    "        \"\"\"\n",
    "        Initialize semantic analyzer with BERT embedder and supporting algorithms\n",
    "        \n",
    "        Args:\n",
    "            bert_embedder: BERT embedder for semantic similarity\n",
    "            similarity_algorithms: Similarity algorithms for merchant matching\n",
    "            preprocessor: Merchant name preprocessor\n",
    "        \"\"\"\n",
    "        self.bert_embedder = bert_embedder or AdvancedBERTEmbedder()\n",
    "        self.similarity_algorithms = similarity_algorithms or SimilarityAlgorithms()\n",
    "        self.preprocessor = preprocessor or MerchantPreprocessor()\n",
    "        \n",
    "        # Thresholds for semantic analysis\n",
    "        self.high_similarity_threshold = 0.85\n",
    "        self.medium_similarity_threshold = 0.70\n",
    "        self.low_similarity_threshold = 0.55\n",
    "        \n",
    "        # Cached semantic clusters for merchant names\n",
    "        self.merchant_clusters = {}\n",
    "        self.cluster_embeddings = {}\n",
    "        \n",
    "        # Track if domain adaptation has been performed\n",
    "        self.adapted_to_domain = False\n",
    "    \n",
    "    def analyze_semantic_match(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Analyze the semantic match between two merchant names\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First merchant name\n",
    "            s2 (str): Second merchant name\n",
    "            domain (str, optional): Domain for specialized analysis\n",
    "            \n",
    "        Returns:\n",
    "            dict: Semantic analysis results\n",
    "        \"\"\"\n",
    "        if not s1 or not s2:\n",
    "            return {\n",
    "                'semantic_similarity': 0.0,\n",
    "                'match_level': 'no_match',\n",
    "                'analysis': 'Empty input provided'\n",
    "            }\n",
    "        \n",
    "        # Preprocess inputs\n",
    "        s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "        \n",
    "        # Check if strings are empty after preprocessing\n",
    "        if not s1_clean or not s2_clean:\n",
    "            return {\n",
    "                'semantic_similarity': 0.0,\n",
    "                'match_level': 'no_match',\n",
    "                'analysis': 'Empty strings after preprocessing'\n",
    "            }\n",
    "        \n",
    "        # Calculate semantic similarity\n",
    "        semantic_similarity = self.bert_embedder.compute_similarity(s1_clean, s2_clean)\n",
    "        \n",
    "        # Determine match level\n",
    "        match_level = self._get_match_level(semantic_similarity)\n",
    "        \n",
    "        # Get enhanced analysis\n",
    "        analysis = self._get_enhanced_analysis(s1_clean, s2_clean, semantic_similarity, domain)\n",
    "        \n",
    "        return {\n",
    "            'semantic_similarity': semantic_similarity,\n",
    "            'match_level': match_level,\n",
    "            'analysis': analysis\n",
    "        }\n",
    "    \n",
    "    def _get_match_level(self, similarity_score):\n",
    "        \"\"\"Determine the match level based on similarity score\"\"\"\n",
    "        if similarity_score >= self.high_similarity_threshold:\n",
    "            return 'high_match'\n",
    "        elif similarity_score >= self.medium_similarity_threshold:\n",
    "            return 'medium_match'\n",
    "        elif similarity_score >= self.low_similarity_threshold:\n",
    "            return 'low_match'\n",
    "        else:\n",
    "            return 'no_match'\n",
    "    \n",
    "    def _get_enhanced_analysis(self, s1, s2, similarity, domain=None):\n",
    "        \"\"\"\n",
    "        Get enhanced analysis of semantic match with domain-specific insights\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First preprocessed merchant name\n",
    "            s2 (str): Second preprocessed merchant name\n",
    "            similarity (float): Semantic similarity score\n",
    "            domain (str, optional): Domain for specialized analysis\n",
    "            \n",
    "        Returns:\n",
    "            dict: Enhanced analysis results\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'semantic_matching_points': [],\n",
    "            'context_similarity': 0.0,\n",
    "            'potential_relationship': None,\n",
    "            'confidence': 0.0\n",
    "        }\n",
    "        \n",
    "        # Analyze word-level matching\n",
    "        s1_words = s1.split()\n",
    "        s2_words = s2.split()\n",
    "        \n",
    "        matching_words = set(s1_words).intersection(set(s2_words))\n",
    "        \n",
    "        # Calculate word match ratio\n",
    "        s1_match_ratio = len(matching_words) / len(s1_words) if s1_words else 0\n",
    "        s2_match_ratio = len(matching_words) / len(s2_words) if s2_words else 0\n",
    "        \n",
    "        analysis['word_match_ratio'] = (s1_match_ratio + s2_match_ratio) / 2\n",
    "        analysis['matching_words'] = list(matching_words)\n",
    "        \n",
    "        # Calculate context similarity (how words are used together)\n",
    "        if len(s1_words) > 1 and len(s2_words) > 1:\n",
    "            s1_bigrams = set(zip(s1_words[:-1], s1_words[1:]))\n",
    "            s2_bigrams = set(zip(s2_words[:-1], s2_words[1:]))\n",
    "            \n",
    "            matching_bigrams = s1_bigrams.intersection(s2_bigrams)\n",
    "            \n",
    "            s1_bigram_ratio = len(matching_bigrams) / len(s1_bigrams) if s1_bigrams else 0\n",
    "            s2_bigram_ratio = len(matching_bigrams) / len(s2_bigrams) if s2_bigrams else 0\n",
    "            \n",
    "            analysis['context_similarity'] = (s1_bigram_ratio + s2_bigram_ratio) / 2\n",
    "        \n",
    "        # Identify semantic matching points (key concepts that match)\n",
    "        # Use BERT to identify important semantic units in each name\n",
    "        if hasattr(self.bert_embedder, 'model') and hasattr(self.bert_embedder.model, 'tokenizer'):\n",
    "            try:\n",
    "                # Get token contributions to semantic meaning\n",
    "                s1_tokens = self.bert_embedder.model.tokenizer.tokenize(s1)\n",
    "                s2_tokens = self.bert_embedder.model.tokenizer.tokenize(s2)\n",
    "                \n",
    "                # Filter out special tokens and find matching semantic tokens\n",
    "                s1_tokens = [t for t in s1_tokens if t not in ['[CLS]', '[SEP]']]\n",
    "                s2_tokens = [t for t in s2_tokens if t not in ['[CLS]', '[SEP]']]\n",
    "                \n",
    "                matching_tokens = set(s1_tokens).intersection(set(s2_tokens))\n",
    "                \n",
    "                # Add matching tokens as semantic matching points\n",
    "                analysis['semantic_matching_points'] = list(matching_tokens)\n",
    "            except:\n",
    "                # Fallback to word-level matching if tokenizer isn't available\n",
    "                analysis['semantic_matching_points'] = list(matching_words)\n",
    "        else:\n",
    "            # Fallback to word-level matching if tokenizer isn't available\n",
    "            analysis['semantic_matching_points'] = list(matching_words)\n",
    "        \n",
    "        # Determine potential relationship\n",
    "        if similarity >= self.high_similarity_threshold:\n",
    "            if s1 == s2:\n",
    "                analysis['potential_relationship'] = 'exact_match'\n",
    "                analysis['confidence'] = 0.99\n",
    "            elif len(s1) > len(s2) * 1.5:\n",
    "                analysis['potential_relationship'] = 'expansion'\n",
    "                analysis['confidence'] = 0.90\n",
    "            elif len(s2) > len(s1) * 1.5:\n",
    "                analysis['potential_relationship'] = 'abbreviation'\n",
    "                analysis['confidence'] = 0.90\n",
    "            else:\n",
    "                analysis['potential_relationship'] = 'variant'\n",
    "                analysis['confidence'] = 0.85\n",
    "        elif similarity >= self.medium_similarity_threshold:\n",
    "            if self.similarity_algorithms.acronym_similarity(s1, s2) > 0.7:\n",
    "                analysis['potential_relationship'] = 'acronym'\n",
    "                analysis['confidence'] = 0.80\n",
    "            elif self.similarity_algorithms.token_set_ratio(s1, s2) > 0.8:\n",
    "                analysis['potential_relationship'] = 'reordered'\n",
    "                analysis['confidence'] = 0.75\n",
    "            else:\n",
    "                analysis['potential_relationship'] = 'related'\n",
    "                analysis['confidence'] = 0.65\n",
    "        elif similarity >= self.low_similarity_threshold:\n",
    "            analysis['potential_relationship'] = 'loosely_related'\n",
    "            analysis['confidence'] = 0.50\n",
    "        else:\n",
    "            analysis['potential_relationship'] = 'unrelated'\n",
    "            analysis['confidence'] = 0.90\n",
    "        \n",
    "        # Add domain-specific analysis if domain is provided\n",
    "        if domain:\n",
    "            domain_analysis = self._get_domain_specific_analysis(s1, s2, domain)\n",
    "            if domain_analysis:\n",
    "                analysis['domain_specific'] = domain_analysis\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _get_domain_specific_analysis(self, s1, s2, domain):\n",
    "        \"\"\"\n",
    "        Get domain-specific analysis for semantic matching\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First preprocessed merchant name\n",
    "            s2 (str): Second preprocessed merchant name\n",
    "            domain (str): Domain for specialized analysis\n",
    "            \n",
    "        Returns:\n",
    "            dict: Domain-specific analysis or None if domain not supported\n",
    "        \"\"\"\n",
    "        domain_lower = domain.lower()\n",
    "        \n",
    "        if domain_lower == 'banking' or domain_lower == 'financial':\n",
    "            return self._analyze_financial_merchants(s1, s2)\n",
    "        elif domain_lower == 'retail':\n",
    "            return self._analyze_retail_merchants(s1, s2)\n",
    "        elif domain_lower == 'restaurant' or domain_lower == 'food':\n",
    "            return self._analyze_restaurant_merchants(s1, s2)\n",
    "        elif domain_lower == 'hotel' or domain_lower == 'hospitality':\n",
    "            return self._analyze_hotel_merchants(s1, s2)\n",
    "        elif domain_lower == 'gas_station' or domain_lower == 'fuel':\n",
    "            return self._analyze_gas_station_merchants(s1, s2)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _analyze_financial_merchants(self, s1, s2):\n",
    "        \"\"\"Specialized analysis for financial institutions\"\"\"\n",
    "        analysis = {\n",
    "            'industry': 'financial',\n",
    "            'subtype': None,\n",
    "            'notes': []\n",
    "        }\n",
    "        \n",
    "        # Detect bank type\n",
    "        bank_indicators = ['bank', 'credit union', 'financial', 'invest', 'capital', 'trust']\n",
    "        credit_indicators = ['card', 'credit', 'loan', 'lending', 'mortgage', 'finance']\n",
    "        payment_indicators = ['pay', 'wallet', 'transfer', 'remit', 'money']\n",
    "        \n",
    "        for indicator in bank_indicators:\n",
    "            if indicator in s1 or indicator in s2:\n",
    "                analysis['subtype'] = 'banking'\n",
    "                analysis['notes'].append(f\"Banking institution detected via keyword: '{indicator}'\")\n",
    "                break\n",
    "                \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in credit_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'credit'\n",
    "                    analysis['notes'].append(f\"Credit institution detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in payment_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'payment'\n",
    "                    analysis['notes'].append(f\"Payment service detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        # If still not classified, use default\n",
    "        if not analysis['subtype']:\n",
    "            analysis['subtype'] = 'general_financial'\n",
    "            \n",
    "        # Check for branch patterns\n",
    "        branch_patterns = ['branch', 'location', 'center', 'office', 'atm']\n",
    "        for pattern in branch_patterns:\n",
    "            if pattern in s1 and pattern not in s2:\n",
    "                analysis['notes'].append(f\"'{s1}' appears to be a branch of '{s2}'\")\n",
    "                break\n",
    "            elif pattern in s2 and pattern not in s1:\n",
    "                analysis['notes'].append(f\"'{s2}' appears to be a branch of '{s1}'\")\n",
    "                break\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_retail_merchants(self, s1, s2):\n",
    "        \"\"\"Specialized analysis for retail merchants\"\"\"\n",
    "        analysis = {\n",
    "            'industry': 'retail',\n",
    "            'subtype': None,\n",
    "            'notes': []\n",
    "        }\n",
    "        \n",
    "        # Detect retail type\n",
    "        grocery_indicators = ['grocery', 'market', 'supermarket', 'food', 'mart']\n",
    "        department_indicators = ['department', 'store', 'mall', 'center', 'warehouse']\n",
    "        specialty_indicators = ['electronics', 'furniture', 'clothing', 'apparel', 'hardware', 'pharmacy']\n",
    "        \n",
    "        for indicator in grocery_indicators:\n",
    "            if indicator in s1 or indicator in s2:\n",
    "                analysis['subtype'] = 'grocery'\n",
    "                analysis['notes'].append(f\"Grocery retailer detected via keyword: '{indicator}'\")\n",
    "                break\n",
    "                \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in department_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'department'\n",
    "                    analysis['notes'].append(f\"Department store detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in specialty_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'specialty'\n",
    "                    analysis['notes'].append(f\"Specialty retailer detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        # If still not classified, use default\n",
    "        if not analysis['subtype']:\n",
    "            analysis['subtype'] = 'general_retail'\n",
    "            \n",
    "        # Check for location patterns\n",
    "        location_patterns = ['#', 'store', 'location', 'supercenter', 'express']\n",
    "        for pattern in location_patterns:\n",
    "            if pattern in s1 and pattern not in s2:\n",
    "                analysis['notes'].append(f\"'{s1}' appears to be a specific location of '{s2}'\")\n",
    "                break\n",
    "            elif pattern in s2 and pattern not in s1:\n",
    "                analysis['notes'].append(f\"'{s2}' appears to be a specific location of '{s1}'\")\n",
    "                break\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_restaurant_merchants(self, s1, s2):\n",
    "        \"\"\"Specialized analysis for restaurant merchants\"\"\"\n",
    "        analysis = {\n",
    "            'industry': 'restaurant',\n",
    "            'subtype': None,\n",
    "            'notes': []\n",
    "        }\n",
    "        \n",
    "        # Detect restaurant type\n",
    "        fast_food_indicators = ['mcdonald', 'burger', 'taco', 'pizza', 'wendy', 'kfc', 'subway']\n",
    "        cafe_indicators = ['coffee', 'cafe', 'starbucks', 'tea', 'bakery', 'dunkin']\n",
    "        dining_indicators = ['restaurant', 'grill', 'kitchen', 'house', 'steakhouse', 'bistro']\n",
    "        \n",
    "        for indicator in fast_food_indicators:\n",
    "            if indicator in s1 or indicator in s2:\n",
    "                analysis['subtype'] = 'fast_food'\n",
    "                analysis['notes'].append(f\"Fast food restaurant detected via keyword: '{indicator}'\")\n",
    "                break\n",
    "                \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in cafe_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'cafe'\n",
    "                    analysis['notes'].append(f\"Cafe detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in dining_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'dining'\n",
    "                    analysis['notes'].append(f\"Dining restaurant detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        # If still not classified, use default\n",
    "        if not analysis['subtype']:\n",
    "            analysis['subtype'] = 'general_restaurant'\n",
    "            \n",
    "        # Check for location patterns\n",
    "        location_patterns = ['#', 'restaurant', 'location', 'express', 'drive']\n",
    "        for pattern in location_patterns:\n",
    "            if pattern in s1 and pattern not in s2:\n",
    "                analysis['notes'].append(f\"'{s1}' appears to be a specific location of '{s2}'\")\n",
    "                break\n",
    "            elif pattern in s2 and pattern not in s1:\n",
    "                analysis['notes'].append(f\"'{s2}' appears to be a specific location of '{s1}'\")\n",
    "                break\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_hotel_merchants(self, s1, s2):\n",
    "        \"\"\"Specialized analysis for hotel merchants\"\"\"\n",
    "        analysis = {\n",
    "            'industry': 'hotel',\n",
    "            'subtype': None,\n",
    "            'notes': []\n",
    "        }\n",
    "        \n",
    "        # Detect hotel type\n",
    "        luxury_indicators = ['resort', 'spa', 'luxury', 'grand', 'palace', 'ritz']\n",
    "        budget_indicators = ['inn', 'motel', 'lodge', 'suites', 'stay', 'sleep']\n",
    "        chain_indicators = ['marriott', 'hilton', 'hyatt', 'holiday inn', 'sheraton', 'westin']\n",
    "        \n",
    "        for indicator in luxury_indicators:\n",
    "            if indicator in s1 or indicator in s2:\n",
    "                analysis['subtype'] = 'luxury'\n",
    "                analysis['notes'].append(f\"Luxury hotel detected via keyword: '{indicator}'\")\n",
    "                break\n",
    "                \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in budget_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'budget'\n",
    "                    analysis['notes'].append(f\"Budget hotel detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in chain_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'chain'\n",
    "                    analysis['notes'].append(f\"Hotel chain detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        # If still not classified, use default\n",
    "        if not analysis['subtype']:\n",
    "            analysis['subtype'] = 'general_hotel'\n",
    "            \n",
    "        # Check for property patterns\n",
    "        property_patterns = ['hotel', 'resort', 'suites', 'inn', 'by']\n",
    "        for pattern in property_patterns:\n",
    "            if pattern in s1 and pattern not in s2:\n",
    "                analysis['notes'].append(f\"'{s1}' appears to be a specific property of '{s2}'\")\n",
    "                break\n",
    "            elif pattern in s2 and pattern not in s1:\n",
    "                analysis['notes'].append(f\"'{s2}' appears to be a specific property of '{s1}'\")\n",
    "                break\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_gas_station_merchants(self, s1, s2):\n",
    "        \"\"\"Specialized analysis for gas station merchants\"\"\"\n",
    "        analysis = {\n",
    "            'industry': 'gas_station',\n",
    "            'subtype': None,\n",
    "            'notes': []\n",
    "        }\n",
    "        \n",
    "        # Detect gas station type\n",
    "        major_indicators = ['shell', 'exxon', 'mobil', 'bp', 'chevron', 'texaco']\n",
    "        convenience_indicators = ['7-eleven', 'circle k', 'speedway', 'am/pm', 'quicktrip']\n",
    "        service_indicators = ['service', 'auto', 'repair', 'maintenance', 'lube']\n",
    "        \n",
    "        for indicator in major_indicators:\n",
    "            if indicator in s1 or indicator in s2:\n",
    "                analysis['subtype'] = 'major_brand'\n",
    "                analysis['notes'].append(f\"Major gas station brand detected via keyword: '{indicator}'\")\n",
    "                break\n",
    "                \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in convenience_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'convenience'\n",
    "                    analysis['notes'].append(f\"Convenience store gas station detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        if not analysis['subtype']:\n",
    "            for indicator in service_indicators:\n",
    "                if indicator in s1 or indicator in s2:\n",
    "                    analysis['subtype'] = 'service'\n",
    "                    analysis['notes'].append(f\"Service station detected via keyword: '{indicator}'\")\n",
    "                    break\n",
    "        \n",
    "        # If still not classified, use default\n",
    "        if not analysis['subtype']:\n",
    "            analysis['subtype'] = 'general_gas_station'\n",
    "            \n",
    "        # Check for location patterns\n",
    "        location_patterns = ['gas', 'station', 'fuel', 'mart', 'convenience']\n",
    "        for pattern in location_patterns:\n",
    "            if pattern in s1 and pattern not in s2:\n",
    "                analysis['notes'].append(f\"'{s1}' appears to be a specific location of '{s2}'\")\n",
    "                break\n",
    "            elif pattern in s2 and pattern not in s1:\n",
    "                analysis['notes'].append(f\"'{s2}' appears to be a specific location of '{s1}'\")\n",
    "                break\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def adapt_to_domain(self, examples_df, domain=None):\n",
    "        \"\"\"\n",
    "        Adapt the semantic analyzer to a specific domain using example data\n",
    "        \n",
    "        Args:\n",
    "            examples_df (DataFrame): DataFrame with example merchant name pairs\n",
    "            domain (str, optional): Domain for adaptation\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if adaptation was successful, False otherwise\n",
    "        \"\"\"\n",
    "        if not isinstance(examples_df, pd.DataFrame) or len(examples_df) < 5:\n",
    "            logger.warning(\"Insufficient data for domain adaptation\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"Adapting semantic analyzer to domain: {domain or 'general'}\")\n",
    "        \n",
    "        # First, adapt the BERT embedder if available\n",
    "        if hasattr(self.bert_embedder, 'adapt_to_domain'):\n",
    "            try:\n",
    "                self.bert_embedder.adapt_to_domain(examples_df, epochs=5)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to adapt BERT embedder to domain: {e}\")\n",
    "        \n",
    "        # Build merchant clusters from the examples\n",
    "        try:\n",
    "            self._build_merchant_clusters(examples_df, domain)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to build merchant clusters: {e}\")\n",
    "        \n",
    "        self.adapted_to_domain = True\n",
    "        return True\n",
    "    \n",
    "    def _build_merchant_clusters(self, examples_df, domain=None):\n",
    "        \"\"\"\n",
    "        Build merchant name clusters from example data\n",
    "        \n",
    "        Args:\n",
    "            examples_df (DataFrame): DataFrame with example merchant name pairs\n",
    "            domain (str, optional): Domain for clustering\n",
    "        \"\"\"\n",
    "        # Extract all unique merchant names\n",
    "        all_names = []\n",
    "        \n",
    "        # Check different column combinations to extract merchant names\n",
    "        if 'Acronym' in examples_df.columns and 'Full_Name' in examples_df.columns:\n",
    "            all_names.extend(examples_df['Acronym'].tolist())\n",
    "            all_names.extend(examples_df['Full_Name'].tolist())\n",
    "        elif 'input_name' in examples_df.columns and 'matched_name' in examples_df.columns:\n",
    "            all_names.extend(examples_df['input_name'].tolist())\n",
    "            all_names.extend(examples_df['matched_name'].tolist())\n",
    "        else:\n",
    "            # Just take all string columns as potential merchant names\n",
    "            for col in examples_df.columns:\n",
    "                if examples_df[col].dtype == 'object':\n",
    "                    all_names.extend(examples_df[col].tolist())\n",
    "        \n",
    "        # Filter out non-string values and deduplicate\n",
    "        all_names = [name for name in all_names if isinstance(name, str) and name.strip()]\n",
    "        unique_names = list(set(all_names))\n",
    "        \n",
    "        # Preprocess all names\n",
    "        processed_names = [self.preprocessor.preprocess(name, domain) for name in unique_names]\n",
    "        \n",
    "        # Get embeddings for all names\n",
    "        try:\n",
    "            embeddings = self.bert_embedder.encode(processed_names)\n",
    "            \n",
    "            # Create clusters using hierarchical clustering\n",
    "            from sklearn.cluster import AgglomerativeClustering\n",
    "            \n",
    "            # Determine optimal number of clusters\n",
    "            max_clusters = min(20, len(processed_names) // 2)\n",
    "            if max_clusters < 2:\n",
    "                max_clusters = 2\n",
    "                \n",
    "            clustering = AgglomerativeClustering(\n",
    "                n_clusters=max_clusters,\n",
    "                affinity='euclidean',\n",
    "                linkage='ward'\n",
    "            )\n",
    "            \n",
    "            cluster_labels = clustering.fit_predict(embeddings)\n",
    "            \n",
    "            # Store clusters\n",
    "            self.merchant_clusters = {}\n",
    "            for i, (name, processed, embedding, label) in enumerate(\n",
    "                zip(unique_names, processed_names, embeddings, cluster_labels)\n",
    "            ):\n",
    "                if label not in self.merchant_clusters:\n",
    "                    self.merchant_clusters[label] = []\n",
    "                    self.cluster_embeddings[label] = []\n",
    "                \n",
    "                self.merchant_clusters[label].append((name, processed))\n",
    "                self.cluster_embeddings[label].append(embedding)\n",
    "            \n",
    "            logger.info(f\"Built {len(self.merchant_clusters)} merchant clusters from {len(unique_names)} names\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to create merchant clusters: {e}\")\n",
    "    \n",
    "    def get_semantically_similar_merchants(self, merchant_name, top_k=5, domain=None):\n",
    "        \"\"\"\n",
    "        Get semantically similar merchants from the adapted clusters\n",
    "        \n",
    "        Args:\n",
    "            merchant_name (str): Merchant name to find similar matches for\n",
    "            top_k (int): Number of top similar merchants to return\n",
    "            domain (str, optional): Domain for specialized processing\n",
    "            \n",
    "        Returns:\n",
    "            list: List of tuples with (merchant_name, similarity_score)\n",
    "        \"\"\"\n",
    "        if not self.merchant_clusters:\n",
    "            logger.warning(\"No merchant clusters available. Call adapt_to_domain first.\")\n",
    "            return []\n",
    "            \n",
    "        # Preprocess the input\n",
    "        processed_name = self.preprocessor.preprocess(merchant_name, domain)\n",
    "        \n",
    "        # Get embedding for the input name\n",
    "        query_embedding = self.bert_embedder.encode([processed_name])[0]\n",
    "        \n",
    "        # Find the closest cluster\n",
    "        closest_cluster = None\n",
    "        max_similarity = -1\n",
    "        \n",
    "        for label, cluster_embeddings in self.cluster_embeddings.items():\n",
    "            # Calculate average similarity to cluster\n",
    "            cluster_similarities = [\n",
    "                np.dot(query_embedding, emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(emb))\n",
    "                for emb in cluster_embeddings\n",
    "            ]\n",
    "            avg_similarity = np.mean(cluster_similarities)\n",
    "            \n",
    "            if avg_similarity > max_similarity:\n",
    "                max_similarity = avg_similarity\n",
    "                closest_cluster = label\n",
    "        \n",
    "        # If no close cluster found, return empty list\n",
    "        if closest_cluster is None:\n",
    "            return []\n",
    "            \n",
    "        # Calculate similarity to all merchants in the closest cluster and other high-similarity clusters\n",
    "        all_similarities = []\n",
    "        \n",
    "        # Check the closest cluster and nearby clusters\n",
    "        clusters_to_check = [closest_cluster]\n",
    "        \n",
    "        # Add other potential clusters if they're reasonably similar\n",
    "        for label in self.merchant_clusters:\n",
    "            if label != closest_cluster:\n",
    "                # Calculate similarity to cluster centroid\n",
    "                centroid = np.mean(self.cluster_embeddings[label], axis=0)\n",
    "                similarity = np.dot(query_embedding, centroid) / (np.linalg.norm(query_embedding) * np.linalg.norm(centroid))\n",
    "                \n",
    "                if similarity > 0.7:  # Check other reasonably similar clusters\n",
    "                    clusters_to_check.append(label)\n",
    "        \n",
    "        # Calculate similarities to all merchants in selected clusters\n",
    "        for label in clusters_to_check:\n",
    "            for i, (name, processed) in enumerate(self.merchant_clusters[label]):\n",
    "                embedding = self.cluster_embeddings[label][i]\n",
    "                similarity = np.dot(query_embedding, embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(embedding))\n",
    "                all_similarities.append((name, similarity))\n",
    "        \n",
    "        # Sort by similarity score and return top-k\n",
    "        all_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return all_similarities[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fbeaf20-0da5-4b28-9ad1-0ad23ef8e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Enhanced Merchant Matching System\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f3c9332-f991-4f3b-ae05-7e412e47e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1: Core Matcher Class Definition and Initialization\n",
    "\n",
    "class EnhancedMerchantMatcher:\n",
    "    \"\"\"\n",
    "    Advanced merchant matching system that combines multiple algorithms with machine learning\n",
    "    for high-accuracy merchant name matching across various domains.\n",
    "    \n",
    "    Key features:\n",
    "    - Multi-algorithm approach combining string similarity, semantic similarity, and pattern matching\n",
    "    - Domain-specific customization for different industries\n",
    "    - Machine learning integration for optimal feature weighting\n",
    "    - Comprehensive explanation of matching decisions\n",
    "    - Batch processing capabilities for large datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 preprocessor=None, \n",
    "                 similarity_algorithms=None,\n",
    "                 pattern_recognition=None,\n",
    "                 semantic_analyzer=None,\n",
    "                 bert_embedder=None,\n",
    "                 weights=None,\n",
    "                 thresholds=None):\n",
    "        \"\"\"\n",
    "        Initialize enhanced merchant matcher with configurable components\n",
    "        \n",
    "        Args:\n",
    "            preprocessor: Merchant name preprocessor for text normalization\n",
    "            similarity_algorithms: Collection of string similarity algorithms\n",
    "            pattern_recognition: Pattern recognition for merchant naming patterns\n",
    "            semantic_analyzer: Semantic analysis using language models\n",
    "            bert_embedder: BERT embedder for semantic similarity\n",
    "            weights (dict): Custom weights for different matching algorithms\n",
    "            thresholds (dict): Custom thresholds for match level determination\n",
    "        \"\"\"\n",
    "        # Initialize BERT embedder first since other components depend on it\n",
    "        self.bert_embedder = bert_embedder or AdvancedBERTEmbedder(\n",
    "            model_name='sentence-transformers/all-mpnet-base-v2',  # Using high-quality pretrained model\n",
    "            pooling_strategy='mean',  # Better for merchant name comparisons than CLS token\n",
    "            cache_size=10000,  # Increased cache for better performance on large datasets\n",
    "            device=device  # Use GPU if available\n",
    "        )\n",
    "        \n",
    "        # Initialize preprocessing component\n",
    "        self.preprocessor = preprocessor or MerchantPreprocessor()\n",
    "        \n",
    "        # Initialize similarity algorithms\n",
    "        self.similarity_algorithms = similarity_algorithms or SimilarityAlgorithms(\n",
    "            preprocessor=self.preprocessor,\n",
    "            bert_embedder=self.bert_embedder\n",
    "        )\n",
    "        \n",
    "        # Initialize pattern recognition\n",
    "        self.pattern_recognition = pattern_recognition or PatternRecognition(\n",
    "            preprocessor=self.preprocessor,\n",
    "            similarity_algorithms=self.similarity_algorithms\n",
    "        )\n",
    "        \n",
    "        # Initialize semantic analyzer\n",
    "        self.semantic_analyzer = semantic_analyzer or BertSemanticAnalyzer(\n",
    "            bert_embedder=self.bert_embedder,\n",
    "            similarity_algorithms=self.similarity_algorithms,\n",
    "            preprocessor=self.preprocessor\n",
    "        )\n",
    "        \n",
    "        # Set match thresholds (customizable)\n",
    "        self.thresholds = thresholds or {\n",
    "            'high': 0.85,   # Threshold for high confidence matches\n",
    "            'medium': 0.75, # Threshold for medium confidence matches\n",
    "            'low': 0.60     # Threshold for low confidence matches\n",
    "        }\n",
    "        \n",
    "        # Set feature weights (customizable)\n",
    "        self.weights = weights or {\n",
    "            'string_similarity': 0.20,    # Traditional string matching (Jaro-Winkler)\n",
    "            'token_set_similarity': 0.15, # Word-level matching regardless of order\n",
    "            'semantic_similarity': 0.30,  # Deep meaning similarity via BERT\n",
    "            'pattern_match': 0.25,        # Industry-specific patterns\n",
    "            'contains_check': 0.05,       # One name contains the other\n",
    "            'acronym_check': 0.05         # Acronym/abbreviation relationship\n",
    "        }\n",
    "        \n",
    "        # Initialize model attributes for ML-enhanced matching\n",
    "        self.ml_model = None              # Will hold trained classifier model\n",
    "        self.feature_names = None         # Feature names for model input\n",
    "        self.feature_importance = None    # Importance of each feature after training\n",
    "        self.model_trained = False        # Flag to track if model is ready\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.cache = {}                   # Cache for previously computed matches\n",
    "        self.cache_hits = 0\n",
    "        self.cache_size = 1000            # Max cache size\n",
    "        \n",
    "        logger.info(\"Enhanced Merchant Matcher initialized with all components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12515428-a96e-415e-bc23-0d0e22189d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2: Core Matching Algorithm Implementation\n",
    "\n",
    "def match_merchants(self, s1, s2, domain=None, return_details=False, use_cache=True):\n",
    "    \"\"\"\n",
    "    Match two merchant names using an enhanced multi-algorithm approach\n",
    "    \n",
    "    Args:\n",
    "        s1 (str): First merchant name\n",
    "        s2 (str): Second merchant name\n",
    "        domain (str, optional): Domain for specialized matching (e.g., banking, retail)\n",
    "        return_details (bool): Whether to return detailed match information\n",
    "        use_cache (bool): Whether to use and update the cache\n",
    "        \n",
    "    Returns:\n",
    "        float or dict: Match score (0-1) or detailed match information dictionary\n",
    "    \"\"\"\n",
    "    # Check cache first if enabled\n",
    "    if use_cache:\n",
    "        cache_key = f\"{s1}|{s2}|{domain}\"\n",
    "        if cache_key in self.cache:\n",
    "            self.cache_hits += 1\n",
    "            return self.cache[cache_key] if not return_details else self.cache[cache_key + \"_details\"]\n",
    "    \n",
    "    # Check for empty inputs\n",
    "    if not s1 or not s2:\n",
    "        result = 0.0\n",
    "        details = {\n",
    "            'match_score': 0.0,\n",
    "            'match_level': 'no_match',\n",
    "            'explanation': 'Empty input provided'\n",
    "        }\n",
    "        if use_cache:\n",
    "            self._update_cache(s1, s2, domain, result, details)\n",
    "        return details if return_details else result\n",
    "    \n",
    "    # Preprocess inputs based on domain-specific rules\n",
    "    s1_clean, s2_clean = self.preprocessor.preprocess_pair(s1, s2, domain)\n",
    "    \n",
    "    # Check if strings are empty after preprocessing\n",
    "    if not s1_clean or not s2_clean:\n",
    "        result = 0.0\n",
    "        details = {\n",
    "            'match_score': 0.0,\n",
    "            'match_level': 'no_match',\n",
    "            'explanation': 'Empty strings after preprocessing'\n",
    "        }\n",
    "        if use_cache:\n",
    "            self._update_cache(s1, s2, domain, result, details)\n",
    "        return details if return_details else result\n",
    "    \n",
    "    # Exact match fast path (significant performance optimization)\n",
    "    if s1_clean == s2_clean:\n",
    "        result = 1.0\n",
    "        details = {\n",
    "            'match_score': 1.0,\n",
    "            'match_level': 'exact_match',\n",
    "            'explanation': 'Exact match after preprocessing'\n",
    "        }\n",
    "        if use_cache:\n",
    "            self._update_cache(s1, s2, domain, result, details)\n",
    "        return details if return_details else result\n",
    "    \n",
    "    # Length check optimization - if lengths are too different, they're unlikely to match\n",
    "    len_ratio = min(len(s1_clean), len(s2_clean)) / max(len(s1_clean), len(s2_clean))\n",
    "    if len_ratio < 0.3:  # Very different lengths suggest different entities\n",
    "        # Quick check for acronyms before rejecting\n",
    "        acronym_score = self.similarity_algorithms.specialized_acronym_similarity(s1_clean, s2_clean, domain)\n",
    "        if acronym_score < 0.8:  # Not a strong acronym relationship\n",
    "            result = 0.2 * acronym_score  # Low score with some influence from acronym check\n",
    "            details = {\n",
    "                'match_score': result,\n",
    "                'match_level': 'no_match',\n",
    "                'explanation': 'Very different lengths and not a clear acronym relationship'\n",
    "            }\n",
    "            if use_cache:\n",
    "                self._update_cache(s1, s2, domain, result, details)\n",
    "            return details if return_details else result\n",
    "    \n",
    "    # Calculate individual similarity scores with optimized order (fastest first)\n",
    "    # First, calculate string similarity scores (faster than semantic)\n",
    "    string_sim = self.similarity_algorithms.jaro_winkler_similarity(s1_clean, s2_clean)\n",
    "    token_set_sim = self.similarity_algorithms.token_set_ratio(s1_clean, s2_clean)\n",
    "    contains_score = self.similarity_algorithms.contains_ratio(s1_clean, s2_clean)\n",
    "    acronym_score = self.similarity_algorithms.specialized_acronym_similarity(s1_clean, s2_clean, domain)\n",
    "    \n",
    "    # Early decision based on string metrics (optimization for clear non-matches)\n",
    "    if max(string_sim, token_set_sim, contains_score, acronym_score) < 0.4:\n",
    "        result = max(string_sim, token_set_sim, contains_score, acronym_score)\n",
    "        details = {\n",
    "            'match_score': result,\n",
    "            'match_level': 'no_match',\n",
    "            'explanation': 'Low similarity across all string metrics'\n",
    "        }\n",
    "        if use_cache:\n",
    "            self._update_cache(s1, s2, domain, result, details)\n",
    "        return details if return_details else result\n",
    "    \n",
    "    # Early decision based on string metrics (optimization for very high confidence matches)\n",
    "    high_string_match = (string_sim > 0.95 or token_set_sim > 0.95 or \n",
    "                         contains_score > 0.95 or acronym_score > 0.95)\n",
    "    if high_string_match:\n",
    "        result = max(string_sim, token_set_sim, contains_score, acronym_score)\n",
    "        details = {\n",
    "            'match_score': result,\n",
    "            'match_level': 'high_match',\n",
    "            'explanation': 'Very high string similarity'\n",
    "        }\n",
    "        if use_cache:\n",
    "            self._update_cache(s1, s2, domain, result, details)\n",
    "        return details if return_details else result\n",
    "    \n",
    "    # Only compute more expensive pattern and semantic metrics if needed\n",
    "    patterns = self.pattern_recognition.detect_merchant_patterns(s1_clean, s2_clean, domain)\n",
    "    pattern_score = max([p.get('confidence', 0) for p in patterns.values()]) if patterns else 0.0\n",
    "    \n",
    "    semantic_sim = self.semantic_analyzer.analyze_semantic_match(\n",
    "        s1_clean, s2_clean, domain\n",
    "    )['semantic_similarity']\n",
    "    \n",
    "    # Collect all features for either ML model or weighted scoring\n",
    "    features = {\n",
    "        'string_similarity': string_sim,\n",
    "        'token_set_similarity': token_set_sim,\n",
    "        'semantic_similarity': semantic_sim,\n",
    "        'contains_score': contains_score,\n",
    "        'acronym_score': acronym_score,\n",
    "        'pattern_score': pattern_score,\n",
    "        # Add contextual features for more robust matching\n",
    "        'length_ratio': len_ratio,\n",
    "        'word_count_ratio': min(len(s1_clean.split()), len(s2_clean.split())) / \n",
    "                           max(len(s1_clean.split()), len(s2_clean.split())) \n",
    "                           if max(len(s1_clean.split()), len(s2_clean.split())) > 0 else 0,\n",
    "        'common_prefix_length': self._common_prefix_length(s1_clean, s2_clean) / \n",
    "                               min(len(s1_clean), len(s2_clean)) if min(len(s1_clean), len(s2_clean)) > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    # Determine score using ML model if trained, otherwise use weighted average\n",
    "    if self.model_trained and self.ml_model is not None:\n",
    "        try:\n",
    "            # Prepare features for the model\n",
    "            feature_vector = [features[feat] for feat in self.feature_names]\n",
    "            feature_vector = np.array(feature_vector).reshape(1, -1)\n",
    "            \n",
    "            # Get model prediction (confidence of match)\n",
    "            score = float(self.ml_model.predict_proba(feature_vector)[0, 1])\n",
    "            explanation = \"Score determined by machine learning model\"\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error using ML model for prediction: {e}\")\n",
    "            # Fall back to weighted average\n",
    "            score = self._calculate_weighted_score(features)\n",
    "            explanation = \"Score determined by weighted algorithm average (ML model failed)\"\n",
    "    else:\n",
    "        # Use weighted average of different similarity measures\n",
    "        score = self._calculate_weighted_score(features)\n",
    "        explanation = \"Score determined by weighted algorithm average\"\n",
    "    \n",
    "    # Determine match level\n",
    "    match_level = self._determine_match_level(score)\n",
    "    \n",
    "    # Generate comprehensive explanation\n",
    "    full_explanation = self._generate_explanation(\n",
    "        s1, s2, s1_clean, s2_clean, \n",
    "        features, patterns, match_level, domain,\n",
    "        explanation\n",
    "    )\n",
    "    \n",
    "    # Prepare result\n",
    "    result = score\n",
    "    details = {\n",
    "        'match_score': score,\n",
    "        'match_level': match_level,\n",
    "        'explanation': full_explanation,\n",
    "        'features': features,\n",
    "        'patterns': patterns,\n",
    "        'processed_s1': s1_clean,\n",
    "        'processed_s2': s2_clean\n",
    "    }\n",
    "    \n",
    "    # Update cache\n",
    "    if use_cache:\n",
    "        self._update_cache(s1, s2, domain, result, details)\n",
    "    \n",
    "    return details if return_details else result\n",
    "\n",
    "def _update_cache(self, s1, s2, domain, result, details):\n",
    "    \"\"\"Update the cache with match results\"\"\"\n",
    "    cache_key = f\"{s1}|{s2}|{domain}\"\n",
    "    # Also cache the reverse pair to maximize cache hits\n",
    "    reverse_key = f\"{s2}|{s1}|{domain}\"\n",
    "    \n",
    "    # Enforce cache size limit using simple LRU strategy\n",
    "    if len(self.cache) >= self.cache_size:\n",
    "        # Remove oldest item (first key in dict)\n",
    "        self.cache.pop(next(iter(self.cache)))\n",
    "    \n",
    "    self.cache[cache_key] = result\n",
    "    self.cache[cache_key + \"_details\"] = details\n",
    "    self.cache[reverse_key] = result\n",
    "    self.cache[reverse_key + \"_details\"] = details\n",
    "\n",
    "def _common_prefix_length(self, s1, s2):\n",
    "    \"\"\"Calculate the length of the common prefix between two strings\"\"\"\n",
    "    common_len = 0\n",
    "    for c1, c2 in zip(s1, s2):\n",
    "        if c1 == c2:\n",
    "            common_len += 1\n",
    "        else:\n",
    "            break\n",
    "    return common_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4153d92-1f82-4e45-bb00-53a3bb8c3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3: Score Calculation and Match Level Determination\n",
    "\n",
    "def _calculate_weighted_score(self, features):\n",
    "    \"\"\"\n",
    "    Calculate weighted score based on individual feature scores\n",
    "    with adaptive weighting based on feature values\n",
    "    \n",
    "    Args:\n",
    "        features (dict): Dictionary of feature names and values\n",
    "        \n",
    "    Returns:\n",
    "        float: Weighted score between 0 and 1\n",
    "    \"\"\"\n",
    "    weighted_sum = 0.0\n",
    "    total_weight = 0.0\n",
    "    \n",
    "    # Dynamic weights adjustment based on feature values\n",
    "    adjusted_weights = self.weights.copy()\n",
    "    \n",
    "    # Boost importance of high-value features\n",
    "    for feature_name, value in features.items():\n",
    "        if feature_name in adjusted_weights and value > 0.9:\n",
    "            # Boost weights of very strong signals\n",
    "            adjusted_weights[feature_name] *= 1.5\n",
    "        elif feature_name in adjusted_weights and value < 0.3:\n",
    "            # Reduce weights of very weak signals\n",
    "            adjusted_weights[feature_name] *= 0.5\n",
    "    \n",
    "    # Special case: if acronym score is high, boost its importance\n",
    "    if features.get('acronym_score', 0) > 0.8:\n",
    "        adjusted_weights['acronym_check'] = max(\n",
    "            adjusted_weights.get('acronym_check', 0) * 2.0,\n",
    "            0.3  # Ensure significant weight\n",
    "        )\n",
    "    \n",
    "    # Apply all weights\n",
    "    for feature_name, weight in adjusted_weights.items():\n",
    "        if feature_name in features:\n",
    "            weighted_sum += features[feature_name] * weight\n",
    "            total_weight += weight\n",
    "    \n",
    "    # Add other features with small weights\n",
    "    for feature_name, value in features.items():\n",
    "        if feature_name not in adjusted_weights:\n",
    "            weighted_sum += value * 0.05\n",
    "            total_weight += 0.05\n",
    "    \n",
    "    # Normalize\n",
    "    normalized_score = weighted_sum / total_weight if total_weight > 0 else 0.0\n",
    "    \n",
    "    # Apply sigmoid scaling for better distribution of scores\n",
    "    # This helps separate clear matches from borderline cases\n",
    "    return self._sigmoid_scale(normalized_score)\n",
    "\n",
    "def _sigmoid_scale(self, score, steepness=10, midpoint=0.5):\n",
    "    \"\"\"\n",
    "    Apply sigmoid scaling to concentrate scores at the extremes\n",
    "    \n",
    "    Args:\n",
    "        score (float): Input score between 0 and 1\n",
    "        steepness (float): Controls how steep the sigmoid curve is\n",
    "        midpoint (float): Point around which to center the sigmoid\n",
    "        \n",
    "    Returns:\n",
    "        float: Scaled score between 0 and 1\n",
    "    \"\"\"\n",
    "    # Skip scaling for extreme values\n",
    "    if score > 0.95:\n",
    "        return 1.0\n",
    "    if score < 0.05:\n",
    "        return 0.0\n",
    "        \n",
    "    # Apply sigmoid transformation\n",
    "    scaled = 1 / (1 + np.exp(-steepness * (score - midpoint)))\n",
    "    \n",
    "    # Rescale from [0.27, 0.73] to [0, 1]\n",
    "    min_sigmoid = 1 / (1 + np.exp(-steepness * (0 - midpoint)))\n",
    "    max_sigmoid = 1 / (1 + np.exp(-steepness * (1 - midpoint)))\n",
    "    \n",
    "    return (scaled - min_sigmoid) / (max_sigmoid - min_sigmoid)\n",
    "\n",
    "def _determine_match_level(self, score):\n",
    "    \"\"\"\n",
    "    Determine the match level based on the score\n",
    "    \n",
    "    Args:\n",
    "        score (float): Match score between 0 and 1\n",
    "        \n",
    "    Returns:\n",
    "        str: Match level category\n",
    "    \"\"\"\n",
    "    if score >= self.thresholds['high']:\n",
    "        return 'high_match'\n",
    "    elif score >= self.thresholds['medium']:\n",
    "        return 'medium_match'\n",
    "    elif score >= self.thresholds['low']:\n",
    "        return 'low_match'\n",
    "    else:\n",
    "        return 'no_match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b39bcf5-2187-484e-adce-e33dc85820f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4: Explanation Generation\n",
    "\n",
    "def _generate_explanation(self, s1, s2, s1_clean, s2_clean, features, patterns, match_level, domain, base_explanation):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive human-readable explanation of the match\n",
    "    \n",
    "    Args:\n",
    "        s1 (str): Original first merchant name\n",
    "        s2 (str): Original second merchant name\n",
    "        s1_clean (str): Preprocessed first merchant name\n",
    "        s2_clean (str): Preprocessed second merchant name\n",
    "        features (dict): Feature scores dictionary\n",
    "        patterns (dict): Detected patterns dictionary\n",
    "        match_level (str): Determined match level\n",
    "        domain (str): Domain context\n",
    "        base_explanation (str): Base explanation about scoring method\n",
    "        \n",
    "    Returns:\n",
    "        str: Detailed explanation of the match\n",
    "    \"\"\"\n",
    "    explanations = [base_explanation]\n",
    "    \n",
    "    # Add preprocessed form explanation if different from input\n",
    "    if s1 != s1_clean or s2 != s2_clean:\n",
    "        if s1 != s1_clean:\n",
    "            explanations.append(f\"Preprocessed '{s1}' to '{s1_clean}'\")\n",
    "        if s2 != s2_clean:\n",
    "            explanations.append(f\"Preprocessed '{s2}' to '{s2_clean}'\")\n",
    "    \n",
    "    # Add key feature explanations with formatting based on strength\n",
    "    string_sim = features['string_similarity']\n",
    "    if string_sim > 0.9:\n",
    "        explanations.append(f\"STRONG string similarity: {string_sim:.2f}\")\n",
    "    elif string_sim > 0.7:\n",
    "        explanations.append(f\"GOOD string similarity: {string_sim:.2f}\")\n",
    "    else:\n",
    "        explanations.append(f\"String similarity: {string_sim:.2f}\")\n",
    "    \n",
    "    # Add token similarity explanation\n",
    "    token_sim = features['token_set_similarity']\n",
    "    if token_sim > string_sim + 0.2:\n",
    "        explanations.append(f\"Word-level matching ({token_sim:.2f}) much stronger than character-level, \" +\n",
    "                           \"suggesting possible word reordering or extra words\")\n",
    "    \n",
    "    # Add semantic similarity explanation\n",
    "    semantic_sim = features['semantic_similarity']\n",
    "    if semantic_sim > 0.9:\n",
    "        explanations.append(f\"STRONG semantic similarity: {semantic_sim:.2f}\")\n",
    "    elif semantic_sim > 0.7:\n",
    "        explanations.append(f\"GOOD semantic similarity: {semantic_sim:.2f}\")\n",
    "    elif semantic_sim > string_sim + 0.2:\n",
    "        explanations.append(f\"Semantic similarity ({semantic_sim:.2f}) much stronger than string similarity, \" +\n",
    "                           \"suggesting conceptually similar entities with different naming\")\n",
    "    \n",
    "    # Add pattern match explanations\n",
    "    if patterns:\n",
    "        for pattern_type, pattern_info in patterns.items():\n",
    "            confidence = pattern_info.get('confidence', 0)\n",
    "            if confidence > 0.8:\n",
    "                explanations.append(f\"STRONG pattern match: {pattern_type} with confidence {confidence:.2f}\")\n",
    "            elif confidence > 0.6:\n",
    "                explanations.append(f\"Pattern match: {pattern_type} with confidence {confidence:.2f}\")\n",
    "    \n",
    "    # Add specific feature explanations\n",
    "    contains_score = features['contains_score']\n",
    "    if contains_score > 0.9:\n",
    "        explanations.append(\"One name fully contains the other\")\n",
    "    elif contains_score > 0.7:\n",
    "        explanations.append(\"One name partially contains the other\")\n",
    "    \n",
    "    acronym_score = features['acronym_score'] \n",
    "    if acronym_score > 0.9:\n",
    "        explanations.append(\"Strong acronym relationship detected\")\n",
    "    elif acronym_score > 0.7:\n",
    "        explanations.append(\"Possible acronym relationship detected\")\n",
    "    \n",
    "    # Add length difference explanation if significant\n",
    "    len_ratio = features.get('length_ratio', 0)\n",
    "    if len_ratio < 0.5:\n",
    "        explanations.append(f\"Significant length difference (ratio: {len_ratio:.2f})\")\n",
    "    \n",
    "    # Add domain-specific explanation if available\n",
    "    if domain:\n",
    "        explanations.append(f\"Analysis performed in {domain} industry context\")\n",
    "    \n",
    "    # Add match level explanation with appropriate emphasis\n",
    "    if match_level == 'high_match':\n",
    "        explanations.append(\" HIGH CONFIDENCE MATCH: Names very likely refer to the same merchant\")\n",
    "    elif match_level == 'medium_match':\n",
    "        explanations.append(\"MEDIUM CONFIDENCE MATCH: Names probably refer to the same merchant\")\n",
    "    elif match_level == 'low_match':\n",
    "        explanations.append(\"LOW CONFIDENCE MATCH: Names might refer to the same merchant\")\n",
    "    else:\n",
    "        explanations.append(\" NO MATCH: Names likely refer to different merchants\")\n",
    "    \n",
    "    # Add examples of similar merchants if using ML model and confidence is medium\n",
    "    if self.model_trained and match_level == 'medium_match':\n",
    "        explanations.append(\"This match is based on patterns observed in the training data.\")\n",
    "    \n",
    "    return \"\\n\".join(explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08196cea-8adc-48da-8dba-a8585d898bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5: Batch Processing with Parallelization\n",
    "\n",
    "def match_merchant_batch(self, pairs_df, s1_col='s1', s2_col='s2', \n",
    "                         domain_col=None, return_details=False, \n",
    "                         batch_size=1000, use_multiprocessing=True,\n",
    "                         n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Match a batch of merchant name pairs with parallel processing\n",
    "    \n",
    "    Args:\n",
    "        pairs_df (DataFrame): DataFrame with merchant name pairs\n",
    "        s1_col (str): Column name for first merchant names\n",
    "        s2_col (str): Column name for second merchant names\n",
    "        domain_col (str, optional): Column name for domain information\n",
    "        return_details (bool): Whether to return detailed match info\n",
    "        batch_size (int): Size of batches for processing\n",
    "        use_multiprocessing (bool): Whether to use multiprocessing\n",
    "        n_jobs (int): Number of jobs for parallel processing (-1 for all cores)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Original DataFrame with added match scores and levels\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Copy input DataFrame\n",
    "    result_df = pairs_df.copy()\n",
    "    \n",
    "    # Add match columns\n",
    "    result_df['match_score'] = 0.0\n",
    "    result_df['match_level'] = 'no_match'\n",
    "    \n",
    "    if return_details:\n",
    "        result_df['match_details'] = None\n",
    "    \n",
    "    # Check for empty DataFrame\n",
    "    if len(result_df) == 0:\n",
    "        logger.warning(\"Empty DataFrame provided for batch matching\")\n",
    "        return result_df\n",
    "    \n",
    "    # Convert to records for faster access during parallel processing\n",
    "    records = result_df.to_dict('records')\n",
    "    total_records = len(records)\n",
    "    \n",
    "    # Function to process a single record\n",
    "    def process_record(record):\n",
    "        s1 = record[s1_col]\n",
    "        s2 = record[s2_col]\n",
    "        \n",
    "        # Skip invalid entries\n",
    "        if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "            return {\n",
    "                'match_score': 0.0,\n",
    "                'match_level': 'no_match',\n",
    "                'match_details': None if return_details else None\n",
    "            }\n",
    "        \n",
    "        # Get domain if column provided\n",
    "        domain = record[domain_col] if domain_col and domain_col in record else None\n",
    "        \n",
    "        # Match merchants\n",
    "        match_result = self.match_merchants(s1, s2, domain, return_details=return_details)\n",
    "        \n",
    "        if return_details:\n",
    "            # Return score, level and details\n",
    "            return {\n",
    "                'match_score': match_result['match_score'],\n",
    "                'match_level': match_result['match_level'],\n",
    "                'match_details': match_result\n",
    "            }\n",
    "        else:\n",
    "            # Just return score and derived level\n",
    "            score = match_result\n",
    "            level = self._determine_match_level(score)\n",
    "            return {\n",
    "                'match_score': score,\n",
    "                'match_level': level\n",
    "            }\n",
    "    \n",
    "    # Process in parallel if requested\n",
    "    results = []\n",
    "    \n",
    "    if use_multiprocessing and total_records > 100:\n",
    "        try:\n",
    "            from joblib import Parallel, delayed\n",
    "            \n",
    "            # Determine number of jobs\n",
    "            if n_jobs == -1:\n",
    "                import multiprocessing\n",
    "                n_jobs = multiprocessing.cpu_count()\n",
    "            \n",
    "            # Process in batches to save memory\n",
    "            for i in range(0, total_records, batch_size):\n",
    "                batch = records[i:min(i+batch_size, total_records)]\n",
    "                logger.info(f\"Processing batch {i//batch_size + 1}/{(total_records-1)//batch_size + 1} \" +\n",
    "                           f\"({len(batch)} records)\")\n",
    "                \n",
    "                batch_results = Parallel(n_jobs=n_jobs)(\n",
    "                    delayed(process_record)(record) for record in batch\n",
    "                )\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Parallel processing failed with error: {e}. Falling back to sequential processing.\")\n",
    "            results = [process_record(record) for record in records]\n",
    "    else:\n",
    "        # Sequential processing\n",
    "        for i, record in enumerate(records):\n",
    "            if i % 100 == 0:\n",
    "                logger.info(f\"Processing record {i+1}/{total_records}\")\n",
    "            results.append(process_record(record))\n",
    "    \n",
    "    # Update the DataFrame with results\n",
    "    for i, result in enumerate(results):\n",
    "        for key, value in result.items():\n",
    "            if key in result_df.columns:\n",
    "                result_df.iloc[i, result_df.columns.get_loc(key)] = value\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    records_per_second = total_records / processing_time if processing_time > 0 else 0\n",
    "    \n",
    "    logger.info(f\"Batch processing completed in {processing_time:.2f} seconds \" +\n",
    "               f\"({records_per_second:.2f} records/second)\")\n",
    "    logger.info(f\"Cache performance: {self.cache_hits} hits\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c0b6710-ffe5-4a61-9e7d-c21ff023ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.6: Advanced Machine Learning Integration\n",
    "\n",
    "def train_model(self, training_data, target_column='is_match', \n",
    "                test_size=0.2, random_state=42, use_advanced_model=True,\n",
    "                optimize_hyperparams=False):\n",
    "    \"\"\"\n",
    "    Train a machine learning model to improve matching accuracy,\n",
    "    with options for advanced models and hyperparameter optimization\n",
    "    \n",
    "    Args:\n",
    "        training_data (DataFrame): DataFrame with merchant name pairs and match labels\n",
    "        target_column (str): Column name containing match labels (1=match, 0=no match)\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        use_advanced_model (bool): Whether to use a more advanced ensemble model\n",
    "        optimize_hyperparams (bool): Whether to optimize hyperparameters\n",
    "        \n",
    "    Returns:\n",
    "        dict: Training results and metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Import required libraries\n",
    "        import xgboost as xgb\n",
    "        from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "        from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "    except ImportError:\n",
    "        logger.error(\"Required packages not installed. Install xgboost and scikit-learn.\")\n",
    "        return {'success': False, 'error': 'Required packages not installed'}\n",
    "    \n",
    "    try:\n",
    "        # Check if training data has required columns\n",
    "        required_cols = ['s1', 's2', target_column]\n",
    "        if not all(col in training_data.columns for col in required_cols):\n",
    "            return {\n",
    "                'success': False, \n",
    "                'error': f\"Training data missing required columns: {required_cols}\"\n",
    "            }\n",
    "        \n",
    "        # Extract features for each merchant pair\n",
    "        logger.info(f\"Extracting features from training data with {len(training_data)} rows...\")\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        domain_col = 'domain' if 'domain' in training_data.columns else None\n",
    "        \n",
    "        # Use more informative progress reporting for large datasets\n",
    "        total_rows = len(training_data)\n",
    "        for i, row in enumerate(training_data.iterrows()):\n",
    "            idx, row_data = row\n",
    "            s1 = row_data['s1']\n",
    "            s2 = row_data['s2']\n",
    "            domain = row_data[domain_col] if domain_col else None\n",
    "            \n",
    "            # Progress reporting\n",
    "            if i % max(1, total_rows // 20) == 0:\n",
    "                logger.info(f\"Processing row {i+1}/{total_rows} ({(i+1)/total_rows*100:.1f}%)\")\n",
    "            \n",
    "            # Skip rows with empty values\n",
    "            if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                continue\n",
    "            \n",
    "            # Extract features - use detailed version to get all features\n",
    "            match_details = self.match_merchants(s1, s2, domain, return_details=True)\n",
    "            \n",
    "            # Get feature vector\n",
    "            feature_dict = match_details['features']\n",
    "            \n",
    "            # Add derived features for better model performance\n",
    "            feature_dict['string_token_diff'] = abs(feature_dict['string_similarity'] - \n",
    "                                                  feature_dict.get('token_set_similarity', 0))\n",
    "            feature_dict['string_semantic_diff'] = abs(feature_dict['string_similarity'] - \n",
    "                                                     feature_dict.get('semantic_similarity', 0))\n",
    "            feature_dict['has_pattern'] = 1.0 if match_details.get('patterns', {}) else 0.0\n",
    "            \n",
    "            features.append(feature_dict)\n",
    "            labels.append(int(row_data[target_column]))\n",
    "        \n",
    "        # Convert features to DataFrame for better handling\n",
    "        feature_df = pd.DataFrame(features)\n",
    "        \n",
    "        # Store feature names for later use\n",
    "        self.feature_names = list(feature_df.columns)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = feature_df.values\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Report class distribution\n",
    "        train_pos = sum(y_train)\n",
    "        train_neg = len(y_train) - train_pos\n",
    "        test_pos = sum(y_test)\n",
    "        test_neg = len(y_test) - test_pos\n",
    "        \n",
    "        logger.info(f\"Training set: {len(y_train)} samples, {train_pos} positive, {train_neg} negative \" +\n",
    "                   f\"({train_pos/len(y_train)*100:.1f}% positive)\")\n",
    "        logger.info(f\"Test set: {len(y_test)} samples, {test_pos} positive, {test_neg} negative \" +\n",
    "                   f\"({test_pos/len(y_test)*100:.1f}% positive)\")\n",
    "        \n",
    "        # Select model\n",
    "        if use_advanced_model:\n",
    "            # Use a more advanced ensemble approach with multiple models\n",
    "            models = {\n",
    "                'xgboost': xgb.XGBClassifier(\n",
    "                    objective='binary:logistic',\n",
    "                    n_estimators=100,\n",
    "                    max_depth=5,\n",
    "                    learning_rate=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'gradient_boosting': GradientBoostingClassifier(\n",
    "                    n_estimators=100,\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=5,\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'random_forest': RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=10,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # Train and evaluate all models\n",
    "            model_performances = {}\n",
    "            for name, model in models.items():\n",
    "                logger.info(f\"Training {name} model...\")\n",
    "                \n",
    "                # Hyperparameter optimization if requested\n",
    "                if optimize_hyperparams:\n",
    "                    if name == 'xgboost':\n",
    "                        param_grid = {\n",
    "                            'n_estimators': [50, 100, 200],\n",
    "                            'max_depth': [3, 5, 7],\n",
    "                            'learning_rate': [0.01, 0.1, 0.2],\n",
    "                            'subsample': [0.8, 1.0],\n",
    "                            'colsample_bytree': [0.8, 1.0]\n",
    "                        }\n",
    "                    elif name == 'gradient_boosting':\n",
    "                        param_grid = {\n",
    "                            'n_estimators': [50, 100, 200],\n",
    "                            'max_depth': [3, 5, 7],\n",
    "                            'learning_rate': [0.01, 0.1, 0.2],\n",
    "                            'subsample': [0.8, 1.0]\n",
    "                        }\n",
    "                    elif name == 'random_forest':\n",
    "                        param_grid = {\n",
    "                            'n_estimators': [50, 100, 200],\n",
    "                            'max_depth': [5, 10, 15],\n",
    "                            'min_samples_split': [2, 5, 10]\n",
    "                        }\n",
    "                    \n",
    "                    logger.info(f\"Optimizing hyperparameters for {name}...\")\n",
    "                    grid_search = GridSearchCV(\n",
    "                        model, param_grid, cv=3, scoring='roc_auc', n_jobs=-1\n",
    "                    )\n",
    "                    grid_search.fit(X_train, y_train)\n",
    "                    model = grid_search.best_estimator_\n",
    "                    logger.info(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "                else:\n",
    "                    # Train with default parameters\n",
    "                    model.fit(X_train, y_train)\n",
    "                \n",
    "                # Evaluate\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "                classification_metrics = classification_report(y_test, y_pred, output_dict=True)\n",
    "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                \n",
    "                logger.info(f\"{name} model - AUC: {auc_score:.4f}, \" +\n",
    "                           f\"Accuracy: {classification_metrics['accuracy']:.4f}\")\n",
    "                \n",
    "                # Store performance\n",
    "                model_performances[name] = {\n",
    "                    'model': model,\n",
    "                    'auc': auc_score,\n",
    "                    'metrics': classification_metrics,\n",
    "                    'confusion_matrix': conf_matrix\n",
    "                }\n",
    "            \n",
    "            # Select best model based on AUC\n",
    "            best_model_name = max(model_performances, key=lambda k: model_performances[k]['auc'])\n",
    "            best_model_data = model_performances[best_model_name]\n",
    "            self.ml_model = best_model_data['model']\n",
    "            \n",
    "            logger.info(f\"Selected {best_model_name} as best model with AUC: {best_model_data['auc']:.4f}\")\n",
    "            \n",
    "            # Get feature importance\n",
    "            if hasattr(self.ml_model, 'feature_importances_'):\n",
    "                feature_importance = dict(zip(self.feature_names, self.ml_model.feature_importances_))\n",
    "                # Sort by importance\n",
    "                feature_importance = {k: v for k, v in sorted(\n",
    "                    feature_importance.items(), key=lambda item: item[1], reverse=True\n",
    "                )}\n",
    "                self.feature_importance = feature_importance\n",
    "            else:\n",
    "                self.feature_importance = {feature: 1.0/len(self.feature_names) \n",
    "                                         for feature in self.feature_names}\n",
    "            \n",
    "            # Update weights based on feature importance\n",
    "            self._update_weights_from_model()\n",
    "            \n",
    "            # Set flag to indicate model is trained\n",
    "            self.model_trained = True\n",
    "            \n",
    "            # Return results\n",
    "            return {\n",
    "                'success': True,\n",
    "                'model_name': best_model_name,\n",
    "                'auc_score': best_model_data['auc'],\n",
    "                'classification_metrics': best_model_data['metrics'],\n",
    "                'confusion_matrix': best_model_data['confusion_matrix'],\n",
    "                'feature_importance': self.feature_importance,\n",
    "                'feature_names': self.feature_names,\n",
    "                'model_performances': {k: {\n",
    "                    'auc': v['auc'], \n",
    "                    'metrics': v['metrics']\n",
    "                } for k, v in model_performances.items()}\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # Use simpler XGBoost model\n",
    "            logger.info(\"Training XGBoost model...\")\n",
    "            self.ml_model = xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                n_estimators=100,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            self.ml_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred = self.ml_model.predict(X_test)\n",
    "            y_pred_proba = self.ml_model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            classification_metrics = classification_report(y_test, y_pred, output_dict=True)\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Get feature importance\n",
    "            feature_importance = dict(zip(self.feature_names, self.ml_model.feature_importances_))\n",
    "            # Sort by importance\n",
    "            feature_importance = {k: v for k, v in sorted(\n",
    "                feature_importance.items(), key=lambda item: item[1], reverse=True\n",
    "            )}\n",
    "            self.feature_importance = feature_importance\n",
    "            \n",
    "            # Update weights based on feature importance\n",
    "            self._update_weights_from_model()\n",
    "            \n",
    "            # Set flag to indicate model is trained\n",
    "            self.model_trained = True\n",
    "            \n",
    "            logger.info(f\"Model trained successfully. AUC: {auc_score:.4f}\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'model_name': 'xgboost',\n",
    "                'auc_score': auc_score,\n",
    "                'classification_metrics': classification_metrics,\n",
    "                'confusion_matrix': conf_matrix,\n",
    "                'feature_importance': self.feature_importance,\n",
    "                'feature_names': self.feature_names\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training model: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "def _update_weights_from_model(self):\n",
    "    \"\"\"Update weights based on learned feature importance\"\"\"\n",
    "    if not self.feature_importance:\n",
    "        return\n",
    "    \n",
    "    # Map feature importance to weights\n",
    "    importance_to_weight = {\n",
    "        'string_similarity': 'string_similarity',\n",
    "        'token_set_similarity': 'token_set_similarity',\n",
    "        'semantic_similarity': 'semantic_similarity',\n",
    "        'pattern_score': 'pattern_match',\n",
    "        'contains_score': 'contains_check',\n",
    "        'acronym_score': 'acronym_check'\n",
    "    }\n",
    "    \n",
    "    # Calculate new weights\n",
    "    new_weights = {}\n",
    "    for feature, importance in self.feature_importance.items():\n",
    "        if feature in importance_to_weight:\n",
    "            weight_key = importance_to_weight[feature]\n",
    "            new_weights[weight_key] = importance\n",
    "    \n",
    "    # Normalize weights\n",
    "    total = sum(new_weights.values())\n",
    "    if total > 0:\n",
    "        new_weights = {k: v/total for k, v in new_weights.items()}\n",
    "        \n",
    "        # Update weights that exist in both\n",
    "        for k in set(self.weights.keys()) & set(new_weights.keys()):\n",
    "            # Blend original and learned weights (70% learned, 30% original)\n",
    "            self.weights[k] = 0.7 * new_weights[k] + 0.3 * self.weights[k]\n",
    "        \n",
    "        logger.info(f\"Updated weights based on model importance: {self.weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81821be7-dee1-4707-861e-681bb558f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7: Advanced Finding and Threshold Tuning\n",
    "\n",
    "def find_merchant_matches(self, query, candidates, domain=None, top_k=5, \n",
    "                          threshold=0.6, return_details=False, use_cache=True):\n",
    "    \"\"\"\n",
    "    Find best matches for a query merchant name from a list of candidates,\n",
    "    with optimized search for large candidate lists\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query merchant name\n",
    "        candidates (list): List of candidate merchant names\n",
    "        domain (str, optional): Domain for specialized matching\n",
    "        top_k (int): Number of top matches to return\n",
    "        threshold (float): Minimum score threshold\n",
    "        return_details (bool): Whether to return detailed match info\n",
    "        use_cache (bool): Whether to use the match cache\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples with match information\n",
    "    \"\"\"\n",
    "    # Check for empty query\n",
    "    if not query:\n",
    "        return []\n",
    "    \n",
    "    # Check for empty candidates list\n",
    "    if not candidates:\n",
    "        return []\n",
    "    \n",
    "    # Preprocess query\n",
    "    query_clean = self.preprocessor.preprocess(query, domain)\n",
    "    \n",
    "    # Check if query is empty after preprocessing\n",
    "    if not query_clean:\n",
    "        return []\n",
    "    \n",
    "    # For very large candidate lists, use optimization strategies\n",
    "    if len(candidates) > 1000 and self.bert_embedder.initialized:\n",
    "        return self._find_merchant_matches_optimized(\n",
    "            query, query_clean, candidates, domain, top_k, threshold, return_details\n",
    "        )\n",
    "    \n",
    "    # Calculate match scores for all candidates\n",
    "    matches = []\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        # Skip empty candidates\n",
    "        if not candidate:\n",
    "            continue\n",
    "        \n",
    "        # Match with the query\n",
    "        if return_details:\n",
    "            result = self.match_merchants(query_clean, candidate, domain, return_details=True, use_cache=use_cache)\n",
    "            score = result['match_score']\n",
    "            \n",
    "            # Add to matches if above threshold\n",
    "            if score >= threshold:\n",
    "                matches.append((candidate, score, result['match_level'], result))\n",
    "        else:\n",
    "            score = self.match_merchants(query_clean, candidate, domain, return_details=False, use_cache=use_cache)\n",
    "            \n",
    "            # Add to matches if above threshold\n",
    "            if score >= threshold:\n",
    "                level = self._determine_match_level(score)\n",
    "                matches.append((candidate, score, level))\n",
    "    \n",
    "    # Sort by score descending and take top-k\n",
    "    matches.sort(key=lambda x: x[1], reverse=True)\n",
    "    return matches[:top_k]\n",
    "\n",
    "def _find_merchant_matches_optimized(self, query, query_clean, candidates, \n",
    "                                     domain, top_k, threshold, return_details):\n",
    "    \"\"\"\n",
    "    Optimized version of find_merchant_matches for large candidate lists\n",
    "    using semantic embeddings for pre-filtering\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original query merchant name\n",
    "        query_clean (str): Preprocessed query\n",
    "        candidates (list): List of candidate merchant names\n",
    "        domain (str): Domain for specialized matching\n",
    "        top_k (int): Number of top matches to return\n",
    "        threshold (float): Minimum score threshold\n",
    "        return_details (bool): Whether to return detailed match info\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples with match information\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Preprocess all candidates\n",
    "    logger.info(f\"Preprocessing {len(candidates)} candidates...\")\n",
    "    candidates_clean = [self.preprocessor.preprocess(c, domain) for c in candidates]\n",
    "    \n",
    "    # Filter out empty candidates after preprocessing\n",
    "    valid_indices = [i for i, c in enumerate(candidates_clean) if c]\n",
    "    valid_candidates = [candidates[i] for i in valid_indices]\n",
    "    valid_candidates_clean = [candidates_clean[i] for i in valid_indices]\n",
    "    \n",
    "    # Step 2: Use semantic embeddings for initial filtering\n",
    "    logger.info(\"Computing semantic embeddings for candidates...\")\n",
    "    try:\n",
    "        # Get query embedding\n",
    "        query_embedding = self.bert_embedder.encode([query_clean])[0]\n",
    "        \n",
    "        # Get candidate embeddings in batches\n",
    "        batch_size = 1000\n",
    "        all_candidate_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(valid_candidates_clean), batch_size):\n",
    "            batch = valid_candidates_clean[i:i+batch_size]\n",
    "            batch_embeddings = self.bert_embedder.encode(batch)\n",
    "            all_candidate_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        candidate_embeddings = np.vstack(all_candidate_embeddings)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        # Normalize query embedding\n",
    "        query_embedding_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Normalize candidate embeddings\n",
    "        candidate_norms = np.linalg.norm(candidate_embeddings, axis=1, keepdims=True)\n",
    "        normalized_candidates = candidate_embeddings / np.maximum(candidate_norms, 1e-10)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = np.dot(normalized_candidates, query_embedding_norm)\n",
    "        \n",
    "        # Step 3: Select candidates with similarity above relaxed threshold\n",
    "        # Use a more relaxed threshold for pre-filtering to avoid missing matches\n",
    "        relaxed_threshold = max(0.4, threshold - 0.2)\n",
    "        prefilter_indices = np.where(similarities >= relaxed_threshold)[0]\n",
    "        \n",
    "        logger.info(f\"Pre-filtered to {len(prefilter_indices)} candidates in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # If too few candidates pass the filter, take the top candidates\n",
    "        if len(prefilter_indices) < top_k * 2:\n",
    "            # Take at least top_k*2 candidates\n",
    "            top_indices = np.argsort(similarities)[::-1][:top_k * 2]\n",
    "            prefilter_indices = top_indices\n",
    "        \n",
    "        # Step 4: Run full matching algorithm on pre-filtered candidates\n",
    "        prefiltered_candidates = [valid_candidates[i] for i in prefilter_indices]\n",
    "        \n",
    "        # Use the standard matching for the reduced candidate set\n",
    "        standard_matches = []\n",
    "        \n",
    "        for candidate in prefiltered_candidates:\n",
    "            if return_details:\n",
    "                result = self.match_merchants(query_clean, candidate, domain, return_details=True)\n",
    "                score = result['match_score']\n",
    "                \n",
    "                if score >= threshold:\n",
    "                    standard_matches.append((candidate, score, result['match_level'], result))\n",
    "            else:\n",
    "                score = self.match_merchants(query_clean, candidate, domain, return_details=False)\n",
    "                \n",
    "                if score >= threshold:\n",
    "                    level = self._determine_match_level(score)\n",
    "                    standard_matches.append((candidate, score, level))\n",
    "        \n",
    "        # Sort by score and take top-k\n",
    "        standard_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        logger.info(f\"Found {len(standard_matches)} matches above threshold {threshold} \" +\n",
    "                   f\"in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        return standard_matches[:top_k]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Optimized matching failed with error: {e}. Falling back to standard approach.\")\n",
    "        # Fall back to standard approach\n",
    "        return self.find_merchant_matches(\n",
    "            query, candidates, domain, top_k, threshold, return_details, use_cache=True\n",
    "        )\n",
    "\n",
    "def tune_thresholds(self, validation_data, target_column='is_match', domain_col=None):\n",
    "    \"\"\"\n",
    "    Tune matching thresholds based on validation data to optimize\n",
    "    precision/recall tradeoff\n",
    "    \n",
    "    Args:\n",
    "        validation_data (DataFrame): DataFrame with merchant pairs and ground truth\n",
    "        target_column (str): Column containing match labels (1=match, 0=no match)\n",
    "        domain_col (str, optional): Column containing domain information\n",
    "        \n",
    "    Returns:\n",
    "        dict: Optimized thresholds and metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.metrics import precision_recall_curve, f1_score, roc_curve, auc, confusion_matrix\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        logger.error(\"Required packages not installed. Install scikit-learn and matplotlib.\")\n",
    "        return {'success': False, 'error': 'Required packages not installed'}\n",
    "    \n",
    "    try:\n",
    "        # Check if validation data has required columns\n",
    "        required_cols = ['s1', 's2', target_column]\n",
    "        if not all(col in validation_data.columns for col in required_cols):\n",
    "            return {\n",
    "                'success': False, \n",
    "                'error': f\"Validation data missing required columns: {required_cols}\"\n",
    "            }\n",
    "        \n",
    "        # Get scores for all pairs\n",
    "        logger.info(f\"Calculating match scores for {len(validation_data)} validation pairs...\")\n",
    "        \n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        domains = []\n",
    "        \n",
    "        for i, row in validation_data.iterrows():\n",
    "            s1 = row['s1']\n",
    "            s2 = row['s2']\n",
    "            \n",
    "            # Skip rows with empty values\n",
    "            if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                continue\n",
    "            \n",
    "            # Get domain if column provided\n",
    "            domain = row[domain_col] if domain_col and domain_col in validation_data.columns else None\n",
    "            domains.append(domain)\n",
    "            \n",
    "            # Get ground truth\n",
    "            true_label = int(row[target_column])\n",
    "            y_true.append(true_label)\n",
    "            \n",
    "            # Get score\n",
    "            score = self.match_merchants(s1, s2, domain, return_details=False)\n",
    "            y_scores.append(score)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        y_true = np.array(y_true)\n",
    "        y_scores = np.array(y_scores)\n",
    "        \n",
    "        # Calculate precision-recall curve\n",
    "        precision, recall, thresholds_pr = precision_recall_curve(y_true, y_scores)\n",
    "        \n",
    "        # Find threshold that maximizes F1 score\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        best_f1_idx = np.argmax(f1_scores[:-1])  # Exclude the last point which has no threshold\n",
    "        best_f1_threshold = thresholds_pr[best_f1_idx]\n",
    "        best_f1 = f1_scores[best_f1_idx]\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, thresholds_roc = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Find threshold for different precision/recall targets\n",
    "        high_precision_idx = np.argmax(precision >= 0.95)\n",
    "        high_precision_threshold = thresholds_pr[high_precision_idx] if high_precision_idx < len(thresholds_pr) else 0.9\n",
    "        \n",
    "        high_recall_idx = np.argmax(recall[::-1] >= 0.95)\n",
    "        high_recall_threshold = thresholds_pr[len(thresholds_pr) - 1 - high_recall_idx] if high_recall_idx < len(thresholds_pr) else 0.6\n",
    "        \n",
    "        # Calculate confusion matrices at different thresholds\n",
    "        def get_confusion_matrix(threshold):\n",
    "            y_pred = (y_scores >= threshold).astype(int)\n",
    "            return confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Set new thresholds\n",
    "        new_thresholds = {\n",
    "            'high': high_precision_threshold,\n",
    "            'medium': best_f1_threshold,\n",
    "            'low': high_recall_threshold\n",
    "        }\n",
    "        \n",
    "        # Generate performance metrics at different thresholds\n",
    "        threshold_metrics = {}\n",
    "        for name, threshold in new_thresholds.items():\n",
    "            y_pred = (y_scores >= threshold).astype(int)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            \n",
    "            if tp + fp > 0:\n",
    "                prec = tp / (tp + fp)\n",
    "            else:\n",
    "                prec = 0\n",
    "                \n",
    "            if tp + fn > 0:\n",
    "                rec = tp / (tp + fn)\n",
    "            else:\n",
    "                rec = 0\n",
    "                \n",
    "            if prec + rec > 0:\n",
    "                f1 = 2 * (prec * rec) / (prec + rec)\n",
    "            else:\n",
    "                f1 = 0\n",
    "                \n",
    "            threshold_metrics[name] = {\n",
    "                'threshold': threshold,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'f1_score': f1,\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp),\n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        \n",
    "        # Update the matcher's thresholds\n",
    "        self.thresholds = new_thresholds\n",
    "        logger.info(f\"Updated thresholds: {self.thresholds}\")\n",
    "        \n",
    "        # If the matcher has domain-specific data, analyze by domain\n",
    "        domain_analysis = {}\n",
    "        if domain_col and len(set(domains) - {None}) > 1:\n",
    "            unique_domains = list(set(d for d in domains if d))\n",
    "            \n",
    "            for domain in unique_domains:\n",
    "                domain_indices = [i for i, d in enumerate(domains) if d == domain]\n",
    "                if len(domain_indices) < 10:  # Skip domains with too few samples\n",
    "                    continue\n",
    "                    \n",
    "                domain_y_true = y_true[domain_indices]\n",
    "                domain_y_scores = y_scores[domain_indices]\n",
    "                \n",
    "                # Calculate domain-specific precision-recall curve\n",
    "                domain_precision, domain_recall, domain_thresholds = precision_recall_curve(\n",
    "                    domain_y_true, domain_y_scores\n",
    "                )\n",
    "                \n",
    "                # Find domain-specific optimal threshold\n",
    "                if len(domain_thresholds) > 0:\n",
    "                    domain_f1_scores = 2 * (domain_precision[:-1] * domain_recall[:-1]) / (domain_precision[:-1] + domain_recall[:-1] + 1e-10)\n",
    "                    domain_best_idx = np.argmax(domain_f1_scores)\n",
    "                    domain_best_threshold = domain_thresholds[domain_best_idx]\n",
    "                    domain_best_f1 = domain_f1_scores[domain_best_idx]\n",
    "                    \n",
    "                    domain_analysis[domain] = {\n",
    "                        'optimal_threshold': domain_best_threshold,\n",
    "                        'f1_score': domain_best_f1,\n",
    "                        'sample_count': len(domain_indices)\n",
    "                    }\n",
    "        \n",
    "        # Generate and return comprehensive results\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'thresholds': self.thresholds,\n",
    "            'metrics': threshold_metrics,\n",
    "            'roc_auc': roc_auc,\n",
    "            'best_f1': best_f1,\n",
    "            'domain_analysis': domain_analysis if domain_analysis else None\n",
    "        }\n",
    "        \n",
    "        # Generate visualizations if matplotlib is available\n",
    "        if 'matplotlib.pyplot' in sys.modules:\n",
    "            try:\n",
    "                # Create a figure with precision-recall and ROC curves\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "                \n",
    "                # Precision-Recall curve\n",
    "                ax1.plot(recall, precision, 'b-', label=f'Precision-Recall (F1={best_f1:.3f})')\n",
    "                ax1.scatter([recall[best_f1_idx]], [precision[best_f1_idx]], marker='o', color='red', s=100,\n",
    "                           label=f'Best F1 threshold: {best_f1_threshold:.3f}')\n",
    "                ax1.set_xlabel('Recall')\n",
    "                ax1.set_ylabel('Precision')\n",
    "                ax1.set_title('Precision-Recall Curve')\n",
    "                ax1.legend()\n",
    "                ax1.grid(True)\n",
    "                \n",
    "                # ROC curve\n",
    "                ax2.plot(fpr, tpr, 'g-', label=f'ROC (AUC={roc_auc:.3f})')\n",
    "                ax2.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "                ax2.set_xlabel('False Positive Rate')\n",
    "                ax2.set_ylabel('True Positive Rate')\n",
    "                ax2.set_title('ROC Curve')\n",
    "                ax2.legend()\n",
    "                ax2.grid(True)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save as BytesIO object to include in results\n",
    "                import io\n",
    "                buf = io.BytesIO()\n",
    "                plt.savefig(buf, format='png')\n",
    "                buf.seek(0)\n",
    "                \n",
    "                # Convert to base64 for easy display\n",
    "                import base64\n",
    "                result['visualization_base64'] = base64.b64encode(buf.read()).decode('utf-8')\n",
    "                \n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to generate visualization: {e}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error tuning thresholds: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return {'success': False, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd437657-e983-4a9b-9631-1b8c21531bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.8: Domain Adaptation and Utility Methods\n",
    "\n",
    "def adapt_to_domain(self, examples_df, domain=None, epochs=5):\n",
    "    \"\"\"\n",
    "    Adapt all components to a specific domain using example data\n",
    "    \n",
    "    Args:\n",
    "        examples_df (DataFrame): DataFrame with example merchant name pairs\n",
    "        domain (str, optional): Domain name for specialization\n",
    "        epochs (int): Number of training epochs for adaptation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Adaptation results including success metrics\n",
    "    \"\"\"\n",
    "    if not isinstance(examples_df, pd.DataFrame):\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'Invalid input: examples_df must be a pandas DataFrame'\n",
    "        }\n",
    "    \n",
    "    if len(examples_df) < 5:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'Insufficient data: at least 5 examples needed for domain adaptation'\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"Adapting merchant matcher to domain: {domain or 'general'} with {len(examples_df)} examples\")\n",
    "    \n",
    "    results = {\n",
    "        'success': True,\n",
    "        'component_results': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Adapt BERT embedder\n",
    "    if hasattr(self.bert_embedder, 'adapt_to_domain'):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            self.bert_embedder.adapt_to_domain(examples_df, epochs=epochs)\n",
    "            bert_time = time.time() - start_time\n",
    "            \n",
    "            results['component_results']['bert_embedder'] = {\n",
    "                'success': True,\n",
    "                'adaptation_time': bert_time\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to adapt BERT embedder: {e}\")\n",
    "            results['component_results']['bert_embedder'] = {\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # 2. Adapt semantic analyzer\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        self.semantic_analyzer.adapt_to_domain(examples_df, domain)\n",
    "        semantic_time = time.time() - start_time\n",
    "        \n",
    "        results['component_results']['semantic_analyzer'] = {\n",
    "            'success': True,\n",
    "            'adaptation_time': semantic_time\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to adapt semantic analyzer: {e}\")\n",
    "        results['component_results']['semantic_analyzer'] = {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "    \n",
    "    # 3. Tune weights based on the domain if we have labeled data\n",
    "    if 'is_match' in examples_df.columns or 'expected_match' in examples_df.columns:\n",
    "        try:\n",
    "            # Determine which column has match labels\n",
    "            target_column = 'is_match' if 'is_match' in examples_df.columns else 'expected_match'\n",
    "            \n",
    "            # Only use a sample for weight tuning if the dataset is large\n",
    "            tuning_data = examples_df.sample(min(500, len(examples_df))) if len(examples_df) > 500 else examples_df\n",
    "            \n",
    "            # Prepare s1/s2 columns if not already present\n",
    "            if not ('s1' in tuning_data.columns and 's2' in tuning_data.columns):\n",
    "                if 'Acronym' in tuning_data.columns and 'Full_Name' in tuning_data.columns:\n",
    "                    tuning_data['s1'] = tuning_data['Acronym']\n",
    "                    tuning_data['s2'] = tuning_data['Full_Name']\n",
    "                elif 'input_name' in tuning_data.columns and 'matched_name' in tuning_data.columns:\n",
    "                    tuning_data['s1'] = tuning_data['input_name']\n",
    "                    tuning_data['s2'] = tuning_data['matched_name']\n",
    "            \n",
    "            # Tune weights if we have valid data\n",
    "            if 's1' in tuning_data.columns and 's2' in tuning_data.columns:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Calculate optimal weights through grid search\n",
    "                self._tune_weights(tuning_data, target_column, domain)\n",
    "                \n",
    "                weight_time = time.time() - start_time\n",
    "                \n",
    "                results['component_results']['weight_tuning'] = {\n",
    "                    'success': True,\n",
    "                    'adaptation_time': weight_time,\n",
    "                    'new_weights': self.weights\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to tune weights: {e}\")\n",
    "            results['component_results']['weight_tuning'] = {\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # 4. Clear cache after adaptation\n",
    "    self.cache = {}\n",
    "    self.cache_hits = 0\n",
    "    \n",
    "    logger.info(f\"Domain adaptation completed with results: {results}\")\n",
    "    return results\n",
    "\n",
    "def _tune_weights(self, tuning_data, target_column, domain=None):\n",
    "    \"\"\"\n",
    "    Tune weights for different similarity algorithms based on example data\n",
    "    \n",
    "    Args:\n",
    "        tuning_data (DataFrame): DataFrame with merchant name pairs and match labels\n",
    "        target_column (str): Column containing match labels\n",
    "        domain (str, optional): Domain context\n",
    "    \"\"\"\n",
    "    # Define weight combinations to try\n",
    "    weight_combinations = []\n",
    "    \n",
    "    # Generate a range of weight combinations\n",
    "    for semantic_w in [0.2, 0.3, 0.4]:\n",
    "        for string_w in [0.1, 0.2, 0.3]:\n",
    "            for pattern_w in [0.1, 0.2, 0.3]:\n",
    "                for token_w in [0.1, 0.15, 0.2]:\n",
    "                    # Calculate remaining weight\n",
    "                    other_w = 1.0 - semantic_w - string_w - pattern_w - token_w\n",
    "                    \n",
    "                    # Ensure weights sum to 1.0 and are positive\n",
    "                    if other_w >= 0:\n",
    "                        # Split remaining weight between contains and acronym\n",
    "                        acronym_w = other_w / 2\n",
    "                        contains_w = other_w / 2\n",
    "                        \n",
    "                        weight_combinations.append({\n",
    "                            'semantic_similarity': semantic_w,\n",
    "                            'string_similarity': string_w,\n",
    "                            'pattern_match': pattern_w,\n",
    "                            'token_set_similarity': token_w,\n",
    "                            'contains_check': contains_w,\n",
    "                            'acronym_check': acronym_w\n",
    "                        })\n",
    "    \n",
    "    # Add domain-specific weight combinations\n",
    "    if domain:\n",
    "        if domain.lower() in ['banking', 'financial']:\n",
    "            weight_combinations.append({\n",
    "                'semantic_similarity': 0.25,\n",
    "                'string_similarity': 0.15,\n",
    "                'pattern_match': 0.20,\n",
    "                'token_set_similarity': 0.10,\n",
    "                'contains_check': 0.10,\n",
    "                'acronym_check': 0.20  # Higher for banking acronyms\n",
    "            })\n",
    "        elif domain.lower() in ['retail', 'store']:\n",
    "            weight_combinations.append({\n",
    "                'semantic_similarity': 0.30,\n",
    "                'string_similarity': 0.25,\n",
    "                'pattern_match': 0.25,\n",
    "                'token_set_similarity': 0.10,\n",
    "                'contains_check': 0.05,\n",
    "                'acronym_check': 0.05\n",
    "            })\n",
    "        elif domain.lower() in ['restaurant', 'food']:\n",
    "            weight_combinations.append({\n",
    "                'semantic_similarity': 0.35,\n",
    "                'string_similarity': 0.25,\n",
    "                'pattern_match': 0.20,\n",
    "                'token_set_similarity': 0.15,\n",
    "                'contains_check': 0.05,\n",
    "                'acronym_check': 0.0\n",
    "            })\n",
    "    \n",
    "    # Evaluate each weight combination\n",
    "    best_score = 0\n",
    "    best_weights = None\n",
    "    \n",
    "    # Save original weights\n",
    "    original_weights = self.weights.copy()\n",
    "    \n",
    "    # Try each combination\n",
    "    for weights in weight_combinations:\n",
    "        # Set weights\n",
    "        self.weights = weights\n",
    "        \n",
    "        # Evaluate on tuning data\n",
    "        scores = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for i, row in tuning_data.iterrows():\n",
    "            s1 = row['s1']\n",
    "            s2 = row['s2']\n",
    "            \n",
    "            # Skip rows with empty values\n",
    "            if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                continue\n",
    "            \n",
    "            # Get ground truth\n",
    "            true_label = int(row[target_column])\n",
    "            true_labels.append(true_label)\n",
    "            \n",
    "            # Calculate score with current weights\n",
    "            score = self.match_merchants(s1, s2, domain, return_details=False, use_cache=False)\n",
    "            scores.append(score)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        y_true = np.array(true_labels)\n",
    "        y_scores = np.array(scores)\n",
    "        \n",
    "        # Calculate AUC-ROC as the evaluation metric\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        try:\n",
    "            auc_score = roc_auc_score(y_true, y_scores)\n",
    "            \n",
    "            if auc_score > best_score:\n",
    "                best_score = auc_score\n",
    "                best_weights = weights.copy()\n",
    "        except:\n",
    "            # Skip if AUC calculation fails (e.g., only one class)\n",
    "            pass\n",
    "    \n",
    "    # Set weights to the best combination or restore original if no improvement\n",
    "    if best_weights and best_score > 0.5:\n",
    "        self.weights = best_weights\n",
    "        logger.info(f\"Weight tuning complete. Best AUC: {best_score:.4f} with weights: {best_weights}\")\n",
    "    else:\n",
    "        self.weights = original_weights\n",
    "        logger.warning(\"Weight tuning did not improve performance. Reverting to original weights.\")\n",
    "\n",
    "def save_model(self, filepath):\n",
    "    \"\"\"\n",
    "    Save the trained matcher model to a file\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to save the model\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pickle\n",
    "        import os\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        # Prepare model data\n",
    "        model_data = {\n",
    "            'weights': self.weights,\n",
    "            'thresholds': self.thresholds,\n",
    "            'feature_names': self.feature_names,\n",
    "            'feature_importance': self.feature_importance,\n",
    "            'model_trained': self.model_trained\n",
    "        }\n",
    "        \n",
    "        # Check if ML model is trained and can be pickled\n",
    "        if self.model_trained and self.ml_model is not None:\n",
    "            try:\n",
    "                # Test if model can be pickled\n",
    "                pickle.dumps(self.ml_model)\n",
    "                model_data['ml_model'] = self.ml_model\n",
    "            except:\n",
    "                logger.warning(\"ML model could not be pickled. Saving weights only.\")\n",
    "                model_data['ml_model'] = None\n",
    "        \n",
    "        # Save to file\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        logger.info(f\"Model saved to {filepath}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving model: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def load_model(self, filepath):\n",
    "    \"\"\"\n",
    "    Load a trained matcher model from a file\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pickle\n",
    "        \n",
    "        # Load from file\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Update model attributes\n",
    "        self.weights = model_data.get('weights', self.weights)\n",
    "        self.thresholds = model_data.get('thresholds', self.thresholds)\n",
    "        self.feature_names = model_data.get('feature_names', None)\n",
    "        self.feature_importance = model_data.get('feature_importance', None)\n",
    "        self.model_trained = model_data.get('model_trained', False)\n",
    "        \n",
    "        # Load ML model if available\n",
    "        if 'ml_model' in model_data and model_data['ml_model'] is not None:\n",
    "            self.ml_model = model_data['ml_model']\n",
    "        \n",
    "        logger.info(f\"Model loaded from {filepath}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d5becbe-7bf8-4b7b-ae3c-14d7a517ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.9: Interactive Evaluation and Visualization\n",
    "\n",
    "def evaluate_match_interactively(self, s1, s2, domain=None):\n",
    "    \"\"\"\n",
    "    Evaluate a merchant name match with detailed visualization\n",
    "    of why the match was made and component contributions\n",
    "    \n",
    "    Args:\n",
    "        s1 (str): First merchant name\n",
    "        s2 (str): Second merchant name\n",
    "        domain (str, optional): Domain for specialized matching\n",
    "        \n",
    "    Returns:\n",
    "        dict: Comprehensive evaluation results\n",
    "    \"\"\"\n",
    "    # Get detailed match results\n",
    "    match_details = self.match_merchants(s1, s2, domain, return_details=True)\n",
    "    \n",
    "    # Extract key information\n",
    "    score = match_details['match_score']\n",
    "    level = match_details['match_level']\n",
    "    features = match_details['features']\n",
    "    patterns = match_details['patterns']\n",
    "    explanation = match_details['explanation']\n",
    "    \n",
    "    # Get preprocessed forms\n",
    "    s1_clean = match_details['processed_s1']\n",
    "    s2_clean = match_details['processed_s2']\n",
    "    \n",
    "    # Create visualization if matplotlib is available\n",
    "    visualization_data = None\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.patches as patches\n",
    "        import io\n",
    "        import base64\n",
    "        \n",
    "        # Create a figure with multiple subplots\n",
    "        fig = plt.figure(figsize=(12, 9))\n",
    "        gs = fig.add_gridspec(3, 2)\n",
    "        \n",
    "        # 1. Feature contribution plot\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        feature_names = []\n",
    "        feature_values = []\n",
    "        for fname, value in features.items():\n",
    "            if fname in self.weights:\n",
    "                feature_names.append(fname)\n",
    "                feature_values.append(value)\n",
    "        \n",
    "        colors = ['#3498db' if v >= 0.7 else '#e74c3c' for v in feature_values]\n",
    "        ax1.barh(feature_names, feature_values, color=colors)\n",
    "        ax1.set_xlim(0, 1)\n",
    "        ax1.set_title('Feature Contributions')\n",
    "        ax1.axvline(x=0.7, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # 2. Match summary with score visualization\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # Create a horizontal gauge\n",
    "        gauge_height = 0.3\n",
    "        background = patches.Rectangle((0, 0.5-gauge_height/2), 1, gauge_height, \n",
    "                                       facecolor='#ecf0f1', edgecolor='#bdc3c7')\n",
    "        ax2.add_patch(background)\n",
    "        \n",
    "        # Add color regions\n",
    "        low_region = patches.Rectangle((0, 0.5-gauge_height/2), self.thresholds['low'], gauge_height, \n",
    "                                      facecolor='#e74c3c', edgecolor=None, alpha=0.7)\n",
    "        med_region = patches.Rectangle((self.thresholds['low'], 0.5-gauge_height/2), \n",
    "                                      self.thresholds['medium']-self.thresholds['low'], \n",
    "                                      gauge_height, facecolor='#f39c12', edgecolor=None, alpha=0.7)\n",
    "        high_region = patches.Rectangle((self.thresholds['medium'], 0.5-gauge_height/2), \n",
    "                                       self.thresholds['high']-self.thresholds['medium'], \n",
    "                                       gauge_height, facecolor='#2ecc71', edgecolor=None, alpha=0.7)\n",
    "        v_high_region = patches.Rectangle((self.thresholds['high'], 0.5-gauge_height/2), \n",
    "                                         1-self.thresholds['high'], \n",
    "                                         gauge_height, facecolor='#27ae60', edgecolor=None, alpha=0.7)\n",
    "        \n",
    "        ax2.add_patch(low_region)\n",
    "        ax2.add_patch(med_region)\n",
    "        ax2.add_patch(high_region)\n",
    "        ax2.add_patch(v_high_region)\n",
    "        \n",
    "        # Add score marker\n",
    "        marker_width = 0.02\n",
    "        score_marker = patches.Rectangle((score-marker_width/2, 0.5-gauge_height/2-0.05), \n",
    "                                        marker_width, gauge_height+0.1, \n",
    "                                        facecolor='black', edgecolor=None)\n",
    "        ax2.add_patch(score_marker)\n",
    "        \n",
    "        # Add text\n",
    "        ax2.text(0.5, 0.8, f\"Match Score: {score:.3f}\", ha='center', fontsize=14, weight='bold')\n",
    "        ax2.text(0.5, 0.2, f\"Match Level: {level.upper().replace('_', ' ')}\", ha='center', fontsize=12)\n",
    "        \n",
    "        # Add threshold labels\n",
    "        ax2.text(self.thresholds['low'], 0.5-gauge_height/2-0.1, 'Low', ha='center', fontsize=8)\n",
    "        ax2.text(self.thresholds['medium'], 0.5-gauge_height/2-0.1, 'Medium', ha='center', fontsize=8)\n",
    "        ax2.text(self.thresholds['high'], 0.5-gauge_height/2-0.1, 'High', ha='center', fontsize=8)\n",
    "        \n",
    "        # 3. Text comparison visualization\n",
    "        ax3 = fig.add_subplot(gs[1, :])\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        # Split explanation into lines\n",
    "        explanation_lines = explanation.split('\\n')\n",
    "        \n",
    "        # Display original and preprocessed names\n",
    "        ax3.text(0.02, 0.95, f\"Original: '{s1}' vs '{s2}'\", fontsize=12, weight='bold')\n",
    "        ax3.text(0.02, 0.85, f\"Preprocessed: '{s1_clean}' vs '{s2_clean}'\", fontsize=12)\n",
    "        \n",
    "        # Display explanation text\n",
    "        for i, line in enumerate(explanation_lines):\n",
    "            if i < 10:  # Limit to 10 lines to avoid overflow\n",
    "                y_pos = 0.75 - i * 0.05\n",
    "                ax3.text(0.02, y_pos, line, fontsize=10)\n",
    "        \n",
    "        # 4. Pattern visualization if patterns exist\n",
    "        ax4 = fig.add_subplot(gs[2, 0])\n",
    "        if patterns:\n",
    "            ax4.axis('off')\n",
    "            ax4.text(0.5, 0.9, \"Detected Patterns\", ha='center', fontsize=12, weight='bold')\n",
    "            \n",
    "            y_pos = 0.8\n",
    "            for pattern_type, pattern_info in patterns.items():\n",
    "                if y_pos > 0.1:  # Avoid overflow\n",
    "                    ax4.text(0.02, y_pos, f\" {pattern_type}\", fontsize=10, weight='bold')\n",
    "                    confidence = pattern_info.get('confidence', 0)\n",
    "                    ax4.text(0.02, y_pos-0.05, f\"  Confidence: {confidence:.2f}\", fontsize=9)\n",
    "                    if 'explanation' in pattern_info:\n",
    "                        ax4.text(0.02, y_pos-0.1, f\"  {pattern_info['explanation']}\", fontsize=9)\n",
    "                        y_pos -= 0.15\n",
    "                    else:\n",
    "                        y_pos -= 0.1\n",
    "        else:\n",
    "            ax4.axis('off')\n",
    "            ax4.text(0.5, 0.5, \"No patterns detected\", ha='center', va='center', fontsize=12)\n",
    "        \n",
    "        # 5. Word similarity visualization (if text is not too long)\n",
    "        ax5 = fig.add_subplot(gs[2, 1])\n",
    "        if len(s1_clean.split()) <= 10 and len(s2_clean.split()) <= 10:\n",
    "            ax5.axis('off')\n",
    "            ax5.text(0.5, 0.9, \"Word-level Similarity\", ha='center', fontsize=12, weight='bold')\n",
    "            \n",
    "            # Get word-by-word similarities\n",
    "            s1_words = s1_clean.split()\n",
    "            s2_words = s2_clean.split()\n",
    "            \n",
    "            # Calculate similarity matrix\n",
    "            import numpy as np\n",
    "            sim_matrix = np.zeros((len(s1_words), len(s2_words)))\n",
    "            \n",
    "            for i, w1 in enumerate(s1_words):\n",
    "                for j, w2 in enumerate(s2_words):\n",
    "                    # Use Jaro-Winkler for word similarity\n",
    "                    sim_matrix[i, j] = jaro_winkler(w1, w2)\n",
    "            \n",
    "            # Display words with connections\n",
    "            for i, word in enumerate(s1_words):\n",
    "                if i < 5:  # Limit to 5 words to avoid overflow\n",
    "                    ax5.text(0.1, 0.8 - i*0.12, word, fontsize=10, ha='right')\n",
    "            \n",
    "            for j, word in enumerate(s2_words):\n",
    "                if j < 5:  # Limit to 5 words to avoid overflow\n",
    "                    ax5.text(0.9, 0.8 - j*0.12, word, fontsize=10, ha='left')\n",
    "            \n",
    "            # Draw connections for high similarities\n",
    "            for i, w1 in enumerate(s1_words):\n",
    "                if i >= 5:  # Skip if beyond display limit\n",
    "                    continue\n",
    "                    \n",
    "                for j, w2 in enumerate(s2_words):\n",
    "                    if j >= 5:  # Skip if beyond display limit\n",
    "                        continue\n",
    "                        \n",
    "                    sim = sim_matrix[i, j]\n",
    "                    if sim > 0.7:  # Only show strong connections\n",
    "                        # Draw line with alpha based on similarity\n",
    "                        ax5.plot([0.11, 0.89], [0.8 - i*0.12, 0.8 - j*0.12], \n",
    "                               alpha=sim*0.7, color='#3498db', linewidth=sim*2)\n",
    "        else:\n",
    "            ax5.axis('off')\n",
    "            ax5.text(0.5, 0.5, \"Texts too long for\\nword visualization\", \n",
    "                   ha='center', va='center', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save as BytesIO object\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', dpi=100)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Convert to base64 for easy display\n",
    "        visualization_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to generate visualization: {e}\")\n",
    "        import traceback\n",
    "        logger.debug(traceback.format_exc())\n",
    "    \n",
    "    # Return comprehensive evaluation\n",
    "    result = {\n",
    "        'match_score': score,\n",
    "        'match_level': level,\n",
    "        'explanation': explanation,\n",
    "        'features': features,\n",
    "        'patterns': patterns,\n",
    "        's1_original': s1,\n",
    "        's2_original': s2,\n",
    "        's1_processed': s1_clean,\n",
    "        's2_processed': s2_clean,\n",
    "        'thresholds': self.thresholds,\n",
    "        'weights': self.weights,\n",
    "        'visualization_base64': visualization_data\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b23cd21b-204f-45d9-bb1e-5af184db523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Main Execution Pipeline\n",
    "\n",
    "class GMARTMerchantMatchingPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive pipeline for merchant name matching that integrates all components\n",
    "    into a unified workflow with configuration, preprocessing, matching, and evaluation.\n",
    "    \n",
    "    Key features:\n",
    "    - Configuration management for different matching scenarios\n",
    "    - Integrated preprocessing, matching, and evaluation\n",
    "    - Error handling and robust logging\n",
    "    - Support for different deployment environments\n",
    "    - Performance metrics and reporting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path=None, domain=None, use_ml=True, \n",
    "                 debug_mode=False, log_level='INFO'):\n",
    "        \"\"\"\n",
    "        Initialize the merchant matching pipeline with configurable components\n",
    "        \n",
    "        Args:\n",
    "            config_path (str, optional): Path to configuration file\n",
    "            domain (str, optional): Default domain for specialized matching\n",
    "            use_ml (bool): Whether to use machine learning enhancement\n",
    "            debug_mode (bool): Enable additional logging and diagnostics\n",
    "            log_level (str): Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "        \"\"\"\n",
    "        # Configure logging\n",
    "        self._setup_logging(log_level)\n",
    "        \n",
    "        # Set instance attributes\n",
    "        self.config_path = config_path\n",
    "        self.domain = domain\n",
    "        self.use_ml = use_ml\n",
    "        self.debug_mode = debug_mode\n",
    "        \n",
    "        # Load configuration if provided\n",
    "        self.config = self._load_configuration(config_path)\n",
    "        \n",
    "        # Initialize component tracker for lazy loading\n",
    "        self._initialized_components = {}\n",
    "        self._merchant_matcher = None\n",
    "        \n",
    "        logger.info(f\"GMART Merchant Matching Pipeline initialized with domain: {domain}, ML: {use_ml}\")\n",
    "    \n",
    "    def _setup_logging(self, log_level):\n",
    "        \"\"\"Configure logging based on specified level\"\"\"\n",
    "        log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "        \n",
    "        # Set log level based on input\n",
    "        numeric_level = getattr(logging, log_level.upper(), None)\n",
    "        if not isinstance(numeric_level, int):\n",
    "            numeric_level = logging.INFO\n",
    "            \n",
    "        # Configure root logger\n",
    "        logging.basicConfig(level=numeric_level, format=log_format)\n",
    "        \n",
    "        # Create a file handler for persistent logging\n",
    "        try:\n",
    "            file_handler = logging.FileHandler('gmart_matching.log')\n",
    "            file_handler.setFormatter(logging.Formatter(log_format))\n",
    "            logging.getLogger().addHandler(file_handler)\n",
    "        except:\n",
    "            logging.warning(\"Could not create log file. Continuing with console logging only.\")\n",
    "    \n",
    "    def _load_configuration(self, config_path):\n",
    "        \"\"\"\n",
    "        Load configuration from file with fallback to defaults\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): Path to configuration file (JSON or YAML)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Configuration settings\n",
    "        \"\"\"\n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            \"thresholds\": {\n",
    "                \"high\": 0.85,\n",
    "                \"medium\": 0.75,\n",
    "                \"low\": 0.60\n",
    "            },\n",
    "            \"weights\": {\n",
    "                \"string_similarity\": 0.20,\n",
    "                \"token_set_similarity\": 0.15,\n",
    "                \"semantic_similarity\": 0.30,\n",
    "                \"pattern_match\": 0.25,\n",
    "                \"contains_check\": 0.05,\n",
    "                \"acronym_check\": 0.05\n",
    "            },\n",
    "            \"preprocessing\": {\n",
    "                \"remove_business_suffixes\": True,\n",
    "                \"normalize_special_merchants\": True,\n",
    "                \"expand_abbreviations\": True\n",
    "            },\n",
    "            \"feature_extraction\": {\n",
    "                \"use_semantic\": True,\n",
    "                \"use_phonetic\": True,\n",
    "                \"use_acronym_detection\": True\n",
    "            },\n",
    "            \"performance\": {\n",
    "                \"batch_size\": 1000,\n",
    "                \"use_multiprocessing\": True,\n",
    "                \"cache_size\": 10000\n",
    "            },\n",
    "            \"domains\": {\n",
    "                \"banking\": {\n",
    "                    \"weights\": {\n",
    "                        \"acronym_check\": 0.15,\n",
    "                        \"semantic_similarity\": 0.30\n",
    "                    }\n",
    "                },\n",
    "                \"retail\": {\n",
    "                    \"weights\": {\n",
    "                        \"pattern_match\": 0.30,\n",
    "                        \"string_similarity\": 0.25\n",
    "                    }\n",
    "                },\n",
    "                \"restaurant\": {\n",
    "                    \"weights\": {\n",
    "                        \"semantic_similarity\": 0.35,\n",
    "                        \"pattern_match\": 0.25\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Return defaults if no config path\n",
    "        if not config_path:\n",
    "            return default_config\n",
    "            \n",
    "        try:\n",
    "            # Load from file based on extension\n",
    "            if config_path.endswith('.json'):\n",
    "                import json\n",
    "                with open(config_path, 'r') as f:\n",
    "                    loaded_config = json.load(f)\n",
    "            elif config_path.endswith(('.yaml', '.yml')):\n",
    "                try:\n",
    "                    import yaml\n",
    "                    with open(config_path, 'r') as f:\n",
    "                        loaded_config = yaml.safe_load(f)\n",
    "                except ImportError:\n",
    "                    logger.warning(\"YAML package not available. Falling back to default configuration.\")\n",
    "                    return default_config\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported configuration file format: {config_path}\")\n",
    "                return default_config\n",
    "                \n",
    "            # Merge with defaults (recursive)\n",
    "            return self._merge_configs(default_config, loaded_config)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading configuration from {config_path}: {str(e)}\")\n",
    "            return default_config\n",
    "    \n",
    "    def _merge_configs(self, default_config, custom_config):\n",
    "        \"\"\"\n",
    "        Recursively merge custom configuration with defaults\n",
    "        \n",
    "        Args:\n",
    "            default_config (dict): Default configuration dictionary\n",
    "            custom_config (dict): Custom configuration to override defaults\n",
    "            \n",
    "        Returns:\n",
    "            dict: Merged configuration\n",
    "        \"\"\"\n",
    "        result = default_config.copy()\n",
    "        \n",
    "        for key, value in custom_config.items():\n",
    "            # If both are dicts, merge recursively\n",
    "            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
    "                result[key] = self._merge_configs(result[key], value)\n",
    "            # Otherwise override with custom value\n",
    "            else:\n",
    "                result[key] = value\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def _get_merchant_matcher(self):\n",
    "        \"\"\"\n",
    "        Lazy loading of the EnhancedMerchantMatcher to save resources\n",
    "        \n",
    "        Returns:\n",
    "            EnhancedMerchantMatcher: The initialized matcher\n",
    "        \"\"\"\n",
    "        if self._merchant_matcher is None:\n",
    "            # Initialize all components from configuration\n",
    "            thresholds = self.config.get('thresholds', {})\n",
    "            weights = self.config.get('weights', {})\n",
    "            \n",
    "            # Adjust weights based on domain if specified\n",
    "            if self.domain and self.domain in self.config.get('domains', {}):\n",
    "                domain_weights = self.config['domains'][self.domain].get('weights', {})\n",
    "                # Update only specified weights, keeping others\n",
    "                for k, v in domain_weights.items():\n",
    "                    weights[k] = v\n",
    "            \n",
    "            # Initialize BERT embedder\n",
    "            bert_embedder = AdvancedBERTEmbedder(\n",
    "                pooling_strategy='mean',\n",
    "                cache_size=self.config.get('performance', {}).get('cache_size', 10000)\n",
    "            )\n",
    "            \n",
    "            # Initialize preprocessor with config\n",
    "            preproc_config = self.config.get('preprocessing', {})\n",
    "            preprocessor = MerchantPreprocessor()\n",
    "            \n",
    "            # Initialize similarity algorithms\n",
    "            sim_algorithms = SimilarityAlgorithms(\n",
    "                preprocessor=preprocessor,\n",
    "                bert_embedder=bert_embedder\n",
    "            )\n",
    "            \n",
    "            # Initialize pattern recognition\n",
    "            pattern_recognition = PatternRecognition(\n",
    "                preprocessor=preprocessor,\n",
    "                similarity_algorithms=sim_algorithms\n",
    "            )\n",
    "            \n",
    "            # Initialize semantic analyzer\n",
    "            semantic_analyzer = BertSemanticAnalyzer(\n",
    "                bert_embedder=bert_embedder,\n",
    "                similarity_algorithms=sim_algorithms,\n",
    "                preprocessor=preprocessor\n",
    "            )\n",
    "            \n",
    "            # Initialize the merchant matcher\n",
    "            self._merchant_matcher = EnhancedMerchantMatcher(\n",
    "                preprocessor=preprocessor, \n",
    "                similarity_algorithms=sim_algorithms,\n",
    "                pattern_recognition=pattern_recognition,\n",
    "                semantic_analyzer=semantic_analyzer,\n",
    "                bert_embedder=bert_embedder,\n",
    "                weights=weights,\n",
    "                thresholds=thresholds\n",
    "            )\n",
    "            \n",
    "            # Track components\n",
    "            self._initialized_components = {\n",
    "                'preprocessor': preprocessor,\n",
    "                'bert_embedder': bert_embedder,\n",
    "                'similarity_algorithms': sim_algorithms,\n",
    "                'pattern_recognition': pattern_recognition,\n",
    "                'semantic_analyzer': semantic_analyzer\n",
    "            }\n",
    "            \n",
    "            logger.info(\"Merchant matcher initialized with all components\")\n",
    "            \n",
    "            # Training if ML model available\n",
    "            if self.use_ml and hasattr(self, '_training_data') and self._training_data is not None:\n",
    "                try:\n",
    "                    logger.info(\"Training ML model with provided data...\")\n",
    "                    self._merchant_matcher.train_model(self._training_data)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error training ML model: {e}\")\n",
    "        \n",
    "        return self._merchant_matcher\n",
    "    \n",
    "    def match_merchants(self, s1, s2, domain=None, return_details=False):\n",
    "        \"\"\"\n",
    "        Match two merchant names\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First merchant name\n",
    "            s2 (str): Second merchant name\n",
    "            domain (str, optional): Domain for specialized matching\n",
    "            return_details (bool): Whether to return detailed match info\n",
    "            \n",
    "        Returns:\n",
    "            float or dict: Match score or detailed match information\n",
    "        \"\"\"\n",
    "        # Get domain (use instance default if not specified)\n",
    "        effective_domain = domain if domain is not None else self.domain\n",
    "        \n",
    "        # Get matcher and perform match\n",
    "        matcher = self._get_merchant_matcher()\n",
    "        return matcher.match_merchants(s1, s2, effective_domain, return_details)\n",
    "    \n",
    "    def match_batch(self, data, s1_col='s1', s2_col='s2', domain_col=None, output_path=None):\n",
    "        \"\"\"\n",
    "        Process a batch of merchant name pairs\n",
    "        \n",
    "        Args:\n",
    "            data (DataFrame or str): Pandas DataFrame or path to CSV file\n",
    "            s1_col (str): Column name for first merchant names\n",
    "            s2_col (str): Column name for second merchant names\n",
    "            domain_col (str, optional): Column name for domain information\n",
    "            output_path (str, optional): Path to save results\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Results with match scores and levels\n",
    "        \"\"\"\n",
    "        # Load data if string path provided\n",
    "        if isinstance(data, str):\n",
    "            try:\n",
    "                data = pd.read_csv(data)\n",
    "                logger.info(f\"Loaded data from {data} with {len(data)} rows\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading data from {data}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        if s1_col not in data.columns or s2_col not in data.columns:\n",
    "            logger.error(f\"Required columns not found in data: {s1_col}, {s2_col}\")\n",
    "            return None\n",
    "            \n",
    "        # Get matcher and process batch\n",
    "        matcher = self._get_merchant_matcher()\n",
    "        \n",
    "        # Get batch size from config\n",
    "        batch_size = self.config.get('performance', {}).get('batch_size', 1000)\n",
    "        use_multiprocessing = self.config.get('performance', {}).get('use_multiprocessing', True)\n",
    "        \n",
    "        # Process batch\n",
    "        try:\n",
    "            results = matcher.match_merchant_batch(\n",
    "                data, \n",
    "                s1_col=s1_col, \n",
    "                s2_col=s2_col,\n",
    "                domain_col=domain_col,\n",
    "                return_details=False,\n",
    "                batch_size=batch_size,\n",
    "                use_multiprocessing=use_multiprocessing\n",
    "            )\n",
    "            \n",
    "            # Save results if output path provided\n",
    "            if output_path:\n",
    "                results.to_csv(output_path, index=False)\n",
    "                logger.info(f\"Results saved to {output_path}\")\n",
    "                \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing batch: {e}\")\n",
    "            import traceback\n",
    "            logger.debug(traceback.format_exc())\n",
    "            return None\n",
    "    \n",
    "    def find_matches(self, query, candidates, top_k=5, threshold=0.6, domain=None):\n",
    "        \"\"\"\n",
    "        Find top matches for a query merchant name from a list of candidates\n",
    "        \n",
    "        Args:\n",
    "            query (str): Query merchant name\n",
    "            candidates (list): List of candidate merchant names\n",
    "            top_k (int): Number of top matches to return\n",
    "            threshold (float): Minimum score threshold\n",
    "            domain (str, optional): Domain for specialized matching\n",
    "            \n",
    "        Returns:\n",
    "            list: Top matches with scores and levels\n",
    "        \"\"\"\n",
    "        # Get domain (use instance default if not specified)\n",
    "        effective_domain = domain if domain is not None else self.domain\n",
    "        \n",
    "        # Get matcher and find matches\n",
    "        matcher = self._get_merchant_matcher()\n",
    "        return matcher.find_merchant_matches(\n",
    "            query, \n",
    "            candidates, \n",
    "            effective_domain, \n",
    "            top_k, \n",
    "            threshold\n",
    "        )\n",
    "    \n",
    "    def train_with_data(self, training_data, target_column='is_match', test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train the matcher with labeled data to improve accuracy\n",
    "        \n",
    "        Args:\n",
    "            training_data (DataFrame): DataFrame with merchant pairs and match labels\n",
    "            target_column (str): Column name containing match labels\n",
    "            test_size (float): Proportion of data to use for testing\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training results and metrics\n",
    "        \"\"\"\n",
    "        # Store training data for lazy initialization\n",
    "        self._training_data = training_data\n",
    "        \n",
    "        # If matcher already initialized, train it directly\n",
    "        if self._merchant_matcher is not None:\n",
    "            return self._merchant_matcher.train_model(\n",
    "                training_data, \n",
    "                target_column=target_column,\n",
    "                test_size=test_size,\n",
    "                use_advanced_model=True\n",
    "            )\n",
    "        \n",
    "        # Otherwise training will happen on first matcher access\n",
    "        return {\"status\": \"pending\", \"message\": \"Training will occur when matcher is initialized\"}\n",
    "    \n",
    "    def adapt_to_domain(self, examples_df, domain=None):\n",
    "        \"\"\"\n",
    "        Adapt the matcher to a specific domain based on examples\n",
    "        \n",
    "        Args:\n",
    "            examples_df (DataFrame): DataFrame with example merchant name pairs\n",
    "            domain (str, optional): Domain name for specialization\n",
    "            \n",
    "        Returns:\n",
    "            dict: Adaptation results\n",
    "        \"\"\"\n",
    "        effective_domain = domain if domain is not None else self.domain\n",
    "        \n",
    "        # Get matcher and adapt\n",
    "        matcher = self._get_merchant_matcher()\n",
    "        return matcher.adapt_to_domain(examples_df, effective_domain)\n",
    "    \n",
    "    def tune_thresholds(self, validation_data, target_column='is_match'):\n",
    "        \"\"\"\n",
    "        Tune matching thresholds based on validation data\n",
    "        \n",
    "        Args:\n",
    "            validation_data (DataFrame): DataFrame with merchant pairs and ground truth\n",
    "            target_column (str): Column containing match labels (1=match, 0=no match)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Optimized thresholds and metrics\n",
    "        \"\"\"\n",
    "        # Get matcher and tune\n",
    "        matcher = self._get_merchant_matcher()\n",
    "        \n",
    "        domain_col = None\n",
    "        if self.domain is not None and 'domain' in validation_data.columns:\n",
    "            domain_col = 'domain'\n",
    "            \n",
    "        return matcher.tune_thresholds(validation_data, target_column, domain_col)\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to save the model\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        matcher = self._get_merchant_matcher()\n",
    "        return matcher.save_model(filepath)\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load a trained model from a file\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to the saved model\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        matcher = self._get_merchant_matcher()\n",
    "        return matcher.load_model(filepath)\n",
    "    \n",
    "    def evaluate_match(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Generate detailed evaluation of a merchant match\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First merchant name\n",
    "            s2 (str): Second merchant name\n",
    "            domain (str, optional): Domain for specialized matching\n",
    "            \n",
    "        Returns:\n",
    "            dict: Detailed evaluation results\n",
    "        \"\"\"\n",
    "        effective_domain = domain if domain is not None else self.domain\n",
    "        matcher = self._get_merchant_matcher()\n",
    "        return matcher.evaluate_match_interactively(s1, s2, effective_domain)\n",
    "    \n",
    "    def get_component_status(self):\n",
    "        \"\"\"\n",
    "        Get initialization status of all components\n",
    "        \n",
    "        Returns:\n",
    "            dict: Component status information\n",
    "        \"\"\"\n",
    "        # Basic status\n",
    "        status = {\n",
    "            \"pipeline_initialized\": True,\n",
    "            \"merchant_matcher_initialized\": self._merchant_matcher is not None,\n",
    "            \"components_initialized\": list(self._initialized_components.keys()) if self._initialized_components else [],\n",
    "            \"ml_model_trained\": False,\n",
    "            \"domain\": self.domain,\n",
    "            \"config_loaded\": self.config_path is not None\n",
    "        }\n",
    "        \n",
    "        # Add ML model status if matcher initialized\n",
    "        if self._merchant_matcher is not None:\n",
    "            status[\"ml_model_trained\"] = getattr(self._merchant_matcher, \"model_trained\", False)\n",
    "            \n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "398c90c9-7d3c-4c67-89fa-80e06d511cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Interactive Merchant Name Matching\n",
    "\n",
    "class InteractiveMerchantMatcher:\n",
    "    \"\"\"\n",
    "    Interactive interface for testing and analyzing merchant name matches\n",
    "    with rich visualization and explanation capabilities.\n",
    "    \n",
    "    This class provides a user-friendly way to explore merchant name matching,\n",
    "    analyze match decisions, and visualize the matching process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline=None, config_path=None, domain=None):\n",
    "        \"\"\"\n",
    "        Initialize the interactive matcher\n",
    "        \n",
    "        Args:\n",
    "            pipeline (GMARTMerchantMatchingPipeline, optional): Existing pipeline\n",
    "            config_path (str, optional): Path to configuration file\n",
    "            domain (str, optional): Default domain for specialized matching\n",
    "        \"\"\"\n",
    "        # Use existing pipeline or create a new one\n",
    "        if pipeline:\n",
    "            self.pipeline = pipeline\n",
    "        else:\n",
    "            self.pipeline = GMARTMerchantMatchingPipeline(\n",
    "                config_path=config_path,\n",
    "                domain=domain,\n",
    "                debug_mode=True\n",
    "            )\n",
    "        \n",
    "        # Store recent matches for history\n",
    "        self.match_history = []\n",
    "        self.max_history = 20\n",
    "    \n",
    "    def match(self, s1, s2, domain=None, visualize=True):\n",
    "        \"\"\"\n",
    "        Match two merchant names and visualize the results\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First merchant name\n",
    "            s2 (str): Second merchant name\n",
    "            domain (str, optional): Domain for specialized matching\n",
    "            visualize (bool): Whether to generate visualization\n",
    "            \n",
    "        Returns:\n",
    "            dict: Match results with visualization\n",
    "        \"\"\"\n",
    "        # Check inputs\n",
    "        if not s1 or not s2:\n",
    "            print(\"Please provide non-empty merchant names\")\n",
    "            return None\n",
    "        \n",
    "        # Perform detailed match evaluation\n",
    "        result = self.pipeline.evaluate_match(s1, s2, domain)\n",
    "        \n",
    "        # Add to history\n",
    "        self._add_to_history(s1, s2, result['match_score'], result['match_level'], domain)\n",
    "        \n",
    "        # Display results\n",
    "        self._display_results(result, visualize)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _add_to_history(self, s1, s2, score, level, domain):\n",
    "        \"\"\"Add match to history\"\"\"\n",
    "        self.match_history.append({\n",
    "            's1': s1,\n",
    "            's2': s2,\n",
    "            'score': score,\n",
    "            'level': level,\n",
    "            'domain': domain,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        \n",
    "        # Limit history size\n",
    "        if len(self.match_history) > self.max_history:\n",
    "            self.match_history = self.match_history[-self.max_history:]\n",
    "    \n",
    "    def _display_results(self, result, visualize):\n",
    "        \"\"\"Display match results with optional visualization\"\"\"\n",
    "        # Print header\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"MERCHANT MATCH ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Print basic match info\n",
    "        print(f\"\\nOriginal Names:\")\n",
    "        print(f\"  Name 1: '{result['s1_original']}'\")\n",
    "        print(f\"  Name 2: '{result['s2_original']}'\")\n",
    "        \n",
    "        print(f\"\\nPreprocessed Names:\")\n",
    "        print(f\"  Name 1: '{result['s1_processed']}'\")\n",
    "        print(f\"  Name 2: '{result['s2_processed']}'\")\n",
    "        \n",
    "        print(f\"\\nMatch Results:\")\n",
    "        print(f\"  Score: {result['match_score']:.3f}\")\n",
    "        print(f\"  Level: {result['match_level'].upper().replace('_', ' ')}\")\n",
    "        \n",
    "        # Print thresholds\n",
    "        print(\"\\nMatch Thresholds:\")\n",
    "        for level, threshold in result['thresholds'].items():\n",
    "            print(f\"  {level.capitalize()}: {threshold:.2f}\")\n",
    "        \n",
    "        # Print top feature scores\n",
    "        print(\"\\nFeature Scores:\")\n",
    "        for name, score in sorted(result['features'].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"  {name}: {score:.3f}\")\n",
    "        \n",
    "        # Print explanation\n",
    "        print(\"\\nExplanation:\")\n",
    "        for line in result['explanation'].split('\\n'):\n",
    "            print(f\"  {line}\")\n",
    "        \n",
    "        # Display visualization if available and requested\n",
    "        if visualize and result.get('visualization_base64'):\n",
    "            try:\n",
    "                self._display_visualization(result['visualization_base64'])\n",
    "            except Exception as e:\n",
    "                print(f\"\\nVisualization error: {e}\")\n",
    "    \n",
    "    def _display_visualization(self, base64_image):\n",
    "        \"\"\"Display the visualization image\"\"\"\n",
    "        try:\n",
    "            import IPython.display as display\n",
    "            from IPython.display import HTML\n",
    "            import base64\n",
    "            \n",
    "            # Create HTML for image display\n",
    "            html = f\"\"\"\n",
    "            <div style=\"background-color: white; padding: 10px; border-radius: 5px;\">\n",
    "                <img src=\"data:image/png;base64,{base64_image}\" style=\"max-width: 100%\">\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Display in notebook\n",
    "            display.display(HTML(html))\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"\\nVisualization available but IPython display not available.\")\n",
    "            print(\"Run in Jupyter notebook to see visualization.\")\n",
    "    \n",
    "    def compare_matches(self, merchant_name, candidates, domain=None, top_k=5, threshold=0.6):\n",
    "        \"\"\"\n",
    "        Compare a merchant name against multiple candidates\n",
    "        \n",
    "        Args:\n",
    "            merchant_name (str): Reference merchant name\n",
    "            candidates (list): List of candidate merchant names\n",
    "            domain (str, optional): Domain for specialized matching\n",
    "            top_k (int): Number of top matches to display\n",
    "            threshold (float): Minimum score threshold\n",
    "            \n",
    "        Returns:\n",
    "            list: Top matches with scores\n",
    "        \"\"\"\n",
    "        if not merchant_name or not candidates:\n",
    "            print(\"Please provide a merchant name and candidates list\")\n",
    "            return []\n",
    "        \n",
    "        # Find matches\n",
    "        matches = self.pipeline.find_matches(\n",
    "            merchant_name, \n",
    "            candidates, \n",
    "            top_k=top_k, \n",
    "            threshold=threshold, \n",
    "            domain=domain\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"TOP MERCHANT MATCHES FOR: '{merchant_name}'\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nFound {len(matches)} matches above threshold {threshold}\")\n",
    "        \n",
    "        # Print matches in table format\n",
    "        print(\"\\n{:<5} {:<40} {:<10} {:<15}\".format(\"Rank\", \"Merchant Name\", \"Score\", \"Match Level\"))\n",
    "        print(\"-\"*75)\n",
    "        \n",
    "        for i, (name, score, level) in enumerate(matches, 1):\n",
    "            print(\"{:<5} {:<40} {:<10.3f} {:<15}\".format(\n",
    "                i, name[:38], score, level.upper()\n",
    "            ))\n",
    "            \n",
    "        return matches\n",
    "    \n",
    "    def show_match_history(self, limit=10):\n",
    "        \"\"\"\n",
    "        Display recent match history\n",
    "        \n",
    "        Args:\n",
    "            limit (int): Number of recent matches to display\n",
    "            \n",
    "        Returns:\n",
    "            list: Recent match history\n",
    "        \"\"\"\n",
    "        history = self.match_history[-limit:]\n",
    "        \n",
    "        if not history:\n",
    "            print(\"No match history available\")\n",
    "            return []\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MERCHANT MATCH HISTORY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Print matches in table format\n",
    "        print(\"\\n{:<20} {:<20} {:<10} {:<15} {:<15}\".format(\n",
    "            \"Merchant 1\", \"Merchant 2\", \"Score\", \"Match Level\", \"Domain\"\n",
    "        ))\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for entry in reversed(history):\n",
    "            print(\"{:<20} {:<20} {:<10.3f} {:<15} {:<15}\".format(\n",
    "                entry['s1'][:18], \n",
    "                entry['s2'][:18], \n",
    "                entry['score'], \n",
    "                entry['level'].upper(),\n",
    "                entry['domain'] or 'None'\n",
    "            ))\n",
    "            \n",
    "        return history\n",
    "    \n",
    "    def analyze_patterns(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Analyze merchant name patterns in detail\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First merchant name\n",
    "            s2 (str): Second merchant name\n",
    "            domain (str, optional): Domain for specialized matching\n",
    "            \n",
    "        Returns:\n",
    "            dict: Pattern analysis\n",
    "        \"\"\"\n",
    "        # Get component from pipeline\n",
    "        matcher = self.pipeline._get_merchant_matcher()\n",
    "        pattern_recognition = self.pipeline._initialized_components.get('pattern_recognition')\n",
    "        \n",
    "        if not pattern_recognition:\n",
    "            print(\"Pattern recognition component not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Get patterns\n",
    "        patterns = pattern_recognition.detect_merchant_patterns(s1, s2, domain)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"PATTERN ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nMerchant Names:\")\n",
    "        print(f\"  Name 1: '{s1}'\")\n",
    "        print(f\"  Name 2: '{s2}'\")\n",
    "        \n",
    "        if not patterns:\n",
    "            print(\"\\nNo patterns detected between these merchant names\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"\\nDetected Patterns ({len(patterns)}):\")\n",
    "        \n",
    "        for pattern_type, pattern_info in patterns.items():\n",
    "            print(f\"\\n {pattern_type.upper()}\")\n",
    "            print(f\"  Confidence: {pattern_info.get('confidence', 0):.2f}\")\n",
    "            \n",
    "            if 'explanation' in pattern_info:\n",
    "                print(f\"  Explanation: {pattern_info['explanation']}\")\n",
    "                \n",
    "            # Print pattern details based on type\n",
    "            if pattern_type == 'known_equivalent':\n",
    "                print(f\"  Canonical: {pattern_info.get('pattern', ('', ''))[0]}\")\n",
    "            elif 'pattern' in pattern_info:\n",
    "                if isinstance(pattern_info['pattern'], list):\n",
    "                    for i, p in enumerate(pattern_info['pattern']):\n",
    "                        if isinstance(p, dict):\n",
    "                            print(f\"  Pattern {i+1}: {p.get('domain', '')} - {p.get('pattern_type', '')}\")\n",
    "                            if 'original' in p and 'normalized' in p:\n",
    "                                print(f\"    '{p['original']}'  '{p['normalized']}'\")\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def analyze_semantic_similarity(self, s1, s2, domain=None):\n",
    "        \"\"\"\n",
    "        Analyze semantic similarity in detail\n",
    "        \n",
    "        Args:\n",
    "            s1 (str): First merchant name\n",
    "            s2 (str): Second merchant name\n",
    "            domain (str, optional): Domain for specialized matching\n",
    "            \n",
    "        Returns:\n",
    "            dict: Semantic analysis\n",
    "        \"\"\"\n",
    "        # Get components from pipeline\n",
    "        semantic_analyzer = self.pipeline._initialized_components.get('semantic_analyzer')\n",
    "        \n",
    "        if not semantic_analyzer:\n",
    "            print(\"Semantic analyzer component not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Analyze semantic match\n",
    "        semantic_results = semantic_analyzer.analyze_semantic_match(s1, s2, domain)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"SEMANTIC ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nMerchant Names:\")\n",
    "        print(f\"  Name 1: '{s1}'\")\n",
    "        print(f\"  Name 2: '{s2}'\")\n",
    "        \n",
    "        print(f\"\\nSemantic Similarity: {semantic_results['semantic_similarity']:.3f}\")\n",
    "        print(f\"Match Level: {semantic_results['match_level'].upper().replace('_', ' ')}\")\n",
    "        \n",
    "        # Print semantic matching details\n",
    "        analysis = semantic_results.get('analysis', {})\n",
    "        \n",
    "        if 'word_match_ratio' in analysis:\n",
    "            print(f\"\\nWord Match Ratio: {analysis['word_match_ratio']:.3f}\")\n",
    "            \n",
    "        if 'matching_words' in analysis:\n",
    "            print(f\"Matching Words: {', '.join(analysis['matching_words'])}\")\n",
    "            \n",
    "        if 'context_similarity' in analysis:\n",
    "            print(f\"Context Similarity: {analysis['context_similarity']:.3f}\")\n",
    "            \n",
    "        if 'semantic_matching_points' in analysis:\n",
    "            print(f\"Semantic Matching Points: {', '.join(analysis['semantic_matching_points'])}\")\n",
    "            \n",
    "        if 'potential_relationship' in analysis:\n",
    "            print(f\"\\nPotential Relationship: {analysis['potential_relationship'].replace('_', ' ').title()}\")\n",
    "            print(f\"Confidence: {analysis['confidence']:.3f}\")\n",
    "            \n",
    "        # Print domain-specific analysis if available\n",
    "        if 'domain_specific' in analysis:\n",
    "            domain_info = analysis['domain_specific']\n",
    "            print(f\"\\nDomain-Specific Analysis ({domain_info.get('industry', 'unknown')}):\")\n",
    "            print(f\"  Subtype: {domain_info.get('subtype', 'general')}\")\n",
    "            \n",
    "            for note in domain_info.get('notes', []):\n",
    "                print(f\"   {note}\")\n",
    "        \n",
    "        return semantic_results\n",
    "    \n",
    "    def explain_preprocessing(self, merchant_name, domain=None):\n",
    "        \"\"\"\n",
    "        Explain preprocessing steps for a merchant name\n",
    "        \n",
    "        Args:\n",
    "            merchant_name (str): Merchant name to preprocess\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            dict: Preprocessing details\n",
    "        \"\"\"\n",
    "        # Get preprocessor from pipeline\n",
    "        preprocessor = self.pipeline._initialized_components.get('preprocessor')\n",
    "        \n",
    "        if not preprocessor:\n",
    "            print(\"Preprocessor component not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Track original name\n",
    "        original = merchant_name\n",
    "        \n",
    "        # Process step by step\n",
    "        steps = []\n",
    "        \n",
    "        # Step 1: Basic cleanup\n",
    "        step1 = merchant_name.lower().strip()\n",
    "        if step1 != original:\n",
    "            steps.append((\"Convert to lowercase and trim\", step1))\n",
    "        \n",
    "        # Step 2: Handle punctuation\n",
    "        step2 = re.sub(r'([^a-z0-9\\'\\.\\&\\-])', ' ', step1)\n",
    "        if step2 != step1:\n",
    "            steps.append((\"Remove most punctuation\", step2))\n",
    "        \n",
    "        # Step 3: Handle apostrophes\n",
    "        step3 = re.sub(r'\\'s\\b', 's', step2)\n",
    "        step3 = re.sub(r'\\'', '', step3)\n",
    "        if step3 != step2:\n",
    "            steps.append((\"Normalize apostrophes\", step3))\n",
    "        \n",
    "        # Step 4: Normalize spaces\n",
    "        step4 = re.sub(r'\\s+', ' ', step3).strip()\n",
    "        if step4 != step3:\n",
    "            steps.append((\"Normalize spaces\", step4))\n",
    "        \n",
    "        # Step 5: Remove business suffixes\n",
    "        step5 = step4\n",
    "        for pattern, replacement in preprocessor.business_suffixes.items():\n",
    "            step5 = pattern.sub(replacement, step5)\n",
    "        if step5 != step4:\n",
    "            steps.append((\"Remove business suffixes\", step5))\n",
    "        \n",
    "        # Step 6: Apply business patterns\n",
    "        step6 = step5\n",
    "        for pattern, replacement_func in preprocessor.business_patterns.items():\n",
    "            match = pattern.search(step6)\n",
    "            if match:\n",
    "                step6 = replacement_func(match).strip()\n",
    "                break\n",
    "        if step6 != step5:\n",
    "            steps.append((\"Apply business patterns\", step6))\n",
    "        \n",
    "        # Step 7: Expand abbreviations\n",
    "        words = step6.split()\n",
    "        expanded_words = [preprocessor.abbreviations.get(word, word) for word in words]\n",
    "        step7 = ' '.join(expanded_words)\n",
    "        if step7 != step6:\n",
    "            steps.append((\"Expand abbreviations\", step7))\n",
    "        \n",
    "        # Step 8: Remove stopwords\n",
    "        filtered_words = [word for word in step7.split() if word not in preprocessor.stopwords]\n",
    "        step8 = ' '.join(filtered_words)\n",
    "        if step8 != step7:\n",
    "            steps.append((\"Remove stopwords\", step8))\n",
    "        \n",
    "        # Final result\n",
    "        final = preprocessor.preprocess(merchant_name, domain)\n",
    "        if final != step8 and steps:\n",
    "            steps.append((\"Domain-specific processing\", final))\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"PREPROCESSING ANALYSIS FOR: '{original}'\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if not steps:\n",
    "            print(\"\\nNo preprocessing changes were applied\")\n",
    "        else:\n",
    "            print(\"\\nPreprocessing Steps:\")\n",
    "            for i, (description, result) in enumerate(steps, 1):\n",
    "                print(f\"\\n{i}. {description}\")\n",
    "                print(f\"   '{result}'\")\n",
    "        \n",
    "        print(f\"\\nFinal preprocessed result: '{final}'\")\n",
    "        \n",
    "        # Return preprocessing details\n",
    "        return {\n",
    "            'original': original,\n",
    "            'final': final,\n",
    "            'steps': steps,\n",
    "            'domain': domain\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35e6c1a3-ea17-48e4-ba8e-7710bb938631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Batch Processing and PySpark Adaptation\n",
    "# I'll create a comprehensive implementation for batch processing and PySpark adaptation that integrates with the existing merchant matching system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b00f467d-420d-4ebc-b500-7670fb316c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1: Scalable Batch Processing Framework\n",
    "\n",
    "class BatchProcessor:\n",
    "    \"\"\"\n",
    "    High-performance batch processing framework for merchant name matching\n",
    "    with support for various input formats and parallel execution.\n",
    "    \n",
    "    Key features:\n",
    "    - Multi-format support (CSV, Excel, JSON, Parquet)\n",
    "    - Chunked processing for memory efficiency\n",
    "    - Progress tracking and reporting\n",
    "    - Automatic error recovery\n",
    "    - Result aggregation and export\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, matcher=None, chunk_size=10000, n_jobs=-1, \n",
    "                 output_format='csv', temp_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize batch processor with configurable parameters\n",
    "        \n",
    "        Args:\n",
    "            matcher: Merchant matcher instance (EnhancedMerchantMatcher or pipeline)\n",
    "            chunk_size (int): Number of records per processing chunk\n",
    "            n_jobs (int): Number of parallel jobs (-1 for all cores)\n",
    "            output_format (str): Format for output files ('csv', 'excel', 'json', 'parquet')\n",
    "            temp_dir (str): Directory for temporary files\n",
    "        \"\"\"\n",
    "        self.matcher = matcher\n",
    "        self.chunk_size = chunk_size\n",
    "        self.n_jobs = n_jobs\n",
    "        self.output_format = output_format.lower()\n",
    "        self.temp_dir = temp_dir or os.path.join(os.getcwd(), 'temp')\n",
    "        \n",
    "        # Create temp directory if it doesn't exist\n",
    "        if not os.path.exists(self.temp_dir):\n",
    "            os.makedirs(self.temp_dir)\n",
    "            \n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger('BatchProcessor')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Track processing metrics\n",
    "        self.metrics = {\n",
    "            'total_processed': 0,\n",
    "            'successful_matches': 0,\n",
    "            'failed_matches': 0,\n",
    "            'processing_time': 0,\n",
    "            'matches_per_second': 0\n",
    "        }\n",
    "    \n",
    "    def process_file(self, input_file, output_file=None, \n",
    "                     s1_col='s1', s2_col='s2', domain_col=None, \n",
    "                     id_col=None, return_detailed=False):\n",
    "        \"\"\"\n",
    "        Process a file containing merchant name pairs\n",
    "        \n",
    "        Args:\n",
    "            input_file (str): Path to input file\n",
    "            output_file (str, optional): Path to output file (generated if None)\n",
    "            s1_col (str): Column name for first merchant names\n",
    "            s2_col (str): Column name for second merchant names\n",
    "            domain_col (str, optional): Column name for domain information\n",
    "            id_col (str, optional): Column name for unique identifier\n",
    "            return_detailed (bool): Whether to return detailed match info\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to output file with results\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.logger.info(f\"Starting batch processing of {input_file}\")\n",
    "        \n",
    "        # Generate output file name if not provided\n",
    "        if output_file is None:\n",
    "            file_base = os.path.splitext(os.path.basename(input_file))[0]\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = f\"{file_base}_matched_{timestamp}.{self.output_format}\"\n",
    "            # If relative path, put in current directory\n",
    "            if not os.path.isabs(output_file):\n",
    "                output_file = os.path.join(os.getcwd(), output_file)\n",
    "        \n",
    "        # Determine file format and read data\n",
    "        try:\n",
    "            data_chunks = self._read_input_file(input_file, self.chunk_size)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error reading input file: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Process chunks with parallel execution\n",
    "        processed_chunks = []\n",
    "        chunk_paths = []\n",
    "        \n",
    "        self.logger.info(\"Processing data in chunks...\")\n",
    "        \n",
    "        for i, chunk in enumerate(data_chunks):\n",
    "            self.logger.info(f\"Processing chunk {i+1}\")\n",
    "            \n",
    "            # Process chunk\n",
    "            if self.n_jobs == 1:\n",
    "                # Sequential processing\n",
    "                processed_chunk = self._process_chunk(\n",
    "                    chunk, s1_col, s2_col, domain_col, id_col, return_detailed\n",
    "                )\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            else:\n",
    "                # Parallel processing - save chunk to temp file\n",
    "                chunk_path = os.path.join(self.temp_dir, f\"chunk_{i}.csv\")\n",
    "                chunk.to_csv(chunk_path, index=False)\n",
    "                chunk_paths.append(chunk_path)\n",
    "        \n",
    "        # If parallel processing, use joblib to process all chunks\n",
    "        if self.n_jobs != 1 and chunk_paths:\n",
    "            try:\n",
    "                from joblib import Parallel, delayed\n",
    "                \n",
    "                # Process all chunks in parallel\n",
    "                processed_chunks = Parallel(n_jobs=self.n_jobs)(\n",
    "                    delayed(self._process_chunk_file)(\n",
    "                        path, s1_col, s2_col, domain_col, id_col, return_detailed\n",
    "                    ) for path in chunk_paths\n",
    "                )\n",
    "                \n",
    "                # Clean up temp files\n",
    "                for path in chunk_paths:\n",
    "                    try:\n",
    "                        os.remove(path)\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "            except ImportError:\n",
    "                self.logger.warning(\n",
    "                    \"joblib not available for parallel processing. Using sequential processing.\"\n",
    "                )\n",
    "                # Fall back to sequential processing\n",
    "                processed_chunks = []\n",
    "                for path in chunk_paths:\n",
    "                    chunk = pd.read_csv(path)\n",
    "                    processed_chunk = self._process_chunk(\n",
    "                        chunk, s1_col, s2_col, domain_col, id_col, return_detailed\n",
    "                    )\n",
    "                    processed_chunks.append(processed_chunk)\n",
    "                    \n",
    "                    # Clean up temp file\n",
    "                    try:\n",
    "                        os.remove(path)\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # Combine all processed chunks\n",
    "        if processed_chunks:\n",
    "            result_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "            \n",
    "            # Export results\n",
    "            self._write_output_file(result_df, output_file)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics['total_processed'] = len(result_df)\n",
    "            self.metrics['successful_matches'] = len(\n",
    "                result_df[result_df['match_level'] != 'no_match']\n",
    "            )\n",
    "            self.metrics['failed_matches'] = len(\n",
    "                result_df[result_df['match_level'] == 'no_match']\n",
    "            )\n",
    "            self.metrics['processing_time'] = time.time() - start_time\n",
    "            \n",
    "            if self.metrics['processing_time'] > 0:\n",
    "                self.metrics['matches_per_second'] = (\n",
    "                    self.metrics['total_processed'] / self.metrics['processing_time']\n",
    "                )\n",
    "            \n",
    "            self.logger.info(f\"Batch processing completed: {len(result_df)} records processed\")\n",
    "            self.logger.info(\n",
    "                f\"Processing time: {self.metrics['processing_time']:.2f} seconds \"\n",
    "                f\"({self.metrics['matches_per_second']:.2f} records/second)\"\n",
    "            )\n",
    "            \n",
    "            return output_file\n",
    "        else:\n",
    "            self.logger.error(\"No data processed\")\n",
    "            return None\n",
    "    \n",
    "    def _read_input_file(self, file_path, chunk_size):\n",
    "        \"\"\"\n",
    "        Read input file in chunks with format auto-detection\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to input file\n",
    "            chunk_size (int): Chunk size for reading\n",
    "            \n",
    "        Returns:\n",
    "            list: List of DataFrame chunks\n",
    "        \"\"\"\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_ext == '.csv':\n",
    "            # Read CSV in chunks\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                chunks.append(chunk)\n",
    "            return chunks\n",
    "            \n",
    "        elif file_ext in ['.xlsx', '.xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(file_path)\n",
    "            # Split into chunks\n",
    "            return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "            \n",
    "        elif file_ext == '.json':\n",
    "            # Read JSON file\n",
    "            df = pd.read_json(file_path)\n",
    "            # Split into chunks\n",
    "            return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "            \n",
    "        elif file_ext == '.parquet':\n",
    "            # Read Parquet file\n",
    "            try:\n",
    "                import pyarrow.parquet as pq\n",
    "                df = pq.read_table(file_path).to_pandas()\n",
    "                # Split into chunks\n",
    "                return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "            except ImportError:\n",
    "                # Fall back to pandas\n",
    "                df = pd.read_parquet(file_path)\n",
    "                # Split into chunks\n",
    "                return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}\")\n",
    "    \n",
    "    def _process_chunk(self, chunk, s1_col, s2_col, domain_col, id_col, return_detailed):\n",
    "        \"\"\"\n",
    "        Process a chunk of data\n",
    "        \n",
    "        Args:\n",
    "            chunk (DataFrame): DataFrame chunk to process\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            domain_col (str, optional): Column name for domain\n",
    "            id_col (str, optional): Column name for ID\n",
    "            return_detailed (bool): Whether to return detailed results\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Processed DataFrame with match results\n",
    "        \"\"\"\n",
    "        result_df = chunk.copy()\n",
    "        \n",
    "        # Ensure matcher is initialized\n",
    "        if self.matcher is None:\n",
    "            self.logger.error(\"Matcher not initialized\")\n",
    "            return result_df\n",
    "        \n",
    "        # Add result columns if they don't exist\n",
    "        if 'match_score' not in result_df.columns:\n",
    "            result_df['match_score'] = 0.0\n",
    "        if 'match_level' not in result_df.columns:\n",
    "            result_df['match_level'] = 'no_match'\n",
    "        if return_detailed and 'match_details' not in result_df.columns:\n",
    "            result_df['match_details'] = None\n",
    "        \n",
    "        # Check if columns exist\n",
    "        if s1_col not in result_df.columns or s2_col not in result_df.columns:\n",
    "            self.logger.error(f\"Required columns not found: {s1_col}, {s2_col}\")\n",
    "            return result_df\n",
    "        \n",
    "        # Process each row\n",
    "        for idx, row in result_df.iterrows():\n",
    "            try:\n",
    "                s1 = row[s1_col]\n",
    "                s2 = row[s2_col]\n",
    "                \n",
    "                # Skip if empty\n",
    "                if pd.isna(s1) or pd.isna(s2) or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                    continue\n",
    "                \n",
    "                # Get domain if available\n",
    "                domain = row[domain_col] if domain_col and domain_col in row else None\n",
    "                \n",
    "                # Match merchants\n",
    "                if hasattr(self.matcher, 'match_merchants'):\n",
    "                    # EnhancedMerchantMatcher\n",
    "                    match_result = self.matcher.match_merchants(\n",
    "                        s1, s2, domain, return_details=return_detailed\n",
    "                    )\n",
    "                else:\n",
    "                    # GMARTMerchantMatchingPipeline\n",
    "                    match_result = self.matcher.match(\n",
    "                        s1, s2, domain, return_details=return_detailed\n",
    "                    )\n",
    "                \n",
    "                # Update results\n",
    "                if return_detailed:\n",
    "                    result_df.at[idx, 'match_score'] = match_result['match_score']\n",
    "                    result_df.at[idx, 'match_level'] = match_result['match_level']\n",
    "                    result_df.at[idx, 'match_details'] = str(match_result)\n",
    "                else:\n",
    "                    score = match_result\n",
    "                    level = 'high_match' if score >= 0.85 else (\n",
    "                        'medium_match' if score >= 0.75 else (\n",
    "                            'low_match' if score >= 0.6 else 'no_match'\n",
    "                        )\n",
    "                    )\n",
    "                    result_df.at[idx, 'match_score'] = score\n",
    "                    result_df.at[idx, 'match_level'] = level\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing row {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def _process_chunk_file(self, chunk_path, s1_col, s2_col, domain_col, id_col, return_detailed):\n",
    "        \"\"\"\n",
    "        Process a chunk file (for parallel processing)\n",
    "        \n",
    "        Args:\n",
    "            chunk_path (str): Path to chunk file\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            domain_col (str, optional): Column name for domain\n",
    "            id_col (str, optional): Column name for ID\n",
    "            return_detailed (bool): Whether to return detailed results\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Processed DataFrame with match results\n",
    "        \"\"\"\n",
    "        # Read chunk\n",
    "        chunk = pd.read_csv(chunk_path)\n",
    "        \n",
    "        # Process chunk\n",
    "        return self._process_chunk(chunk, s1_col, s2_col, domain_col, id_col, return_detailed)\n",
    "    \n",
    "    def _write_output_file(self, df, output_path):\n",
    "        \"\"\"\n",
    "        Write output file in the specified format\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame to write\n",
    "            output_path (str): Path to output file\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success flag\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n",
    "            \n",
    "            # Write based on format\n",
    "            file_ext = os.path.splitext(output_path)[1].lower()\n",
    "            \n",
    "            if file_ext == '.csv':\n",
    "                df.to_csv(output_path, index=False)\n",
    "                \n",
    "            elif file_ext in ['.xlsx', '.xls']:\n",
    "                df.to_excel(output_path, index=False)\n",
    "                \n",
    "            elif file_ext == '.json':\n",
    "                # Handle non-serializable objects\n",
    "                if 'match_details' in df.columns:\n",
    "                    df['match_details'] = df['match_details'].apply(\n",
    "                        lambda x: str(x) if x is not None else None\n",
    "                    )\n",
    "                df.to_json(output_path, orient='records')\n",
    "                \n",
    "            elif file_ext == '.parquet':\n",
    "                # Remove complex objects for parquet\n",
    "                if 'match_details' in df.columns:\n",
    "                    df = df.drop(columns=['match_details'])\n",
    "                \n",
    "                try:\n",
    "                    import pyarrow as pa\n",
    "                    import pyarrow.parquet as pq\n",
    "                    \n",
    "                    # Convert to pyarrow table and write\n",
    "                    table = pa.Table.from_pandas(df)\n",
    "                    pq.write_table(table, output_path)\n",
    "                except ImportError:\n",
    "                    # Fall back to pandas\n",
    "                    df.to_parquet(output_path, index=False)\n",
    "            else:\n",
    "                # Default to CSV\n",
    "                csv_path = os.path.splitext(output_path)[0] + '.csv'\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                self.logger.warning(\n",
    "                    f\"Unsupported output format: {file_ext}. Saved as CSV: {csv_path}\"\n",
    "                )\n",
    "                \n",
    "            self.logger.info(f\"Results saved to {output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error writing output file: {e}\")\n",
    "            # Try to save to a backup location\n",
    "            try:\n",
    "                backup_path = os.path.join(\n",
    "                    os.getcwd(), \n",
    "                    f\"merchant_matches_backup_{time.strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                )\n",
    "                df.to_csv(backup_path, index=False)\n",
    "                self.logger.info(f\"Backup saved to {backup_path}\")\n",
    "            except:\n",
    "                pass\n",
    "            return False\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Get processing metrics\"\"\"\n",
    "        return self.metrics.copy()\n",
    "    \n",
    "    def merge_results(self, input_paths, output_path, how='inner', on=None):\n",
    "        \"\"\"\n",
    "        Merge multiple result files\n",
    "        \n",
    "        Args:\n",
    "            input_paths (list): List of input file paths\n",
    "            output_path (str): Output file path\n",
    "            how (str): Merge method ('inner', 'outer', 'left', 'right')\n",
    "            on (str or list): Column(s) to join on\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to merged output file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read all input files\n",
    "            dfs = []\n",
    "            for path in input_paths:\n",
    "                file_ext = os.path.splitext(path)[1].lower()\n",
    "                \n",
    "                if file_ext == '.csv':\n",
    "                    df = pd.read_csv(path)\n",
    "                elif file_ext in ['.xlsx', '.xls']:\n",
    "                    df = pd.read_excel(path)\n",
    "                elif file_ext == '.json':\n",
    "                    df = pd.read_json(path)\n",
    "                elif file_ext == '.parquet':\n",
    "                    df = pd.read_parquet(path)\n",
    "                else:\n",
    "                    self.logger.warning(f\"Unsupported file format: {file_ext}\")\n",
    "                    continue\n",
    "                    \n",
    "                dfs.append(df)\n",
    "            \n",
    "            if not dfs:\n",
    "                self.logger.error(\"No valid input files\")\n",
    "                return None\n",
    "                \n",
    "            # Merge all dataframes\n",
    "            result_df = dfs[0]\n",
    "            for df in dfs[1:]:\n",
    "                result_df = pd.merge(result_df, df, how=how, on=on)\n",
    "            \n",
    "            # Write merged result\n",
    "            self._write_output_file(result_df, output_path)\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"Merged {len(dfs)} files with {len(result_df)} rows to {output_path}\"\n",
    "            )\n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error merging results: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79aaa16a-45e5-4fd7-8d84-9872854a1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2: PySpark Integration for Distributed Processing\n",
    "\n",
    "class SparkMerchantMatcher:\n",
    "    \"\"\"\n",
    "    PySpark wrapper for distributed merchant name matching at scale.\n",
    "    \n",
    "    This class enables distributed processing of merchant matching tasks\n",
    "    using Apache Spark, allowing for processing of very large datasets\n",
    "    across a cluster of machines.\n",
    "    \n",
    "    Key features:\n",
    "    - Distributed matching using Spark executors\n",
    "    - Flexible partitioning strategies\n",
    "    - Built-in checkpoint and recovery\n",
    "    - Support for various input/output formats\n",
    "    - Performance optimization for large datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session=None, matcher_config=None, \n",
    "                 checkpoint_dir=None, partition_size=100000):\n",
    "        \"\"\"\n",
    "        Initialize Spark merchant matcher with configuration\n",
    "        \n",
    "        Args:\n",
    "            spark_session: Existing SparkSession or None to create new\n",
    "            matcher_config (dict): Configuration for matcher components\n",
    "            checkpoint_dir (str): Directory for checkpointing\n",
    "            partition_size (int): Target size for partitions\n",
    "        \"\"\"\n",
    "        self.matcher_config = matcher_config or {}\n",
    "        self.partition_size = partition_size\n",
    "        \n",
    "        # Initialize Spark session if not provided\n",
    "        if spark_session is None:\n",
    "            try:\n",
    "                from pyspark.sql import SparkSession\n",
    "                \n",
    "                # Create Spark session\n",
    "                self.spark = SparkSession.builder \\\n",
    "                    .appName(\"MerchantMatcher\") \\\n",
    "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "                    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "                    .config(\"spark.executor.cores\", \"2\") \\\n",
    "                    .getOrCreate()\n",
    "                    \n",
    "                self.logger = self.spark.sparkContext._jvm.org.apache.log4j.LogManager \\\n",
    "                    .getLogger(\"SparkMerchantMatcher\")\n",
    "                    \n",
    "            except ImportError:\n",
    "                raise ImportError(\"PySpark is required for SparkMerchantMatcher\")\n",
    "        else:\n",
    "            self.spark = spark_session\n",
    "            self.logger = None  # Use Python logging\n",
    "            \n",
    "        # Set checkpoint directory if provided\n",
    "        if checkpoint_dir:\n",
    "            self.spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
    "            \n",
    "        # Track initialization state\n",
    "        self._matcher_broadcast = None\n",
    "        self._thresholds_broadcast = None\n",
    "        self._matcher_initialized = False\n",
    "    \n",
    "    def _initialize_matcher_broadcast(self):\n",
    "        \"\"\"Initialize and broadcast the matcher to all executors\"\"\"\n",
    "        if self._matcher_initialized:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            import pickle\n",
    "            import base64\n",
    "            \n",
    "            # Create a serializable version of matcher components\n",
    "            matcher_serialized = {\n",
    "                \"weights\": self.matcher_config.get(\"weights\", {\n",
    "                    \"string_similarity\": 0.20,\n",
    "                    \"token_set_similarity\": 0.15,\n",
    "                    \"semantic_similarity\": 0.30,\n",
    "                    \"pattern_match\": 0.25,\n",
    "                    \"contains_check\": 0.05,\n",
    "                    \"acronym_check\": 0.05\n",
    "                }),\n",
    "                \"thresholds\": self.matcher_config.get(\"thresholds\", {\n",
    "                    \"high\": 0.85,\n",
    "                    \"medium\": 0.75,\n",
    "                    \"low\": 0.60\n",
    "                })\n",
    "            }\n",
    "            \n",
    "            # Broadcast configuration to all executors\n",
    "            self._thresholds_broadcast = self.spark.sparkContext.broadcast(\n",
    "                matcher_serialized[\"thresholds\"]\n",
    "            )\n",
    "            \n",
    "            # Note: We don't broadcast the full matcher object because:\n",
    "            # 1. It may contain non-serializable components (like BERT model)\n",
    "            # 2. It's more efficient to initialize the matcher on each executor\n",
    "            \n",
    "            self._matcher_initialized = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"Failed to initialize matcher broadcast: {e}\")\n",
    "            else:\n",
    "                print(f\"Failed to initialize matcher broadcast: {e}\")\n",
    "    \n",
    "    def process_dataframe(self, df, s1_col='s1', s2_col='s2', domain_col=None,\n",
    "                          id_col=None, output_path=None, output_format='csv'):\n",
    "        \"\"\"\n",
    "        Process a Spark DataFrame with merchant name pairs\n",
    "        \n",
    "        Args:\n",
    "            df: Spark DataFrame with merchant name pairs\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            domain_col (str, optional): Column name for domain\n",
    "            id_col (str, optional): Column name for record ID\n",
    "            output_path (str, optional): Path to save results\n",
    "            output_format (str): Output format (csv, parquet, json)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Spark DataFrame with match results\n",
    "        \"\"\"\n",
    "        from pyspark.sql.functions import udf, col, lit\n",
    "        from pyspark.sql.types import DoubleType, StringType, StructType, StructField\n",
    "        \n",
    "        # Initialize matcher configuration broadcast\n",
    "        self._initialize_matcher_broadcast()\n",
    "        \n",
    "        # Define the schema for the result\n",
    "        result_schema = StructType([\n",
    "            StructField(\"match_score\", DoubleType(), True),\n",
    "            StructField(\"match_level\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Define matching UDF for Spark\n",
    "        def match_merchants_udf(s1, s2, domain=None):\n",
    "            \"\"\"UDF for merchant matching in Spark executors\"\"\"\n",
    "            try:\n",
    "                # Import here for executor scope\n",
    "                import re\n",
    "                import numpy as np\n",
    "                from Levenshtein import distance as levenshtein_distance\n",
    "                from Levenshtein import jaro_winkler, ratio as levenshtein_ratio\n",
    "                \n",
    "                # If both inputs are empty, return no match\n",
    "                if not s1 or not s2 or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                    return (0.0, \"no_match\")\n",
    "                \n",
    "                # Simplified preprocessing\n",
    "                def preprocess(text):\n",
    "                    if not isinstance(text, str):\n",
    "                        return \"\"\n",
    "                    # Lowercase and trim\n",
    "                    text = text.lower().strip()\n",
    "                    # Remove most punctuation and normalize spaces\n",
    "                    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "                    # Normalize spaces\n",
    "                    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                    return text\n",
    "                \n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                \n",
    "                # If preprocessed strings are empty, return no match\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return (0.0, \"no_match\")\n",
    "                \n",
    "                # Fast exact match\n",
    "                if s1_clean == s2_clean:\n",
    "                    return (1.0, \"high_match\")\n",
    "                \n",
    "                # Get thresholds from broadcast\n",
    "                thresholds = _thresholds_bc.value\n",
    "                \n",
    "                # Calculate string similarities\n",
    "                jw_similarity = jaro_winkler(s1_clean, s2_clean)\n",
    "                \n",
    "                # Token set similarity\n",
    "                def token_set_ratio(s1, s2):\n",
    "                    s1_words = set(s1.split())\n",
    "                    s2_words = set(s2.split())\n",
    "                    \n",
    "                    intersection = s1_words.intersection(s2_words)\n",
    "                    union = s1_words.union(s2_words)\n",
    "                    \n",
    "                    if not union:\n",
    "                        return 0.0\n",
    "                    \n",
    "                    return len(intersection) / len(union)\n",
    "                \n",
    "                token_sim = token_set_ratio(s1_clean, s2_clean)\n",
    "                \n",
    "                # Contains check\n",
    "                contains_score = 0.0\n",
    "                if s1_clean in s2_clean or s2_clean in s1_clean:\n",
    "                    contains_score = 1.0\n",
    "                \n",
    "                # Combine scores with simple weights\n",
    "                score = (jw_similarity * 0.5) + (token_sim * 0.3) + (contains_score * 0.2)\n",
    "                \n",
    "                # Determine match level\n",
    "                if score >= thresholds[\"high\"]:\n",
    "                    level = \"high_match\"\n",
    "                elif score >= thresholds[\"medium\"]:\n",
    "                    level = \"medium_match\"\n",
    "                elif score >= thresholds[\"low\"]:\n",
    "                    level = \"low_match\"\n",
    "                else:\n",
    "                    level = \"no_match\"\n",
    "                \n",
    "                return (score, level)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # In case of any error, return no match\n",
    "                return (0.0, \"no_match\")\n",
    "        \n",
    "        # Register broadcast variables in UDF\n",
    "        _thresholds_bc = self._thresholds_broadcast\n",
    "        \n",
    "        # Create UDF\n",
    "        matcher_udf = udf(match_merchants_udf, result_schema)\n",
    "        \n",
    "        # Apply matching to DataFrame\n",
    "        if domain_col and domain_col in df.columns:\n",
    "            result_df = df.withColumn(\n",
    "                \"match_result\",\n",
    "                matcher_udf(col(s1_col), col(s2_col), col(domain_col))\n",
    "            )\n",
    "        else:\n",
    "            result_df = df.withColumn(\n",
    "                \"match_result\",\n",
    "                matcher_udf(col(s1_col), col(s2_col), lit(None))\n",
    "            )\n",
    "        \n",
    "        # Extract fields from struct\n",
    "        result_df = result_df \\\n",
    "            .withColumn(\"match_score\", col(\"match_result.match_score\")) \\\n",
    "            .withColumn(\"match_level\", col(\"match_result.match_level\")) \\\n",
    "            .drop(\"match_result\")\n",
    "        \n",
    "        # Save results if output path provided\n",
    "        if output_path:\n",
    "            if output_format == 'csv':\n",
    "                result_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "            elif output_format == 'parquet':\n",
    "                result_df.write.parquet(output_path, mode=\"overwrite\")\n",
    "            elif output_format == 'json':\n",
    "                result_df.write.json(output_path, mode=\"overwrite\")\n",
    "            else:\n",
    "                # Default to parquet\n",
    "                result_df.write.parquet(output_path, mode=\"overwrite\")\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def process_file(self, input_path, output_path=None, s1_col='s1', s2_col='s2',\n",
    "                    domain_col=None, output_format='csv', options=None):\n",
    "        \"\"\"\n",
    "        Process a file containing merchant name pairs using Spark\n",
    "        \n",
    "        Args:\n",
    "            input_path (str): Path to input file\n",
    "            output_path (str, optional): Path to output directory\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            domain_col (str, optional): Column name for domain\n",
    "            output_format (str): Output format (csv, parquet, json)\n",
    "            options (dict): Additional options for reading input\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Spark DataFrame with match results\n",
    "        \"\"\"\n",
    "        # Determine file format from extension\n",
    "        file_format = input_path.split('.')[-1].lower()\n",
    "        read_options = options or {}\n",
    "        \n",
    "        # Read input file\n",
    "        if file_format == 'csv':\n",
    "            df = self.spark.read.csv(input_path, header=True, inferSchema=True, **read_options)\n",
    "        elif file_format in ['xls', 'xlsx']:\n",
    "            # Excel requires additional libraries\n",
    "            try:\n",
    "                # Read using pandas and convert to Spark DataFrame\n",
    "                import pandas as pd\n",
    "                pandas_df = pd.read_excel(input_path, **read_options)\n",
    "                df = self.spark.createDataFrame(pandas_df)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"pandas is required for reading Excel files\")\n",
    "        elif file_format == 'json':\n",
    "            df = self.spark.read.json(input_path, **read_options)\n",
    "        elif file_format == 'parquet':\n",
    "            df = self.spark.read.parquet(input_path, **read_options)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "        \n",
    "        # Process the DataFrame\n",
    "        return self.process_dataframe(\n",
    "            df, s1_col, s2_col, domain_col, output_path=output_path, output_format=output_format\n",
    "        )\n",
    "    \n",
    "    def find_matches(self, query_df, candidate_df, query_col='merchant_name', \n",
    "                     candidate_col='merchant_name', threshold=0.6, top_k=5,\n",
    "                     domain_col=None, output_path=None, output_format='csv'):\n",
    "        \"\"\"\n",
    "        Find best matches for each query merchant from candidates\n",
    "        \n",
    "        Args:\n",
    "            query_df: Spark DataFrame with query merchants\n",
    "            candidate_df: Spark DataFrame with candidate merchants\n",
    "            query_col (str): Column name for query merchant names\n",
    "            candidate_col (str): Column name for candidate merchant names\n",
    "            threshold (float): Minimum score threshold\n",
    "            top_k (int): Maximum number of matches to return per query\n",
    "            domain_col (str, optional): Column name for domain\n",
    "            output_path (str, optional): Path to save results\n",
    "            output_format (str): Output format (csv, parquet, json)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Spark DataFrame with match results\n",
    "        \"\"\"\n",
    "        from pyspark.sql.functions import udf, col, lit, row_number, desc\n",
    "        from pyspark.sql.types import DoubleType, StringType, StructType, StructField\n",
    "        from pyspark.sql.window import Window\n",
    "        \n",
    "        # Initialize matcher configuration broadcast\n",
    "        self._initialize_matcher_broadcast()\n",
    "        \n",
    "        # Define the schema for the result\n",
    "        result_schema = StructType([\n",
    "            StructField(\"match_score\", DoubleType(), True),\n",
    "            StructField(\"match_level\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Define simplified matching UDF for Spark\n",
    "        def match_merchants_udf(s1, s2, domain=None):\n",
    "            \"\"\"UDF for merchant matching in Spark executors\"\"\"\n",
    "            try:\n",
    "                # Import here for executor scope\n",
    "                import re\n",
    "                import numpy as np\n",
    "                from Levenshtein import distance as levenshtein_distance\n",
    "                from Levenshtein import jaro_winkler, ratio as levenshtein_ratio\n",
    "                \n",
    "                # If both inputs are empty, return no match\n",
    "                if not s1 or not s2 or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                    return (0.0, \"no_match\")\n",
    "                \n",
    "                # Simplified preprocessing\n",
    "                def preprocess(text):\n",
    "                    if not isinstance(text, str):\n",
    "                        return \"\"\n",
    "                    # Lowercase and trim\n",
    "                    text = text.lower().strip()\n",
    "                    # Remove most punctuation and normalize spaces\n",
    "                    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "                    # Normalize spaces\n",
    "                    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                    return text\n",
    "                \n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                \n",
    "                # If preprocessed strings are empty, return no match\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return (0.0, \"no_match\")\n",
    "                \n",
    "                # Fast exact match\n",
    "                if s1_clean == s2_clean:\n",
    "                    return (1.0, \"high_match\")\n",
    "                \n",
    "                # Get thresholds from broadcast\n",
    "                thresholds = _thresholds_bc.value\n",
    "                \n",
    "                # Calculate string similarities\n",
    "                jw_similarity = jaro_winkler(s1_clean, s2_clean)\n",
    "                \n",
    "                # Token set similarity\n",
    "                def token_set_ratio(s1, s2):\n",
    "                    s1_words = set(s1.split())\n",
    "                    s2_words = set(s2.split())\n",
    "                    \n",
    "                    intersection = s1_words.intersection(s2_words)\n",
    "                    union = s1_words.union(s2_words)\n",
    "                    \n",
    "                    if not union:\n",
    "                        return 0.0\n",
    "                    \n",
    "                    return len(intersection) / len(union)\n",
    "                \n",
    "                token_sim = token_set_ratio(s1_clean, s2_clean)\n",
    "                \n",
    "                # Contains check\n",
    "                contains_score = 0.0\n",
    "                if s1_clean in s2_clean or s2_clean in s1_clean:\n",
    "                    contains_score = 1.0\n",
    "                \n",
    "                # Combine scores with simple weights\n",
    "                score = (jw_similarity * 0.5) + (token_sim * 0.3) + (contains_score * 0.2)\n",
    "                \n",
    "                # Determine match level\n",
    "                if score >= thresholds[\"high\"]:\n",
    "                    level = \"high_match\"\n",
    "                elif score >= thresholds[\"medium\"]:\n",
    "                    level = \"medium_match\"\n",
    "                elif score >= thresholds[\"low\"]:\n",
    "                    level = \"low_match\"\n",
    "                else:\n",
    "                    level = \"no_match\"\n",
    "                \n",
    "                return (score, level)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # In case of any error, return no match\n",
    "                return (0.0, \"no_match\")\n",
    "        \n",
    "        # Register broadcast variables in UDF\n",
    "        _thresholds_bc = self._thresholds_broadcast\n",
    "        \n",
    "        # Create UDF\n",
    "        matcher_udf = udf(match_merchants_udf, result_schema)\n",
    "        \n",
    "        # Create cross join with query and candidate DataFrames\n",
    "        cross_df = query_df.crossJoin(\n",
    "            candidate_df.select(\n",
    "                col(candidate_col).alias(\"candidate_name\"), \n",
    "                *[col(c) for c in candidate_df.columns if c != candidate_col]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Apply matching to cross join\n",
    "        if domain_col and domain_col in cross_df.columns:\n",
    "            match_df = cross_df.withColumn(\n",
    "                \"match_result\",\n",
    "                matcher_udf(col(query_col), col(\"candidate_name\"), col(domain_col))\n",
    "            )\n",
    "        else:\n",
    "            match_df = cross_df.withColumn(\n",
    "                \"match_result\",\n",
    "                matcher_udf(col(query_col), col(\"candidate_name\"), lit(None))\n",
    "            )\n",
    "        \n",
    "        # Extract fields from struct\n",
    "        match_df = match_df \\\n",
    "            .withColumn(\"match_score\", col(\"match_result.match_score\")) \\\n",
    "            .withColumn(\"match_level\", col(\"match_result.match_level\")) \\\n",
    "            .drop(\"match_result\")\n",
    "        \n",
    "        # Filter by threshold\n",
    "        match_df = match_df.filter(col(\"match_score\") >= threshold)\n",
    "        \n",
    "        # Get top-k matches for each query\n",
    "        window_spec = Window.partitionBy(query_col).orderBy(desc(\"match_score\"))\n",
    "        top_matches_df = match_df \\\n",
    "            .withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "            .filter(col(\"rank\") <= top_k) \\\n",
    "            .drop(\"rank\")\n",
    "        \n",
    "        # Save results if output path provided\n",
    "        if output_path:\n",
    "            if output_format == 'csv':\n",
    "                top_matches_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "            elif output_format == 'parquet':\n",
    "                top_matches_df.write.parquet(output_path, mode=\"overwrite\")\n",
    "            elif output_format == 'json':\n",
    "                top_matches_df.write.json(output_path, mode=\"overwrite\")\n",
    "            else:\n",
    "                # Default to parquet\n",
    "                top_matches_df.write.parquet(output_path, mode=\"overwrite\")\n",
    "        \n",
    "        return top_matches_df\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the Spark session\"\"\"\n",
    "        if hasattr(self, 'spark') and self.spark:\n",
    "            self.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3615cee-a206-4586-9bc9-52ad35a1455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3: Performance Optimization and Distributed Analysis\n",
    "\n",
    "class MerchantMatchingDistributor:\n",
    "    \"\"\"\n",
    "    Advanced distributed workflow manager for merchant name matching\n",
    "    with sophisticated workload partitioning and monitoring.\n",
    "    \n",
    "    This class provides high-level functions to efficiently distribute\n",
    "    merchant matching tasks across computation resources, with automatic\n",
    "    performance tuning and workflow management.\n",
    "    \n",
    "    Key features:\n",
    "    - Adaptive partitioning based on data characteristics\n",
    "    - Hybrid execution model (local + distributed)\n",
    "    - Incremental processing with checkpoints\n",
    "    - Smart resource allocation\n",
    "    - Performance monitoring dashboard\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config=None, use_spark=False, use_dask=False,\n",
    "                 min_partition_size=1000, max_workers=None, dashboard=False):\n",
    "        \"\"\"\n",
    "        Initialize the distributor with configuration options\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Configuration dictionary\n",
    "            use_spark (bool): Whether to use PySpark for distributed processing\n",
    "            use_dask (bool): Whether to use Dask for distributed processing\n",
    "            min_partition_size (int): Minimum partition size\n",
    "            max_workers (int): Maximum number of workers\n",
    "            dashboard (bool): Enable performance dashboard\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self.use_spark = use_spark\n",
    "        self.use_dask = use_dask\n",
    "        self.min_partition_size = min_partition_size\n",
    "        self.max_workers = max_workers\n",
    "        self.dashboard = dashboard\n",
    "        \n",
    "        # Track performance metrics\n",
    "        self.metrics = {\n",
    "            'processing_time': 0,\n",
    "            'records_processed': 0,\n",
    "            'throughput': 0,\n",
    "            'partitions': 0,\n",
    "            'partition_distribution': {},\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "        }\n",
    "        \n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger('MerchantDistributor')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Initialize components based on configuration\n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize distributed processing components\"\"\"\n",
    "        self.matcher = None\n",
    "        self.spark_matcher = None\n",
    "        self.dask_client = None\n",
    "        \n",
    "        # Initialize matcher\n",
    "        matcher_config = self.config.get('matcher', {})\n",
    "        \n",
    "        # Create EnhancedMerchantMatcher instance\n",
    "        if not hasattr(self, 'matcher') or self.matcher is None:\n",
    "            try:\n",
    "                # Use reference to EnhancedMerchantMatcher from previously imported code\n",
    "                # Assuming it has been properly defined in previous cells\n",
    "                self.matcher = EnhancedMerchantMatcher(\n",
    "                    weights=matcher_config.get('weights'),\n",
    "                    thresholds=matcher_config.get('thresholds')\n",
    "                )\n",
    "                self.logger.info(\"Initialized EnhancedMerchantMatcher\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to initialize EnhancedMerchantMatcher: {e}\")\n",
    "                self.matcher = None\n",
    "        \n",
    "        # Initialize BatchProcessor\n",
    "        self.batch_processor = BatchProcessor(\n",
    "            matcher=self.matcher,\n",
    "            chunk_size=self.config.get('batch_size', 10000),\n",
    "            n_jobs=self.max_workers or -1\n",
    "        )\n",
    "        \n",
    "        # Initialize Spark if requested\n",
    "        if self.use_spark:\n",
    "            try:\n",
    "                from pyspark.sql import SparkSession\n",
    "                \n",
    "                # Create or get Spark session\n",
    "                spark = SparkSession.builder \\\n",
    "                    .appName(\"MerchantMatching\") \\\n",
    "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                    .getOrCreate()\n",
    "                \n",
    "                # Initialize Spark matcher\n",
    "                self.spark_matcher = SparkMerchantMatcher(\n",
    "                    spark_session=spark,\n",
    "                    matcher_config=matcher_config,\n",
    "                    checkpoint_dir=self.config.get('checkpoint_dir')\n",
    "                )\n",
    "                \n",
    "                self.logger.info(\"Initialized Spark-based distributed processing\")\n",
    "                \n",
    "            except ImportError:\n",
    "                self.logger.warning(\"PySpark not available. Falling back to local processing.\")\n",
    "                self.use_spark = False\n",
    "        \n",
    "        # Initialize Dask if requested\n",
    "        if self.use_dask:\n",
    "            try:\n",
    "                import dask\n",
    "                import dask.dataframe as dd\n",
    "                from dask.distributed import Client, LocalCluster\n",
    "                \n",
    "                # Create local cluster if not using existing\n",
    "                if self.config.get('dask_scheduler_address'):\n",
    "                    self.dask_client = Client(self.config['dask_scheduler_address'])\n",
    "                else:\n",
    "                    # Create local cluster with specified workers\n",
    "                    cluster = LocalCluster(\n",
    "                        n_workers=self.max_workers or 4,\n",
    "                        threads_per_worker=2,\n",
    "                        memory_limit='4GB'\n",
    "                    )\n",
    "                    self.dask_client = Client(cluster)\n",
    "                \n",
    "                if self.dashboard:\n",
    "                    self.logger.info(f\"Dask dashboard available at: {self.dask_client.dashboard_link}\")\n",
    "                \n",
    "                self.logger.info(\"Initialized Dask-based distributed processing\")\n",
    "                \n",
    "            except ImportError:\n",
    "                self.logger.warning(\"Dask not available. Falling back to local processing.\")\n",
    "                self.use_dask = False\n",
    "    \n",
    "    def _analyze_dataset(self, data_source):\n",
    "        \"\"\"\n",
    "        Analyze dataset to determine optimal partitioning strategy\n",
    "        \n",
    "        Args:\n",
    "            data_source: DataFrame or file path\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dataset characteristics\n",
    "        \"\"\"\n",
    "        # Initialize results\n",
    "        analysis = {\n",
    "            'record_count': 0,\n",
    "            'estimated_size_mb': 0,\n",
    "            'recommended_partitions': 0,\n",
    "            'columns': [],\n",
    "            'has_domain_column': False,\n",
    "            'string_columns': [],\n",
    "            'domain_distribution': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Handle different input types\n",
    "            if isinstance(data_source, str):\n",
    "                # Analyze file size\n",
    "                import os\n",
    "                file_size = os.path.getsize(data_source) / (1024 * 1024)  # Size in MB\n",
    "                analysis['estimated_size_mb'] = file_size\n",
    "                \n",
    "                # Sample file to analyze structure\n",
    "                import pandas as pd\n",
    "                \n",
    "                # Determine file format\n",
    "                file_ext = os.path.splitext(data_source)[1].lower()\n",
    "                \n",
    "                if file_ext == '.csv':\n",
    "                    # Read sample of CSV\n",
    "                    sample_df = pd.read_csv(data_source, nrows=10000)\n",
    "                elif file_ext in ['.xlsx', '.xls']:\n",
    "                    # Read sample of Excel\n",
    "                    sample_df = pd.read_excel(data_source, nrows=10000)\n",
    "                elif file_ext == '.json':\n",
    "                    # Read sample of JSON\n",
    "                    sample_df = pd.read_json(data_source, lines=True, nrows=10000)\n",
    "                elif file_ext == '.parquet':\n",
    "                    # Read sample of Parquet\n",
    "                    sample_df = pd.read_parquet(data_source)\n",
    "                    # Limit to 10000 rows\n",
    "                    if len(sample_df) > 10000:\n",
    "                        sample_df = sample_df.iloc[:10000]\n",
    "                else:\n",
    "                    self.logger.warning(f\"Unsupported file format for analysis: {file_ext}\")\n",
    "                    return analysis\n",
    "                \n",
    "                # Use the sample for column analysis\n",
    "                analysis['columns'] = list(sample_df.columns)\n",
    "                analysis['string_columns'] = [\n",
    "                    col for col in sample_df.columns \n",
    "                    if sample_df[col].dtype == 'object'\n",
    "                ]\n",
    "                \n",
    "                # Check for domain column\n",
    "                if 'domain' in sample_df.columns:\n",
    "                    analysis['has_domain_column'] = True\n",
    "                    # Get domain distribution\n",
    "                    domain_counts = sample_df['domain'].value_counts()\n",
    "                    analysis['domain_distribution'] = domain_counts.to_dict()\n",
    "                \n",
    "                # Estimate total records for file types\n",
    "                if file_ext == '.csv':\n",
    "                    # Estimate total lines\n",
    "                    with open(data_source, 'r') as f:\n",
    "                        for i, _ in enumerate(f):\n",
    "                            if i >= 100000:  # Limit to prevent slow performance\n",
    "                                break\n",
    "                        total_lines = i + 1\n",
    "                    \n",
    "                    # Adjust for header\n",
    "                    analysis['record_count'] = total_lines - 1\n",
    "                else:\n",
    "                    # For other formats, we might not know exact count\n",
    "                    # Use the sample ratio to estimate\n",
    "                    avg_row_size = file_size / len(sample_df) if len(sample_df) > 0 else 0\n",
    "                    if avg_row_size > 0:\n",
    "                        analysis['record_count'] = int(file_size / avg_row_size)\n",
    "                    else:\n",
    "                        analysis['record_count'] = 0\n",
    "            \n",
    "            elif hasattr(data_source, 'shape'):\n",
    "                # Pandas DataFrame\n",
    "                analysis['record_count'] = data_source.shape[0]\n",
    "                analysis['columns'] = list(data_source.columns)\n",
    "                analysis['string_columns'] = [\n",
    "                    col for col in data_source.columns \n",
    "                    if data_source[col].dtype == 'object'\n",
    "                ]\n",
    "                \n",
    "                # Check for domain column\n",
    "                if 'domain' in data_source.columns:\n",
    "                    analysis['has_domain_column'] = True\n",
    "                    # Get domain distribution\n",
    "                    domain_counts = data_source['domain'].value_counts()\n",
    "                    analysis['domain_distribution'] = domain_counts.to_dict()\n",
    "                \n",
    "                # Estimate size\n",
    "                analysis['estimated_size_mb'] = data_source.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "            \n",
    "            else:\n",
    "                self.logger.warning(f\"Unsupported data source type: {type(data_source)}\")\n",
    "                return analysis\n",
    "            \n",
    "            # Calculate recommended partitions\n",
    "            if analysis['record_count'] > 0:\n",
    "                # Base recommendation on records and estimated size\n",
    "                size_based = max(1, int(analysis['estimated_size_mb'] / 100))  # ~100MB per partition\n",
    "                count_based = max(1, int(analysis['record_count'] / self.min_partition_size))\n",
    "                \n",
    "                analysis['recommended_partitions'] = max(size_based, count_based)\n",
    "                \n",
    "                # Limit by available workers\n",
    "                if self.max_workers:\n",
    "                    analysis['recommended_partitions'] = min(\n",
    "                        analysis['recommended_partitions'], \n",
    "                        self.max_workers * 2\n",
    "                    )\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing dataset: {e}\")\n",
    "            return analysis\n",
    "    \n",
    "    def process_dataset(self, data_source, output_path=None, s1_col='s1', s2_col='s2',\n",
    "                        domain_col=None, id_col=None, return_detailed=False):\n",
    "        \"\"\"\n",
    "        Process a dataset with optimized distributed execution\n",
    "        \n",
    "        Args:\n",
    "            data_source: DataFrame or file path\n",
    "            output_path (str, optional): Path to save results\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            domain_col (str, optional): Column name for domain\n",
    "            id_col (str, optional): Column name for record ID\n",
    "            return_detailed (bool): Whether to return detailed match info\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame or str: Results DataFrame or path to results file\n",
    "        \"\"\"\n",
    "        # Analyze dataset for optimal partitioning\n",
    "        self.metrics['start_time'] = time.time()\n",
    "        \n",
    "        analysis = self._analyze_dataset(data_source)\n",
    "        self.logger.info(\n",
    "            f\"Dataset analysis: {analysis['record_count']} records, \"\n",
    "            f\"{analysis['estimated_size_mb']:.2f} MB, \"\n",
    "            f\"{analysis['recommended_partitions']} recommended partitions\"\n",
    "        )\n",
    "        \n",
    "        # Choose execution strategy based on dataset and available resources\n",
    "        if self.use_spark and analysis['record_count'] > 100000:\n",
    "            # Use Spark for very large datasets\n",
    "            self.logger.info(\"Using Spark for distributed processing\")\n",
    "            return self._process_with_spark(\n",
    "                data_source, output_path, s1_col, s2_col, domain_col, id_col\n",
    "            )\n",
    "        elif self.use_dask and analysis['record_count'] > 50000:\n",
    "            # Use Dask for large datasets\n",
    "            self.logger.info(\"Using Dask for distributed processing\")\n",
    "            return self._process_with_dask(\n",
    "                data_source, output_path, s1_col, s2_col, domain_col, id_col, return_detailed\n",
    "            )\n",
    "        else:\n",
    "            # Use batch processor for smaller datasets\n",
    "            self.logger.info(\"Using batch processor for local processing\")\n",
    "            return self._process_with_batch(\n",
    "                data_source, output_path, s1_col, s2_col, domain_col, id_col, return_detailed\n",
    "            )\n",
    "    \n",
    "    def _process_with_batch(self, data_source, output_path, s1_col, s2_col, \n",
    "                           domain_col, id_col, return_detailed):\n",
    "        \"\"\"Process using BatchProcessor\"\"\"\n",
    "        try:\n",
    "            # Handle different input types\n",
    "            if isinstance(data_source, str):\n",
    "                # Process file\n",
    "                result_path = self.batch_processor.process_file(\n",
    "                    data_source, output_path, s1_col, s2_col, domain_col, id_col, return_detailed\n",
    "                )\n",
    "                \n",
    "                # Update metrics\n",
    "                self.metrics.update(self.batch_processor.get_metrics())\n",
    "                \n",
    "                # Return path to results\n",
    "                return result_path\n",
    "            else:\n",
    "                # Process DataFrame\n",
    "                result_df = self.batch_processor._process_chunk(\n",
    "                    data_source, s1_col, s2_col, domain_col, id_col, return_detailed\n",
    "                )\n",
    "                \n",
    "                # Save if output path provided\n",
    "                if output_path:\n",
    "                    self.batch_processor._write_output_file(result_df, output_path)\n",
    "                \n",
    "                # Update metrics\n",
    "                self.metrics['records_processed'] = len(result_df)\n",
    "                self.metrics['processing_time'] = time.time() - self.metrics['start_time']\n",
    "                if self.metrics['processing_time'] > 0:\n",
    "                    self.metrics['throughput'] = self.metrics['records_processed'] / self.metrics['processing_time']\n",
    "                \n",
    "                return result_df\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in batch processing: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.metrics['end_time'] = time.time()\n",
    "    \n",
    "    def _process_with_spark(self, data_source, output_path, s1_col, s2_col, domain_col, id_col):\n",
    "        \"\"\"Process using Spark\"\"\"\n",
    "        try:\n",
    "            if self.spark_matcher is None:\n",
    "                self.logger.error(\"Spark matcher not initialized\")\n",
    "                return None\n",
    "            \n",
    "            # Process with Spark\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if isinstance(data_source, str):\n",
    "                # Process file\n",
    "                result_df = self.spark_matcher.process_file(\n",
    "                    data_source, output_path, s1_col, s2_col, domain_col\n",
    "                )\n",
    "            else:\n",
    "                # Convert to Spark DataFrame if needed\n",
    "                if hasattr(data_source, 'toPandas'):\n",
    "                    # Already a Spark DataFrame\n",
    "                    spark_df = data_source\n",
    "                else:\n",
    "                    # Convert Pandas DataFrame to Spark DataFrame\n",
    "                    spark_df = self.spark_matcher.spark.createDataFrame(data_source)\n",
    "                \n",
    "                # Process DataFrame\n",
    "                result_df = self.spark_matcher.process_dataframe(\n",
    "                    spark_df, s1_col, s2_col, domain_col, id_col, output_path\n",
    "                )\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics['records_processed'] = result_df.count()\n",
    "            self.metrics['processing_time'] = time.time() - start_time\n",
    "            if self.metrics['processing_time'] > 0:\n",
    "                self.metrics['throughput'] = self.metrics['records_processed'] / self.metrics['processing_time']\n",
    "            \n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in Spark processing: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.metrics['end_time'] = time.time()\n",
    "    \n",
    "    def _process_with_dask(self, data_source, output_path, s1_col, s2_col, \n",
    "                          domain_col, id_col, return_detailed):\n",
    "        \"\"\"Process using Dask\"\"\"\n",
    "        try:\n",
    "            if self.dask_client is None:\n",
    "                self.logger.error(\"Dask client not initialized\")\n",
    "                return None\n",
    "            \n",
    "            import dask.dataframe as dd\n",
    "            import pandas as pd\n",
    "            \n",
    "            # Convert input to Dask DataFrame\n",
    "            if isinstance(data_source, str):\n",
    "                # Read file with Dask\n",
    "                file_ext = os.path.splitext(data_source)[1].lower()\n",
    "                \n",
    "                if file_ext == '.csv':\n",
    "                    dask_df = dd.read_csv(data_source)\n",
    "                elif file_ext in ['.xlsx', '.xls']:\n",
    "                    # Dask doesn't support Excel natively, use pandas\n",
    "                    pandas_df = pd.read_excel(data_source)\n",
    "                    dask_df = dd.from_pandas(pandas_df, npartitions=self.max_workers or 4)\n",
    "                elif file_ext == '.json':\n",
    "                    dask_df = dd.read_json(data_source, lines=True)\n",
    "                elif file_ext == '.parquet':\n",
    "                    dask_df = dd.read_parquet(data_source)\n",
    "                else:\n",
    "                    self.logger.error(f\"Unsupported file format for Dask: {file_ext}\")\n",
    "                    return None\n",
    "            elif isinstance(data_source, pd.DataFrame):\n",
    "                # Convert Pandas DataFrame to Dask DataFrame\n",
    "                dask_df = dd.from_pandas(data_source, npartitions=self.max_workers or 4)\n",
    "            elif isinstance(data_source, dd.DataFrame):\n",
    "                # Already a Dask DataFrame\n",
    "                dask_df = data_source\n",
    "            else:\n",
    "                self.logger.error(f\"Unsupported data source type for Dask: {type(data_source)}\")\n",
    "                return None\n",
    "            \n",
    "            # Define matching function for Dask\n",
    "            def match_merchants_func(df):\n",
    "                \"\"\"Function to apply matcher to a partition\"\"\"\n",
    "                result_df = df.copy()\n",
    "                \n",
    "                # Add result columns if they don't exist\n",
    "                if 'match_score' not in result_df.columns:\n",
    "                    result_df['match_score'] = 0.0\n",
    "                if 'match_level' not in result_df.columns:\n",
    "                    result_df['match_level'] = 'no_match'\n",
    "                if return_detailed and 'match_details' not in result_df.columns:\n",
    "                    result_df['match_details'] = None\n",
    "                \n",
    "                # Process each row\n",
    "                for idx, row in result_df.iterrows():\n",
    "                    try:\n",
    "                        s1 = row[s1_col]\n",
    "                        s2 = row[s2_col]\n",
    "                        \n",
    "                        # Skip if empty\n",
    "                        if pd.isna(s1) or pd.isna(s2) or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                            continue\n",
    "                        \n",
    "                        # Get domain if available\n",
    "                        domain = row[domain_col] if domain_col and domain_col in row else None\n",
    "                        \n",
    "                        # Match with simplified algorithm\n",
    "                        from Levenshtein import jaro_winkler\n",
    "                        \n",
    "                        # Simplified preprocessing\n",
    "                        def preprocess(text):\n",
    "                            if not isinstance(text, str):\n",
    "                                return \"\"\n",
    "                            # Lowercase and trim\n",
    "                            text = text.lower().strip()\n",
    "                            # Remove punctuation and normalize spaces\n",
    "                            import re\n",
    "                            text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "                            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                            return text\n",
    "                        \n",
    "                        s1_clean = preprocess(s1)\n",
    "                        s2_clean = preprocess(s2)\n",
    "                        \n",
    "                        # If preprocessed strings are empty, skip\n",
    "                        if not s1_clean or not s2_clean:\n",
    "                            continue\n",
    "                        \n",
    "                        # Fast exact match\n",
    "                        if s1_clean == s2_clean:\n",
    "                            result_df.at[idx, 'match_score'] = 1.0\n",
    "                            result_df.at[idx, 'match_level'] = 'high_match'\n",
    "                            continue\n",
    "                        \n",
    "                        # Calculate similarity\n",
    "                        jw_sim = jaro_winkler(s1_clean, s2_clean)\n",
    "                        \n",
    "                        # Simple token similarity\n",
    "                        def token_sim(s1, s2):\n",
    "                            s1_words = set(s1.split())\n",
    "                            s2_words = set(s2.split())\n",
    "                            if not s1_words or not s2_words:\n",
    "                                return 0.0\n",
    "                            return len(s1_words.intersection(s2_words)) / len(s1_words.union(s2_words))\n",
    "                        \n",
    "                        token_similarity = token_sim(s1_clean, s2_clean)\n",
    "                        \n",
    "                        # Contains check\n",
    "                        contains = 1.0 if s1_clean in s2_clean or s2_clean in s1_clean else 0.0\n",
    "                        \n",
    "                        # Combine with simple weights\n",
    "                        score = 0.5 * jw_sim + 0.3 * token_similarity + 0.2 * contains\n",
    "                        \n",
    "                        # Determine level\n",
    "                        if score >= 0.85:\n",
    "                            level = 'high_match'\n",
    "                        elif score >= 0.75:\n",
    "                            level = 'medium_match'\n",
    "                        elif score >= 0.6:\n",
    "                            level = 'low_match'\n",
    "                        else:\n",
    "                            level = 'no_match'\n",
    "                        \n",
    "                        # Update results\n",
    "                        result_df.at[idx, 'match_score'] = score\n",
    "                        result_df.at[idx, 'match_level'] = level\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        # Skip on error\n",
    "                        continue\n",
    "                \n",
    "                return result_df\n",
    "            \n",
    "            # Apply function to each partition\n",
    "            start_time = time.time()\n",
    "            result_dask_df = dask_df.map_partitions(match_merchants_func)\n",
    "            \n",
    "            # Compute results\n",
    "            result_df = result_dask_df.compute()\n",
    "            \n",
    "            # Save results if output path provided\n",
    "            if output_path:\n",
    "                # Determine output format from extension\n",
    "                file_ext = os.path.splitext(output_path)[1].lower()\n",
    "                \n",
    "                if file_ext == '.csv':\n",
    "                    result_df.to_csv(output_path, index=False)\n",
    "                elif file_ext in ['.xlsx', '.xls']:\n",
    "                    result_df.to_excel(output_path, index=False)\n",
    "                elif file_ext == '.json':\n",
    "                    result_df.to_json(output_path, orient='records', lines=True)\n",
    "                elif file_ext == '.parquet':\n",
    "                    result_df.to_parquet(output_path, index=False)\n",
    "                else:\n",
    "                    # Default to CSV\n",
    "                    csv_path = os.path.splitext(output_path)[0] + '.csv'\n",
    "                    result_df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics['records_processed'] = len(result_df)\n",
    "            self.metrics['processing_time'] = time.time() - start_time\n",
    "            if self.metrics['processing_time'] > 0:\n",
    "                self.metrics['throughput'] = self.metrics['records_processed'] / self.metrics['processing_time']\n",
    "            \n",
    "            self.metrics['partitions'] = dask_df.npartitions\n",
    "            \n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in Dask processing: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.metrics['end_time'] = time.time()\n",
    "    \n",
    "    def get_performance_report(self):\n",
    "        \"\"\"\n",
    "        Get detailed performance report\n",
    "        \n",
    "        Returns:\n",
    "            dict: Performance metrics and analysis\n",
    "        \"\"\"\n",
    "        report = self.metrics.copy()\n",
    "        \n",
    "        # Add processing mode\n",
    "        if hasattr(self, 'spark_matcher') and self.spark_matcher is not None:\n",
    "            report['processing_mode'] = 'spark'\n",
    "        elif hasattr(self, 'dask_client') and self.dask_client is not None:\n",
    "            report['processing_mode'] = 'dask'\n",
    "        else:\n",
    "            report['processing_mode'] = 'batch'\n",
    "        \n",
    "        # Format times\n",
    "        if report['start_time'] and report['end_time']:\n",
    "            report['total_duration_seconds'] = report['end_time'] - report['start_time']\n",
    "            report['formatted_duration'] = self._format_duration(report['total_duration_seconds'])\n",
    "            \n",
    "            # Add timestamp\n",
    "            from datetime import datetime\n",
    "            report['timestamp'] = datetime.fromtimestamp(report['end_time']).strftime(\n",
    "                '%Y-%m-%d %H:%M:%S'\n",
    "            )\n",
    "        \n",
    "        # Add formatted throughput\n",
    "        if report.get('throughput', 0) > 0:\n",
    "            report['formatted_throughput'] = f\"{report['throughput']:.2f} records/second\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _format_duration(self, seconds):\n",
    "        \"\"\"Format duration in human-readable form\"\"\"\n",
    "        if seconds < 60:\n",
    "            return f\"{seconds:.2f} seconds\"\n",
    "        elif seconds < 3600:\n",
    "            minutes = seconds / 60\n",
    "            return f\"{minutes:.2f} minutes\"\n",
    "        else:\n",
    "            hours = seconds / 3600\n",
    "            return f\"{hours:.2f} hours\"\n",
    "    \n",
    "    def visualize_performance(self, output_path=None):\n",
    "        \"\"\"\n",
    "        Create performance visualization\n",
    "        \n",
    "        Args:\n",
    "            output_path (str, optional): Path to save visualization\n",
    "            \n",
    "        Returns:\n",
    "            str or None: Path to visualization or None if not generated\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            \n",
    "            # Create figure with performance metrics\n",
    "            fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "            \n",
    "            # Throughput and record count\n",
    "            axs[0].bar(['Records Processed'], [self.metrics['records_processed']], color='blue')\n",
    "            axs[0].set_ylabel('Count')\n",
    "            axs[0].set_title('Records Processed')\n",
    "            \n",
    "            # Add throughput as text\n",
    "            if self.metrics.get('throughput', 0) > 0:\n",
    "                axs[0].text(\n",
    "                    0, self.metrics['records_processed'] * 0.5,\n",
    "                    f\"Throughput: {self.metrics['throughput']:.2f} records/second\",\n",
    "                    fontsize=12\n",
    "                )\n",
    "            \n",
    "            # Processing time\n",
    "            axs[1].bar(['Processing Time'], [self.metrics['processing_time']], color='green')\n",
    "            axs[1].set_ylabel('Seconds')\n",
    "            axs[1].set_title('Processing Time')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save if output path provided\n",
    "            if output_path:\n",
    "                plt.savefig(output_path)\n",
    "                plt.close()\n",
    "                return output_path\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        except ImportError:\n",
    "            self.logger.warning(\"Matplotlib not available for visualization\")\n",
    "            return None\n",
    "    \n",
    "    def shutdown(self):\n",
    "        \"\"\"Shutdown all distributed resources\"\"\"\n",
    "        # Stop Spark session\n",
    "        if hasattr(self, 'spark_matcher') and self.spark_matcher is not None:\n",
    "            try:\n",
    "                self.spark_matcher.stop()\n",
    "                self.logger.info(\"Spark session stopped\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Close Dask client\n",
    "        if hasattr(self, 'dask_client') and self.dask_client is not None:\n",
    "            try:\n",
    "                self.dask_client.close()\n",
    "                self.logger.info(\"Dask client closed\")\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01587bb-961e-4a20-8297-2fcc35371ffb",
   "metadata": {},
   "source": [
    "Cell 11: Evaluation and Testing\n",
    "I'll now implement a comprehensive evaluation framework for the merchant matching system, focusing on rigorous comparison methods and performance analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3afe4d18-12fb-4118-a576-34c5cb1ad4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Evaluation and Testing\n",
    "\n",
    "# 11.1: Evaluation Framework and Metrics Calculation\n",
    "\n",
    "class MerchantMatchingEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation framework for merchant name matching algorithms\n",
    "    with rigorous statistical analysis and visualization capabilities.\n",
    "    \n",
    "    This class enables thorough assessment of matching algorithm performance,\n",
    "    comparison between different approaches, and statistical validation of improvements.\n",
    "    \n",
    "    Key features:\n",
    "    - Standard metrics calculation (precision, recall, F1, accuracy)\n",
    "    - Advanced metrics (AUC-ROC, AUC-PR, MCC, confusion matrices)\n",
    "    - Multi-algorithm comparison framework\n",
    "    - Statistical significance testing\n",
    "    - Cross-validation for robustness\n",
    "    - Performance visualization across different dimensions\n",
    "    - Error analysis and classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ground_truth_data=None, test_size=0.2, random_state=42, \n",
    "                matcher=None, baseline_matchers=None):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with ground truth data and matchers\n",
    "        \n",
    "        Args:\n",
    "            ground_truth_data (DataFrame): DataFrame with labeled merchant pairs\n",
    "            test_size (float): Proportion of data for testing (if splitting)\n",
    "            random_state (int): Random seed for reproducibility\n",
    "            matcher: Primary matcher to evaluate\n",
    "            baseline_matchers (dict): Dictionary of {name: matcher} for comparison\n",
    "        \"\"\"\n",
    "        self.ground_truth_data = ground_truth_data\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.matcher = matcher\n",
    "        self.baseline_matchers = baseline_matchers or {}\n",
    "        \n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger('MerchantEvaluator')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Tracking for results\n",
    "        self.results = {}\n",
    "        self.statistical_tests = {}\n",
    "        self.error_analysis = {}\n",
    "        self.cross_validation_results = {}\n",
    "        \n",
    "        # Default column names\n",
    "        self.default_cols = {\n",
    "            's1_col': 's1',\n",
    "            's2_col': 's2',\n",
    "            'label_col': 'is_match',\n",
    "            'domain_col': 'domain'\n",
    "        }\n",
    "        \n",
    "        # Initialize standard baseline matchers if none provided\n",
    "        if not self.baseline_matchers:\n",
    "            self._initialize_baseline_matchers()\n",
    "    \n",
    "    def _initialize_baseline_matchers(self):\n",
    "        \"\"\"Initialize standard baseline matchers for comparison\"\"\"\n",
    "        try:\n",
    "            # Create dictionary of baseline matchers\n",
    "            # These are simple functions that return a similarity score\n",
    "            \n",
    "            # Import required libraries\n",
    "            from Levenshtein import distance as levenshtein_distance\n",
    "            from Levenshtein import jaro_winkler, ratio as levenshtein_ratio\n",
    "            import textdistance\n",
    "            from fuzzywuzzy import fuzz\n",
    "            import jellyfish\n",
    "            import re\n",
    "            \n",
    "            def preprocess(text):\n",
    "                \"\"\"Simple preprocessing for baseline matchers\"\"\"\n",
    "                if not isinstance(text, str):\n",
    "                    return \"\"\n",
    "                # Lowercase and trim\n",
    "                text = text.lower().strip()\n",
    "                # Remove most punctuation and normalize spaces\n",
    "                text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "                # Normalize spaces\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                return text\n",
    "                \n",
    "            # Define matcher functions\n",
    "            def jaro_winkler_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                return jaro_winkler(s1_clean, s2_clean)\n",
    "            \n",
    "            def levenshtein_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                max_len = max(len(s1_clean), len(s2_clean))\n",
    "                if max_len == 0:\n",
    "                    return 0.0\n",
    "                distance = levenshtein_distance(s1_clean, s2_clean)\n",
    "                return 1.0 - (distance / max_len)\n",
    "            \n",
    "            def token_sort_ratio_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                return fuzz.token_sort_ratio(s1_clean, s2_clean) / 100.0\n",
    "            \n",
    "            def token_set_ratio_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                return fuzz.token_set_ratio(s1_clean, s2_clean) / 100.0\n",
    "            \n",
    "            def jaccard_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                return textdistance.jaccard.normalized_similarity(s1_clean, s2_clean)\n",
    "            \n",
    "            def cosine_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                return textdistance.cosine.normalized_similarity(s1_clean, s2_clean)\n",
    "            \n",
    "            def sorensen_dice_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                return textdistance.sorensen_dice.normalized_similarity(s1_clean, s2_clean)\n",
    "            \n",
    "            def overlap_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                return textdistance.overlap.normalized_similarity(s1_clean, s2_clean)\n",
    "            \n",
    "            def metaphone_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                # Calculate metaphone similarity at word level\n",
    "                s1_words = s1_clean.split()\n",
    "                s2_words = s2_clean.split()\n",
    "                if not s1_words or not s2_words:\n",
    "                    return 0.0\n",
    "                    \n",
    "                # Get metaphone codes for each word\n",
    "                s1_codes = [jellyfish.metaphone(word) for word in s1_words]\n",
    "                s2_codes = [jellyfish.metaphone(word) for word in s2_words]\n",
    "                \n",
    "                # Count matching codes\n",
    "                matches = 0\n",
    "                for code in s1_codes:\n",
    "                    if code in s2_codes:\n",
    "                        matches += 1\n",
    "                        s2_codes.remove(code)\n",
    "                \n",
    "                total = max(len(s1_words), len(s2_words))\n",
    "                return matches / total if total > 0 else 0.0\n",
    "            \n",
    "            def soundex_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                # Calculate soundex similarity at word level\n",
    "                s1_words = s1_clean.split()\n",
    "                s2_words = s2_clean.split()\n",
    "                if not s1_words or not s2_words:\n",
    "                    return 0.0\n",
    "                    \n",
    "                # Get soundex codes for each word\n",
    "                s1_codes = [jellyfish.soundex(word) for word in s1_words]\n",
    "                s2_codes = [jellyfish.soundex(word) for word in s2_words]\n",
    "                \n",
    "                # Count matching codes\n",
    "                matches = 0\n",
    "                for code in s1_codes:\n",
    "                    if code in s2_codes:\n",
    "                        matches += 1\n",
    "                        s2_codes.remove(code)\n",
    "                \n",
    "                total = max(len(s1_words), len(s2_words))\n",
    "                return matches / total if total > 0 else 0.0\n",
    "            \n",
    "            def contains_matcher(s1, s2, domain=None):\n",
    "                s1_clean = preprocess(s1)\n",
    "                s2_clean = preprocess(s2)\n",
    "                if not s1_clean or not s2_clean:\n",
    "                    return 0.0\n",
    "                if s1_clean in s2_clean or s2_clean in s1_clean:\n",
    "                    return 1.0\n",
    "                else:\n",
    "                    # Check word-level containment\n",
    "                    s1_words = set(s1_clean.split())\n",
    "                    s2_words = set(s2_clean.split())\n",
    "                    if s1_words.issubset(s2_words) or s2_words.issubset(s1_words):\n",
    "                        return 0.9\n",
    "                    # Check overlap\n",
    "                    intersection = s1_words.intersection(s2_words)\n",
    "                    shorter_len = min(len(s1_words), len(s2_words))\n",
    "                    if shorter_len == 0:\n",
    "                        return 0.0\n",
    "                    return len(intersection) / shorter_len\n",
    "            \n",
    "            # Add all matchers to baseline dictionary\n",
    "            self.baseline_matchers = {\n",
    "                'Jaro-Winkler': jaro_winkler_matcher,\n",
    "                'Levenshtein': levenshtein_matcher,\n",
    "                'Token Sort Ratio': token_sort_ratio_matcher,\n",
    "                'Token Set Ratio': token_set_ratio_matcher,\n",
    "                'Jaccard': jaccard_matcher,\n",
    "                'Cosine': cosine_matcher,\n",
    "                'Sorensen-Dice': sorensen_dice_matcher,\n",
    "                'Overlap': overlap_matcher,\n",
    "                'Metaphone': metaphone_matcher,\n",
    "                'Soundex': soundex_matcher,\n",
    "                'Contains': contains_matcher\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"Initialized {len(self.baseline_matchers)} baseline matchers\")\n",
    "            \n",
    "        except ImportError as e:\n",
    "            self.logger.warning(f\"Could not initialize all baseline matchers: {e}\")\n",
    "            self.baseline_matchers = {}\n",
    "    \n",
    "    def evaluate_matcher(self, matcher, test_data=None, s1_col=None, s2_col=None, \n",
    "                        label_col=None, domain_col=None, threshold=0.75, name=\"Primary\"):\n",
    "        \"\"\"\n",
    "        Evaluate a single matcher on test data\n",
    "        \n",
    "        Args:\n",
    "            matcher: Matcher to evaluate (function or object with match_merchants method)\n",
    "            test_data (DataFrame): Test data with merchant pairs and labels\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            label_col (str): Column name for match label (1=match, 0=no match)\n",
    "            domain_col (str): Column name for domain information\n",
    "            threshold (float): Score threshold for binary classification\n",
    "            name (str): Name for the matcher in results\n",
    "            \n",
    "        Returns:\n",
    "            dict: Evaluation metrics\n",
    "        \"\"\"\n",
    "        # Use provided test data or split ground truth data\n",
    "        if test_data is None:\n",
    "            if self.ground_truth_data is None:\n",
    "                self.logger.error(\"No test data or ground truth data provided\")\n",
    "                return None\n",
    "            test_data = self._split_data(self.ground_truth_data)[1]\n",
    "        \n",
    "        # Use provided column names or defaults\n",
    "        s1_col = s1_col or self.default_cols['s1_col']\n",
    "        s2_col = s2_col or self.default_cols['s2_col']\n",
    "        label_col = label_col or self.default_cols['label_col']\n",
    "        domain_col = domain_col or self.default_cols['domain_col']\n",
    "        \n",
    "        # Check required columns\n",
    "        if s1_col not in test_data.columns or s2_col not in test_data.columns:\n",
    "            self.logger.error(f\"Required columns missing: {s1_col}, {s2_col}\")\n",
    "            return None\n",
    "        \n",
    "        if label_col not in test_data.columns:\n",
    "            self.logger.error(f\"Label column missing: {label_col}\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate match scores\n",
    "        self.logger.info(f\"Evaluating matcher: {name}\")\n",
    "        \n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        domains = []\n",
    "        errors = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, row in test_data.iterrows():\n",
    "            try:\n",
    "                s1 = row[s1_col]\n",
    "                s2 = row[s2_col]\n",
    "                \n",
    "                # Skip if missing data\n",
    "                if pd.isna(s1) or pd.isna(s2) or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                    continue\n",
    "                \n",
    "                # Get true label\n",
    "                true_label = int(row[label_col])\n",
    "                y_true.append(true_label)\n",
    "                \n",
    "                # Get domain if available\n",
    "                domain = row[domain_col] if domain_col and domain_col in row else None\n",
    "                if domain:\n",
    "                    domains.append(domain)\n",
    "                \n",
    "                # Calculate similarity score based on matcher type\n",
    "                if hasattr(matcher, 'match_merchants'):\n",
    "                    # Matcher is an object with match_merchants method\n",
    "                    score = matcher.match_merchants(s1, s2, domain)\n",
    "                else:\n",
    "                    # Matcher is a function\n",
    "                    score = matcher(s1, s2, domain)\n",
    "                \n",
    "                y_scores.append(score)\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors.append((i, str(e)))\n",
    "                # Skip this pair\n",
    "                continue\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if len(y_true) < 10:\n",
    "            self.logger.error(f\"Insufficient data for evaluation: {len(y_true)} valid pairs\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        y_true = np.array(y_true)\n",
    "        y_scores = np.array(y_scores)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self._calculate_metrics(y_true, y_scores, threshold)\n",
    "        \n",
    "        # Add additional information\n",
    "        metrics['processing_time'] = processing_time\n",
    "        metrics['avg_processing_time'] = processing_time / len(y_true) if len(y_true) > 0 else 0\n",
    "        metrics['error_count'] = len(errors)\n",
    "        metrics['error_rate'] = len(errors) / (len(y_true) + len(errors)) if (len(y_true) + len(errors)) > 0 else 0\n",
    "        \n",
    "        # Add domain-specific metrics if domains available\n",
    "        if domains:\n",
    "            metrics['domain_metrics'] = self._calculate_domain_metrics(\n",
    "                y_true, y_scores, domains, threshold\n",
    "            )\n",
    "        \n",
    "        # Store results\n",
    "        self.results[name] = metrics\n",
    "        \n",
    "        # Log summary\n",
    "        self.logger.info(\n",
    "            f\"Evaluation complete: {name}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "            f\"F1: {metrics['f1_score']:.4f}, AUC: {metrics['auc_roc']:.4f}\"\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_scores, threshold):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive evaluation metrics\n",
    "        \n",
    "        Args:\n",
    "            y_true (array): True binary labels\n",
    "            y_scores (array): Predicted scores\n",
    "            threshold (float): Score threshold for binary classification\n",
    "            \n",
    "        Returns:\n",
    "            dict: Calculated metrics\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score, precision_score, recall_score, f1_score,\n",
    "            roc_auc_score, precision_recall_curve, auc,\n",
    "            confusion_matrix, matthews_corrcoef, balanced_accuracy_score\n",
    "        )\n",
    "        \n",
    "        # Convert scores to binary predictions using threshold\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        \n",
    "        # Basic metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'matthews_corrcoef': matthews_corrcoef(y_true, y_pred),\n",
    "        }\n",
    "        \n",
    "        # Confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        metrics['true_positives'] = int(tp)\n",
    "        metrics['false_positives'] = int(fp)\n",
    "        metrics['true_negatives'] = int(tn)\n",
    "        metrics['false_negatives'] = int(fn)\n",
    "        \n",
    "        # Calculate AUC metrics if possible\n",
    "        try:\n",
    "            metrics['auc_roc'] = roc_auc_score(y_true, y_scores)\n",
    "            \n",
    "            # Precision-recall curve and AUC\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "            metrics['auc_pr'] = auc(recall, precision)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not calculate AUC metrics: {e}\")\n",
    "            metrics['auc_roc'] = 0.0\n",
    "            metrics['auc_pr'] = 0.0\n",
    "        \n",
    "        # Calculate optimal threshold using F1 score\n",
    "        try:\n",
    "            thresholds = np.linspace(0, 1, 100)\n",
    "            f1_scores = []\n",
    "            \n",
    "            for t in thresholds:\n",
    "                y_pred_t = (y_scores >= t).astype(int)\n",
    "                f1 = f1_score(y_true, y_pred_t, zero_division=0)\n",
    "                f1_scores.append(f1)\n",
    "            \n",
    "            optimal_idx = np.argmax(f1_scores)\n",
    "            metrics['optimal_threshold'] = thresholds[optimal_idx]\n",
    "            metrics['optimal_f1_score'] = f1_scores[optimal_idx]\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not calculate optimal threshold: {e}\")\n",
    "            metrics['optimal_threshold'] = threshold\n",
    "            metrics['optimal_f1_score'] = metrics['f1_score']\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_domain_metrics(self, y_true, y_scores, domains, threshold):\n",
    "        \"\"\"\n",
    "        Calculate metrics for each domain separately\n",
    "        \n",
    "        Args:\n",
    "            y_true (array): True binary labels\n",
    "            y_scores (array): Predicted scores\n",
    "            domains (list): Domain for each pair\n",
    "            threshold (float): Score threshold for binary classification\n",
    "            \n",
    "        Returns:\n",
    "            dict: Domain-specific metrics\n",
    "        \"\"\"\n",
    "        # Get unique domains\n",
    "        unique_domains = list(set(domains))\n",
    "        \n",
    "        # Calculate metrics for each domain\n",
    "        domain_metrics = {}\n",
    "        \n",
    "        for domain in unique_domains:\n",
    "            # Get indices for this domain\n",
    "            indices = [i for i, d in enumerate(domains) if d == domain]\n",
    "            \n",
    "            # Skip if too few samples\n",
    "            if len(indices) < 10:\n",
    "                continue\n",
    "                \n",
    "            # Calculate metrics for this domain\n",
    "            domain_y_true = np.array([y_true[i] for i in indices])\n",
    "            domain_y_scores = np.array([y_scores[i] for i in indices])\n",
    "            \n",
    "            domain_metrics[domain] = self._calculate_metrics(\n",
    "                domain_y_true, domain_y_scores, threshold\n",
    "            )\n",
    "        \n",
    "        return domain_metrics\n",
    "    \n",
    "    def _split_data(self, data, stratify_col=None):\n",
    "        \"\"\"\n",
    "        Split data into training and testing sets\n",
    "        \n",
    "        Args:\n",
    "            data (DataFrame): Data to split\n",
    "            stratify_col (str, optional): Column to use for stratified sampling\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (train_data, test_data)\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        if stratify_col and stratify_col in data.columns:\n",
    "            stratify = data[stratify_col]\n",
    "        else:\n",
    "            stratify = None\n",
    "        \n",
    "        train_data, test_data = train_test_split(\n",
    "            data,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state,\n",
    "            stratify=stratify\n",
    "        )\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    def compare_matchers(self, test_data=None, s1_col=None, s2_col=None, \n",
    "                         label_col=None, domain_col=None, threshold=0.75):\n",
    "        \"\"\"\n",
    "        Compare multiple matchers on the same test data\n",
    "        \n",
    "        Args:\n",
    "            test_data (DataFrame): Test data with merchant pairs and labels\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            label_col (str): Column name for match label (1=match, 0=no match)\n",
    "            domain_col (str): Column name for domain information\n",
    "            threshold (float): Score threshold for binary classification\n",
    "            \n",
    "        Returns:\n",
    "            dict: Comparison results for all matchers\n",
    "        \"\"\"\n",
    "        # Use provided test data or split ground truth data\n",
    "        if test_data is None:\n",
    "            if self.ground_truth_data is None:\n",
    "                self.logger.error(\"No test data or ground truth data provided\")\n",
    "                return None\n",
    "            test_data = self._split_data(self.ground_truth_data)[1]\n",
    "        \n",
    "        # Use provided column names or defaults\n",
    "        s1_col = s1_col or self.default_cols['s1_col']\n",
    "        s2_col = s2_col or self.default_cols['s2_col']\n",
    "        label_col = label_col or self.default_cols['label_col']\n",
    "        domain_col = domain_col or self.default_cols['domain_col']\n",
    "        \n",
    "        # Reset results\n",
    "        self.results = {}\n",
    "        \n",
    "        # Evaluate primary matcher if available\n",
    "        if self.matcher:\n",
    "            self.evaluate_matcher(\n",
    "                self.matcher,\n",
    "                test_data,\n",
    "                s1_col,\n",
    "                s2_col,\n",
    "                label_col,\n",
    "                domain_col,\n",
    "                threshold,\n",
    "                \"Primary\"\n",
    "            )\n",
    "        \n",
    "        # Evaluate all baseline matchers\n",
    "        for name, matcher in self.baseline_matchers.items():\n",
    "            self.evaluate_matcher(\n",
    "                matcher,\n",
    "                test_data,\n",
    "                s1_col,\n",
    "                s2_col,\n",
    "                label_col,\n",
    "                domain_col,\n",
    "                threshold,\n",
    "                name\n",
    "            )\n",
    "        \n",
    "        # Perform statistical significance testing\n",
    "        if len(self.results) > 1 and \"Primary\" in self.results:\n",
    "            self._perform_statistical_tests(test_data, s1_col, s2_col, label_col, domain_col)\n",
    "        \n",
    "        # Return results\n",
    "        return self.results\n",
    "    \n",
    "    def _perform_statistical_tests(self, test_data, s1_col, s2_col, label_col, domain_col):\n",
    "        \"\"\"\n",
    "        Perform statistical significance tests on matcher results\n",
    "        \n",
    "        Args:\n",
    "            test_data (DataFrame): Test data\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            label_col (str): Column name for true label\n",
    "            domain_col (str): Column name for domain\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy import stats\n",
    "            import numpy as np\n",
    "            \n",
    "            # Reset statistical tests\n",
    "            self.statistical_tests = {}\n",
    "            \n",
    "            # We need to compare predictions of each matcher on exactly the same examples\n",
    "            # First, collect predictions from all matchers\n",
    "            matcher_predictions = {}\n",
    "            \n",
    "            # Primary matcher (target for comparison)\n",
    "            if \"Primary\" not in self.results:\n",
    "                return\n",
    "                \n",
    "            primary_matcher = self.matcher\n",
    "            primary_predictions = []\n",
    "            ground_truth = []\n",
    "            \n",
    "            # Other matchers\n",
    "            baseline_matchers = {}\n",
    "            baseline_predictions = {}\n",
    "            \n",
    "            # Initialize for baseline matchers\n",
    "            for name in self.results:\n",
    "                if name != \"Primary\":\n",
    "                    baseline_matchers[name] = self.baseline_matchers.get(name)\n",
    "                    baseline_predictions[name] = []\n",
    "            \n",
    "            # Collect predictions for each example\n",
    "            for i, row in test_data.iterrows():\n",
    "                try:\n",
    "                    s1 = row[s1_col]\n",
    "                    s2 = row[s2_col]\n",
    "                    \n",
    "                    # Skip if missing data\n",
    "                    if pd.isna(s1) or pd.isna(s2) or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                        continue\n",
    "                    \n",
    "                    # Get true label\n",
    "                    true_label = int(row[label_col])\n",
    "                    \n",
    "                    # Get domain if available\n",
    "                    domain = row[domain_col] if domain_col and domain_col in row else None\n",
    "                    \n",
    "                    # Get primary matcher prediction\n",
    "                    if hasattr(primary_matcher, 'match_merchants'):\n",
    "                        # Matcher is an object with match_merchants method\n",
    "                        primary_score = primary_matcher.match_merchants(s1, s2, domain)\n",
    "                    else:\n",
    "                        # Matcher is a function\n",
    "                        primary_score = primary_matcher(s1, s2, domain)\n",
    "                    \n",
    "                    # Skip if primary matcher fails\n",
    "                    if primary_score is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get baseline matcher predictions\n",
    "                    baseline_scores = {}\n",
    "                    all_valid = True\n",
    "                    \n",
    "                    for name, matcher in baseline_matchers.items():\n",
    "                        try:\n",
    "                            if matcher is None:\n",
    "                                continue\n",
    "                                \n",
    "                            if hasattr(matcher, 'match_merchants'):\n",
    "                                score = matcher.match_merchants(s1, s2, domain)\n",
    "                            else:\n",
    "                                score = matcher(s1, s2, domain)\n",
    "                                \n",
    "                            if score is None:\n",
    "                                all_valid = False\n",
    "                                break\n",
    "                                \n",
    "                            baseline_scores[name] = score\n",
    "                        except Exception:\n",
    "                            all_valid = False\n",
    "                            break\n",
    "                    \n",
    "                    # Only include examples where all matchers produced valid scores\n",
    "                    if all_valid and baseline_scores:\n",
    "                        ground_truth.append(true_label)\n",
    "                        primary_predictions.append(primary_score)\n",
    "                        \n",
    "                        for name, score in baseline_scores.items():\n",
    "                            baseline_predictions[name].append(score)\n",
    "                    \n",
    "                except Exception:\n",
    "                    # Skip examples with errors\n",
    "                    continue\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            ground_truth = np.array(ground_truth)\n",
    "            primary_predictions = np.array(primary_predictions)\n",
    "            \n",
    "            for name in baseline_predictions:\n",
    "                baseline_predictions[name] = np.array(baseline_predictions[name])\n",
    "            \n",
    "            # Perform McNemar's test for binary classification\n",
    "            # First convert scores to binary predictions using optimal thresholds\n",
    "            primary_threshold = self.results[\"Primary\"].get(\"optimal_threshold\", 0.75)\n",
    "            primary_binary = (primary_predictions >= primary_threshold).astype(int)\n",
    "            \n",
    "            # For each baseline matcher\n",
    "            for name, predictions in baseline_predictions.items():\n",
    "                if len(predictions) != len(primary_binary):\n",
    "                    continue\n",
    "                    \n",
    "                baseline_threshold = self.results[name].get(\"optimal_threshold\", 0.75)\n",
    "                baseline_binary = (predictions >= baseline_threshold).astype(int)\n",
    "                \n",
    "                # Create contingency table for McNemar's test\n",
    "                # [both wrong, baseline right & primary wrong,\n",
    "                #  baseline wrong & primary right, both right]\n",
    "                contingency_table = [\n",
    "                    sum((baseline_binary == 0) & (primary_binary == 0) & (ground_truth == 1) | \n",
    "                        (baseline_binary == 1) & (primary_binary == 1) & (ground_truth == 0)),\n",
    "                    sum((baseline_binary == 1) & (primary_binary == 0) & (ground_truth == 1) | \n",
    "                        (baseline_binary == 0) & (primary_binary == 1) & (ground_truth == 0)),\n",
    "                    sum((baseline_binary == 0) & (primary_binary == 1) & (ground_truth == 1) | \n",
    "                        (baseline_binary == 1) & (primary_binary == 0) & (ground_truth == 0)),\n",
    "                    sum((baseline_binary == 1) & (primary_binary == 1) & (ground_truth == 1) | \n",
    "                        (baseline_binary == 0) & (primary_binary == 0) & (ground_truth == 0))\n",
    "                ]\n",
    "                \n",
    "                # Reshape for statsmodels\n",
    "                table = np.array([[contingency_table[0], contingency_table[1]],\n",
    "                                  [contingency_table[2], contingency_table[3]]])\n",
    "                \n",
    "                # Perform McNemar's test\n",
    "                try:\n",
    "                    mcnemar_result = stats.mcnemar(table, exact=True)\n",
    "                    \n",
    "                    # Store results\n",
    "                    self.statistical_tests[name] = {\n",
    "                        'test_name': \"McNemar's test\",\n",
    "                        'statistic': float(mcnemar_result.statistic),\n",
    "                        'p_value': float(mcnemar_result.pvalue),\n",
    "                        'significant': mcnemar_result.pvalue < 0.05,\n",
    "                        'contingency_table': contingency_table\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"McNemar's test failed for {name}: {e}\")\n",
    "                    \n",
    "                # Also perform signed-rank test on score differences\n",
    "                try:\n",
    "                    # Calculate absolute errors\n",
    "                    primary_errors = np.abs(primary_predictions - ground_truth)\n",
    "                    baseline_errors = np.abs(predictions - ground_truth)\n",
    "                    \n",
    "                    # Perform Wilcoxon signed-rank test\n",
    "                    wilcoxon_result = stats.wilcoxon(baseline_errors, primary_errors)\n",
    "                    \n",
    "                    # Store results\n",
    "                    self.statistical_tests[f\"{name}_wilcoxon\"] = {\n",
    "                        'test_name': \"Wilcoxon signed-rank test\",\n",
    "                        'statistic': float(wilcoxon_result.statistic),\n",
    "                        'p_value': float(wilcoxon_result.pvalue),\n",
    "                        'significant': wilcoxon_result.pvalue < 0.05\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Wilcoxon test failed for {name}: {e}\")\n",
    "            \n",
    "            self.logger.info(f\"Completed statistical significance testing against {len(baseline_predictions)} baselines\")\n",
    "            \n",
    "        except ImportError:\n",
    "            self.logger.warning(\"SciPy not available for statistical testing\")\n",
    "        \n",
    "    def perform_cross_validation(self, data=None, matcher=None, n_splits=5,\n",
    "                                s1_col=None, s2_col=None, label_col=None, domain_col=None,\n",
    "                                threshold=0.75, name=\"Primary\", stratify=True):\n",
    "        \"\"\"\n",
    "        Perform cross-validation for robust performance estimation\n",
    "        \n",
    "        Args:\n",
    "            data (DataFrame): Data with merchant pairs and labels\n",
    "            matcher: Matcher to evaluate\n",
    "            n_splits (int): Number of cross-validation splits\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            label_col (str): Column name for match label\n",
    "            domain_col (str): Column name for domain\n",
    "            threshold (float): Score threshold for binary classification\n",
    "            name (str): Name for the matcher in results\n",
    "            stratify (bool): Whether to use stratified sampling\n",
    "            \n",
    "        Returns:\n",
    "            dict: Cross-validation results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from sklearn.model_selection import StratifiedKFold, KFold\n",
    "            \n",
    "            # Use provided data or ground truth data\n",
    "            if data is None:\n",
    "                if self.ground_truth_data is None:\n",
    "                    self.logger.error(\"No data or ground truth data provided\")\n",
    "                    return None\n",
    "                data = self.ground_truth_data\n",
    "            \n",
    "            # Use provided matcher or primary matcher\n",
    "            if matcher is None:\n",
    "                matcher = self.matcher\n",
    "            \n",
    "            if matcher is None:\n",
    "                self.logger.error(\"No matcher provided\")\n",
    "                return None\n",
    "            \n",
    "            # Use provided column names or defaults\n",
    "            s1_col = s1_col or self.default_cols['s1_col']\n",
    "            s2_col = s2_col or self.default_cols['s2_col']\n",
    "            label_col = label_col or self.default_cols['label_col']\n",
    "            domain_col = domain_col or self.default_cols['domain_col']\n",
    "            \n",
    "            # Check required columns\n",
    "            if s1_col not in data.columns or s2_col not in data.columns:\n",
    "                self.logger.error(f\"Required columns missing: {s1_col}, {s2_col}\")\n",
    "                return None\n",
    "            \n",
    "            if label_col not in data.columns:\n",
    "                self.logger.error(f\"Label column missing: {label_col}\")\n",
    "                return None\n",
    "            \n",
    "            # Create cross-validation splits\n",
    "            if stratify and label_col in data.columns:\n",
    "                # Use stratified sampling to maintain class balance\n",
    "                cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=self.random_state)\n",
    "                splits = list(cv.split(data, data[label_col]))\n",
    "            else:\n",
    "                # Use regular k-fold cross-validation\n",
    "                cv = KFold(n_splits=n_splits, shuffle=True, random_state=self.random_state)\n",
    "                splits = list(cv.split(data))\n",
    "            \n",
    "            # Initialize results\n",
    "            cv_results = {\n",
    "                'fold_metrics': [],\n",
    "                'aggregate_metrics': {},\n",
    "                'roc_curves': [],\n",
    "                'pr_curves': []\n",
    "            }\n",
    "            \n",
    "            # For each fold\n",
    "            for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "                # Split data\n",
    "                train_data = data.iloc[train_idx]\n",
    "                test_data = data.iloc[test_idx]\n",
    "                \n",
    "                # Evaluate on this fold\n",
    "                fold_metrics = self.evaluate_matcher(\n",
    "                    matcher, \n",
    "                    test_data, \n",
    "                    s1_col, \n",
    "                    s2_col, \n",
    "                    label_col, \n",
    "                    domain_col, \n",
    "                    threshold, \n",
    "                    f\"{name}_fold{fold+1}\"\n",
    "                )\n",
    "                \n",
    "                # Add fold number\n",
    "                fold_metrics['fold'] = fold + 1\n",
    "                \n",
    "                # Add to results\n",
    "                cv_results['fold_metrics'].append(fold_metrics)\n",
    "                \n",
    "                # Store ROC and PR curves\n",
    "                if test_data is not None:\n",
    "                    # Calculate predictions\n",
    "                    y_true = []\n",
    "                    y_scores = []\n",
    "                    \n",
    "                    for i, row in test_data.iterrows():\n",
    "                        try:\n",
    "                            s1 = row[s1_col]\n",
    "                            s2 = row[s2_col]\n",
    "                            \n",
    "                            # Skip if missing data\n",
    "                            if pd.isna(s1) or pd.isna(s2) or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                                continue\n",
    "                            \n",
    "                            # Get true label\n",
    "                            true_label = int(row[label_col])\n",
    "                            y_true.append(true_label)\n",
    "                            \n",
    "                            # Get domain if available\n",
    "                            domain = row[domain_col] if domain_col and domain_col in row else None\n",
    "                            \n",
    "                            # Calculate similarity score\n",
    "                            if hasattr(matcher, 'match_merchants'):\n",
    "                                score = matcher.match_merchants(s1, s2, domain)\n",
    "                            else:\n",
    "                                score = matcher(s1, s2, domain)\n",
    "                            \n",
    "                            y_scores.append(score)\n",
    "                            \n",
    "                        except Exception:\n",
    "                            # Skip examples with errors\n",
    "                            continue\n",
    "                    \n",
    "                    # Convert to numpy arrays\n",
    "                    y_true = np.array(y_true)\n",
    "                    y_scores = np.array(y_scores)\n",
    "                    \n",
    "                    # Calculate ROC curve\n",
    "                    try:\n",
    "                        from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "                        \n",
    "                        fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores)\n",
    "                        cv_results['roc_curves'].append({\n",
    "                            'fold': fold + 1,\n",
    "                            'fpr': fpr.tolist(),\n",
    "                            'tpr': tpr.tolist(),\n",
    "                            'thresholds': roc_thresholds.tolist()\n",
    "                        })\n",
    "                        \n",
    "                        # Calculate PR curve\n",
    "                        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_scores)\n",
    "                        cv_results['pr_curves'].append({\n",
    "                            'fold': fold + 1,\n",
    "                            'precision': precision.tolist(),\n",
    "                            'recall': recall.tolist(),\n",
    "                            'thresholds': pr_thresholds.tolist() if len(pr_thresholds) > 0 else []\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"Error calculating curves for fold {fold+1}: {e}\")\n",
    "            \n",
    "            # Calculate aggregate metrics across folds\n",
    "            metrics_keys = [\n",
    "                'accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1_score',\n",
    "                'matthews_corrcoef', 'auc_roc', 'auc_pr'\n",
    "            ]\n",
    "            \n",
    "            for key in metrics_keys:\n",
    "                values = [m.get(key, 0) for m in cv_results['fold_metrics'] if key in m]\n",
    "                if values:\n",
    "                    cv_results['aggregate_metrics'][key] = {\n",
    "                        'mean': np.mean(values),\n",
    "                        'std': np.std(values),\n",
    "                        'min': np.min(values),\n",
    "                        'max': np.max(values),\n",
    "                        'values': values\n",
    "                    }\n",
    "            \n",
    "            # Store in instance\n",
    "            self.cross_validation_results[name] = cv_results\n",
    "            \n",
    "            # Log summary\n",
    "            self.logger.info(\n",
    "                f\"Cross-validation complete for {name}: \"\n",
    "                f\"Mean F1: {cv_results['aggregate_metrics'].get('f1_score', {}).get('mean', 0):.4f}  \"\n",
    "                f\"{cv_results['aggregate_metrics'].get('f1_score', {}).get('std', 0):.4f}\"\n",
    "            )\n",
    "            \n",
    "            return cv_results\n",
    "            \n",
    "        except ImportError:\n",
    "            self.logger.warning(\"scikit-learn not available for cross-validation\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_errors(self, test_data=None, matcher=None, \n",
    "                     s1_col=None, s2_col=None, label_col=None, domain_col=None,\n",
    "                     threshold=0.75, name=\"Primary\"):\n",
    "        \"\"\"\n",
    "        Perform detailed error analysis to understand matcher weaknesses\n",
    "        \n",
    "        Args:\n",
    "            test_data (DataFrame): Test data\n",
    "            matcher: Matcher to analyze\n",
    "            s1_col (str): Column name for first merchant name\n",
    "            s2_col (str): Column name for second merchant name\n",
    "            label_col (str): Column name for label\n",
    "            domain_col (str): Column name for domain\n",
    "            threshold (float): Score threshold for binary classification\n",
    "            name (str): Name for the matcher in results\n",
    "            \n",
    "        Returns:\n",
    "            dict: Error analysis results\n",
    "        \"\"\"\n",
    "        # Use provided test data or split ground truth data\n",
    "        if test_data is None:\n",
    "            if self.ground_truth_data is None:\n",
    "                self.logger.error(\"No test data or ground truth data provided\")\n",
    "                return None\n",
    "            test_data = self._split_data(self.ground_truth_data)[1]\n",
    "        \n",
    "        # Use provided matcher or primary matcher\n",
    "        if matcher is None:\n",
    "            matcher = self.matcher\n",
    "        \n",
    "        if matcher is None:\n",
    "            self.logger.error(\"No matcher provided\")\n",
    "            return None\n",
    "        \n",
    "        # Use provided column names or defaults\n",
    "        s1_col = s1_col or self.default_cols['s1_col']\n",
    "        s2_col = s2_col or self.default_cols['s2_col']\n",
    "        label_col = label_col or self.default_cols['label_col']\n",
    "        domain_col = domain_col or self.default_cols['domain_col']\n",
    "        \n",
    "        # Initialize error analysis\n",
    "        analysis = {\n",
    "            'false_positives': [],\n",
    "            'false_negatives': [],\n",
    "            'error_patterns': {},\n",
    "            'summary': {},\n",
    "            'error_counts': {\n",
    "                'false_positives': 0,\n",
    "                'false_negatives': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Collect predictions and analyze errors\n",
    "        for i, row in test_data.iterrows():\n",
    "            try:\n",
    "                s1 = row[s1_col]\n",
    "                s2 = row[s2_col]\n",
    "                \n",
    "                # Skip if missing data\n",
    "                if pd.isna(s1) or pd.isna(s2) or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                    continue\n",
    "                \n",
    "                # Get true label\n",
    "                true_label = int(row[label_col])\n",
    "                \n",
    "                # Get domain if available\n",
    "                domain = row[domain_col] if domain_col and domain_col in row else None\n",
    "                \n",
    "                # Calculate similarity score\n",
    "                if hasattr(matcher, 'match_merchants'):\n",
    "                    score = matcher.match_merchants(s1, s2, domain)\n",
    "                else:\n",
    "                    score = matcher(s1, s2, domain)\n",
    "                \n",
    "                # Convert to binary prediction\n",
    "                pred_label = 1 if score >= threshold else 0\n",
    "                \n",
    "                # Check for error\n",
    "                if pred_label != true_label:\n",
    "                    if pred_label == 1 and true_label == 0:\n",
    "                        # False positive\n",
    "                        analysis['false_positives'].append({\n",
    "                            'id': i,\n",
    "                            's1': s1,\n",
    "                            's2': s2,\n",
    "                            'domain': domain,\n",
    "                            'score': score,\n",
    "                            'threshold': threshold\n",
    "                        })\n",
    "                        analysis['error_counts']['false_positives'] += 1\n",
    "                    elif pred_label == 0 and true_label == 1:\n",
    "                        # False negative\n",
    "                        analysis['false_negatives'].append({\n",
    "                            'id': i,\n",
    "                            's1': s1,\n",
    "                            's2': s2,\n",
    "                            'domain': domain,\n",
    "                            'score': score,\n",
    "                            'threshold': threshold\n",
    "                        })\n",
    "                        analysis['error_counts']['false_negatives'] += 1\n",
    "            \n",
    "            except Exception:\n",
    "                # Skip examples with errors\n",
    "                continue\n",
    "        \n",
    "        # Analyze error patterns\n",
    "        analysis['error_patterns'] = self._identify_error_patterns(\n",
    "            analysis['false_positives'], \n",
    "            analysis['false_negatives']\n",
    "        )\n",
    "        \n",
    "        # Create summary\n",
    "        analysis['summary'] = {\n",
    "            'total_errors': analysis['error_counts']['false_positives'] + analysis['error_counts']['false_negatives'],\n",
    "            'false_positive_rate': analysis['error_counts']['false_positives'] / len(test_data) if len(test_data) > 0 else 0,\n",
    "            'false_negative_rate': analysis['error_counts']['false_negatives'] / len(test_data) if len(test_data) > 0 else 0,\n",
    "            'error_rate': (analysis['error_counts']['false_positives'] + analysis['error_counts']['false_negatives']) / len(test_data) if len(test_data) > 0 else 0,\n",
    "            'top_error_patterns': sorted(\n",
    "                analysis['error_patterns'].items(), \n",
    "                key=lambda x: x[1]['count'], \n",
    "                reverse=True\n",
    "            )[:5] if analysis['error_patterns'] else []\n",
    "        }\n",
    "        \n",
    "        # Store in instance\n",
    "        self.error_analysis[name] = analysis\n",
    "        \n",
    "        # Log summary\n",
    "        self.logger.info(\n",
    "            f\"Error analysis complete for {name}: \"\n",
    "            f\"False positives: {analysis['error_counts']['false_positives']}, \"\n",
    "            f\"False negatives: {analysis['error_counts']['false_negatives']}\"\n",
    "        )\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _identify_error_patterns(self, false_positives, false_negatives):\n",
    "        \"\"\"\n",
    "        Identify common patterns in errors\n",
    "        \n",
    "        Args:\n",
    "            false_positives (list): List of false positive examples\n",
    "            false_negatives (list): List of false negative examples\n",
    "            \n",
    "        Returns:\n",
    "            dict: Error patterns with explanations\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        \n",
    "        # Helper function to detect patterns\n",
    "        def check_patterns(errors, error_type):\n",
    "            for error in errors:\n",
    "                s1 = error['s1'].lower()\n",
    "                s2 = error['s2'].lower()\n",
    "                \n",
    "                # Check for length difference\n",
    "                len_ratio = min(len(s1), len(s2)) / max(len(s1), len(s2)) if max(len(s1), len(s2)) > 0 else 0\n",
    "                if len_ratio < 0.5:\n",
    "                    pattern = \"large_length_difference\"\n",
    "                    if pattern not in patterns:\n",
    "                        patterns[pattern] = {\n",
    "                            'name': \"Large length difference\",\n",
    "                            'description': \"One name is much shorter than the other\",\n",
    "                            'count': 0,\n",
    "                            'examples': [],\n",
    "                            'error_types': set()\n",
    "                        }\n",
    "                    patterns[pattern]['count'] += 1\n",
    "                    patterns[pattern]['error_types'].add(error_type)\n",
    "                    if len(patterns[pattern]['examples']) < 3:\n",
    "                        patterns[pattern]['examples'].append((s1, s2))\n",
    "                \n",
    "                # Check for containment\n",
    "                if s1 in s2 or s2 in s1:\n",
    "                    pattern = \"name_containment\"\n",
    "                    if pattern not in patterns:\n",
    "                        patterns[pattern] = {\n",
    "                            'name': \"Name containment\",\n",
    "                            'description': \"One name is contained within the other\",\n",
    "                            'count': 0,\n",
    "                            'examples': [],\n",
    "                            'error_types': set()\n",
    "                        }\n",
    "                    patterns[pattern]['count'] += 1\n",
    "                    patterns[pattern]['error_types'].add(error_type)\n",
    "                    if len(patterns[pattern]['examples']) < 3:\n",
    "                        patterns[pattern]['examples'].append((s1, s2))\n",
    "                \n",
    "                # Check for acronyms\n",
    "                if len(s1.split()) == 1 and len(s1) <= 5:\n",
    "                    # s1 might be acronym\n",
    "                    s2_words = s2.split()\n",
    "                    if len(s2_words) >= 2:\n",
    "                        s2_initials = ''.join([w[0] for w in s2_words if w])\n",
    "                        if s1 in s2_initials or s2_initials in s1:\n",
    "                            pattern = \"acronym_confusion\"\n",
    "                            if pattern not in patterns:\n",
    "                                patterns[pattern] = {\n",
    "                                    'name': \"Acronym confusion\",\n",
    "                                    'description': \"One name is an acronym or abbreviation of the other\",\n",
    "                                    'count': 0,\n",
    "                                    'examples': [],\n",
    "                                    'error_types': set()\n",
    "                                }\n",
    "                            patterns[pattern]['count'] += 1\n",
    "                            patterns[pattern]['error_types'].add(error_type)\n",
    "                            if len(patterns[pattern]['examples']) < 3:\n",
    "                                patterns[pattern]['examples'].append((s1, s2))\n",
    "                \n",
    "                # Check for word reordering\n",
    "                s1_words = set(s1.split())\n",
    "                s2_words = set(s2.split())\n",
    "                if s1_words == s2_words and s1 != s2:\n",
    "                    pattern = \"word_reordering\"\n",
    "                    if pattern not in patterns:\n",
    "                        patterns[pattern] = {\n",
    "                            'name': \"Word reordering\",\n",
    "                            'description': \"Names have the same words but in different order\",\n",
    "                            'count': 0,\n",
    "                            'examples': [],\n",
    "                            'error_types': set()\n",
    "                        }\n",
    "                    patterns[pattern]['count'] += 1\n",
    "                    patterns[pattern]['error_types'].add(error_type)\n",
    "                    if len(patterns[pattern]['examples']) < 3:\n",
    "                        patterns[pattern]['examples'].append((s1, s2))\n",
    "                \n",
    "                # Check for partial word matches\n",
    "                common_words = s1_words.intersection(s2_words)\n",
    "                if common_words and len(common_words) / max(len(s1_words), len(s2_words)) > 0.5:\n",
    "                    pattern = \"partial_word_match\"\n",
    "                    if pattern not in patterns:\n",
    "                        patterns[pattern] = {\n",
    "                            'name': \"Partial word match\",\n",
    "                            'description': \"Names share many common words\",\n",
    "                            'count': 0,\n",
    "                            'examples': [],\n",
    "                            'error_types': set()\n",
    "                        }\n",
    "                    patterns[pattern]['count'] += 1\n",
    "                    patterns[pattern]['error_types'].add(error_type)\n",
    "                    if len(patterns[pattern]['examples']) < 3:\n",
    "                        patterns[pattern]['examples'].append((s1, s2))\n",
    "        \n",
    "        # Check both error types\n",
    "        check_patterns(false_positives, \"false_positive\")\n",
    "        check_patterns(false_negatives, \"false_negative\")\n",
    "        \n",
    "        return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22f8709c-4046-43dc-b63c-1ec92024dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2: Visualization and Reporting\n",
    "\n",
    "class MerchantMatchingVisualizer:\n",
    "    \"\"\"\n",
    "    Comprehensive visualization toolkit for merchant matching evaluation results,\n",
    "    providing intuitive graphical displays of performance metrics, comparisons,\n",
    "    and error analysis.\n",
    "    \n",
    "    Key features:\n",
    "    - Performance metric visualizations (ROC curves, PR curves, confusion matrices)\n",
    "    - Multi-algorithm comparison charts\n",
    "    - Cross-validation result visualization\n",
    "    - Domain-specific performance analysis\n",
    "    - Error pattern visualization\n",
    "    - Interactive HTML report generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator=None, output_dir=None, interactive=True):\n",
    "        \"\"\"\n",
    "        Initialize visualizer with evaluator and output settings\n",
    "        \n",
    "        Args:\n",
    "            evaluator (MerchantMatchingEvaluator): Evaluator with results\n",
    "            output_dir (str): Directory for saving visualizations\n",
    "            interactive (bool): Whether to create interactive visualizations\n",
    "        \"\"\"\n",
    "        self.evaluator = evaluator\n",
    "        self.output_dir = output_dir or os.path.join(os.getcwd(), 'merchant_matching_results')\n",
    "        self.interactive = interactive\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        \n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger('MerchantVisualizer')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Check if matplotlib is available\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            self.matplotlib_available = True\n",
    "        except ImportError:\n",
    "            self.logger.warning(\"Matplotlib not available. Visualizations limited.\")\n",
    "            self.matplotlib_available = False\n",
    "        \n",
    "        # Check if plotly is available for interactive visualizations\n",
    "        if self.interactive:\n",
    "            try:\n",
    "                import plotly.graph_objects as go\n",
    "                self.plotly_available = True\n",
    "            except ImportError:\n",
    "                self.logger.warning(\"Plotly not available. Interactive visualizations disabled.\")\n",
    "                self.plotly_available = False\n",
    "                self.interactive = False\n",
    "        else:\n",
    "            self.plotly_available = False\n",
    "    \n",
    "    def plot_roc_curves(self, save=True, filename='roc_curves.png', figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Plot ROC curves for all evaluated matchers\n",
    "        \n",
    "        Args:\n",
    "            save (bool): Whether to save the plot to file\n",
    "            filename (str): Filename for saved plot\n",
    "            figsize (tuple): Figure size (width, height) in inches\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure or None: Figure object or None if plotting failed\n",
    "        \"\"\"\n",
    "        if not self.matplotlib_available or not self.evaluator or not self.evaluator.results:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            from sklearn.metrics import roc_curve, auc\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=figsize)\n",
    "            \n",
    "            # Sort matchers by AUC (descending)\n",
    "            sorted_matchers = sorted(\n",
    "                self.evaluator.results.items(),\n",
    "                key=lambda x: x[1].get('auc_roc', 0),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Plot ROC curve for each matcher\n",
    "            for name, metrics in sorted_matchers:\n",
    "                if 'auc_roc' not in metrics:\n",
    "                    continue\n",
    "                \n",
    "                # Check if we have test data available from the evaluator\n",
    "                test_data = getattr(self.evaluator, 'last_test_data', None)\n",
    "                if test_data is not None:\n",
    "                    # Extract column names\n",
    "                    s1_col = self.evaluator.default_cols['s1_col']\n",
    "                    s2_col = self.evaluator.default_cols['s2_col']\n",
    "                    label_col = self.evaluator.default_cols['label_col']\n",
    "                    domain_col = self.evaluator.default_cols['domain_col']\n",
    "                    \n",
    "                    # Get matcher\n",
    "                    if name == \"Primary\":\n",
    "                        matcher = self.evaluator.matcher\n",
    "                    else:\n",
    "                        matcher = self.evaluator.baseline_matchers.get(name)\n",
    "                    \n",
    "                    if matcher is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate predictions\n",
    "                    y_true = []\n",
    "                    y_scores = []\n",
    "                    \n",
    "                    for i, row in test_data.iterrows():\n",
    "                        try:\n",
    "                            s1 = row[s1_col]\n",
    "                            s2 = row[s2_col]\n",
    "                            \n",
    "                            # Skip if missing data\n",
    "                            if pd.isna(s1) or pd.isna(s2) or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                                continue\n",
    "                            \n",
    "                            # Get true label\n",
    "                            true_label = int(row[label_col])\n",
    "                            y_true.append(true_label)\n",
    "                            \n",
    "                            # Get domain if available\n",
    "                            domain = row[domain_col] if domain_col and domain_col in row else None\n",
    "                            \n",
    "                            # Calculate similarity score\n",
    "                            if hasattr(matcher, 'match_merchants'):\n",
    "                                score = matcher.match_merchants(s1, s2, domain)\n",
    "                            else:\n",
    "                                score = matcher(s1, s2, domain)\n",
    "                            \n",
    "                            y_scores.append(score)\n",
    "                            \n",
    "                        except Exception:\n",
    "                            # Skip examples with errors\n",
    "                            continue\n",
    "                    \n",
    "                    # Convert to numpy arrays\n",
    "                    y_true = np.array(y_true)\n",
    "                    y_scores = np.array(y_scores)\n",
    "                    \n",
    "                    # Calculate ROC curve\n",
    "                    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    \n",
    "                    # Plot ROC curve\n",
    "                    plt.plot(\n",
    "                        fpr, tpr, \n",
    "                        label=f\"{name} (AUC = {roc_auc:.3f})\",\n",
    "                        linewidth=2\n",
    "                    )\n",
    "                else:\n",
    "                    # Use AUC from metrics\n",
    "                    plt.plot(\n",
    "                        [0, 1], [0, 1], \n",
    "                        label=f\"{name} (AUC = {metrics['auc_roc']:.3f})\",\n",
    "                        linewidth=2\n",
    "                    )\n",
    "            \n",
    "            # Add diagonal line\n",
    "            plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "            \n",
    "            # Add labels and legend\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(alpha=0.3)\n",
    "            \n",
    "            # Save if requested\n",
    "            if save:\n",
    "                save_path = os.path.join(self.output_dir, filename)\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                self.logger.info(f\"ROC curves saved to {save_path}\")\n",
    "            \n",
    "            return plt.gcf()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error plotting ROC curves: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_precision_recall_curves(self, save=True, filename='pr_curves.png', figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Plot precision-recall curves for all evaluated matchers\n",
    "        \n",
    "        Args:\n",
    "            save (bool): Whether to save the plot to file\n",
    "            filename (str): Filename for saved plot\n",
    "            figsize (tuple): Figure size (width, height) in inches\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure or None: Figure object or None if plotting failed\n",
    "        \"\"\"\n",
    "        if not self.matplotlib_available or not self.evaluator or not self.evaluator.results:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            from sklearn.metrics import precision_recall_curve, auc\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=figsize)\n",
    "            \n",
    "            # Sort matchers by PR AUC (descending)\n",
    "            sorted_matchers = sorted(\n",
    "                self.evaluator.results.items(),\n",
    "                key=lambda x: x[1].get('auc_pr', 0),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Plot PR curve for each matcher\n",
    "            for name, metrics in sorted_matchers:\n",
    "                if 'auc_pr' not in metrics:\n",
    "                    continue\n",
    "                \n",
    "                # Check if we have test data available from the evaluator\n",
    "                test_data = getattr(self.evaluator, 'last_test_data', None)\n",
    "                if test_data is not None:\n",
    "                    # Extract column names\n",
    "                    s1_col = self.evaluator.default_cols['s1_col']\n",
    "                    s2_col = self.evaluator.default_cols['s2_col']\n",
    "                    label_col = self.evaluator.default_cols['label_col']\n",
    "                    domain_col = self.evaluator.default_cols['domain_col']\n",
    "                    \n",
    "                    # Get matcher\n",
    "                    if name == \"Primary\":\n",
    "                        matcher = self.evaluator.matcher\n",
    "                    else:\n",
    "                        matcher = self.evaluator.baseline_matchers.get(name)\n",
    "                    \n",
    "                    if matcher is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate predictions\n",
    "                    y_true = []\n",
    "                    y_scores = []\n",
    "                    \n",
    "                    for i, row in test_data.iterrows():\n",
    "                        try:\n",
    "                            s1 = row[s1_col]\n",
    "                            s2 = row[s2_col]\n",
    "                            \n",
    "                            # Skip if missing data\n",
    "                            if pd.isna(s1) or pd.isna(s2) or not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                                continue\n",
    "                            \n",
    "                            # Get true label\n",
    "                            true_label = int(row[label_col])\n",
    "                            y_true.append(true_label)\n",
    "                            \n",
    "                            # Get domain if available\n",
    "                            domain = row[domain_col] if domain_col and domain_col in row else None\n",
    "                            \n",
    "                            # Calculate similarity score\n",
    "                            if hasattr(matcher, 'match_merchants'):\n",
    "                                score = matcher.match_merchants(s1, s2, domain)\n",
    "                            else:\n",
    "                                score = matcher(s1, s2, domain)\n",
    "                            \n",
    "                            y_scores.append(score)\n",
    "                            \n",
    "                        except Exception:\n",
    "                            # Skip examples with errors\n",
    "                            continue\n",
    "                    \n",
    "                    # Convert to numpy arrays\n",
    "                    y_true = np.array(y_true)\n",
    "                    y_scores = np.array(y_scores)\n",
    "                    \n",
    "                    # Calculate PR curve\n",
    "                    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    \n",
    "                    # Plot PR curve\n",
    "                    plt.plot(\n",
    "                        recall, precision, \n",
    "                        label=f\"{name} (AUC = {pr_auc:.3f})\",\n",
    "                        linewidth=2\n",
    "                    )\n",
    "                else:\n",
    "                    # Use AUC from metrics\n",
    "                    plt.plot(\n",
    "                        [0, 1], [1, 0], \n",
    "                        label=f\"{name} (AUC = {metrics['auc_pr']:.3f})\",\n",
    "                        linewidth=2\n",
    "                    )\n",
    "            \n",
    "            # Add labels and legend\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Precision-Recall Curves')\n",
    "            plt.legend(loc='lower left')\n",
    "            plt.grid(alpha=0.3)\n",
    "            \n",
    "            # Save if requested\n",
    "            if save:\n",
    "                save_path = os.path.join(self.output_dir, filename)\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                self.logger.info(f\"Precision-recall curves saved to {save_path}\")\n",
    "            \n",
    "            return plt.gcf()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error plotting precision-recall curves: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_metric_comparison(self, metrics=None, save=True, filename='metric_comparison.png', figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Plot comparison of multiple metrics across all matchers\n",
    "        \n",
    "        Args:\n",
    "            metrics (list): List of metrics to compare\n",
    "            save (bool): Whether to save the plot to file\n",
    "            filename (str): Filename for saved plot\n",
    "            figsize (tuple): Figure size (width, height) in inches\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure or None: Figure object or None if plotting failed\n",
    "        \"\"\"\n",
    "        if not self.matplotlib_available or not self.evaluator or not self.evaluator.results:\n",
    "            return None\n",
    "        \n",
    "        # Default metrics to compare\n",
    "        if metrics is None:\n",
    "            metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=figsize)\n",
    "            \n",
    "            # Get matcher names and sort by f1_score\n",
    "            matcher_names = sorted(\n",
    "                self.evaluator.results.keys(),\n",
    "                key=lambda x: self.evaluator.results[x].get('f1_score', 0),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Set up bar positions\n",
    "            x = np.arange(len(matcher_names))\n",
    "            width = 0.15  # width of bars\n",
    "            \n",
    "            # Create a bar for each metric\n",
    "            for i, metric in enumerate(metrics):\n",
    "                values = [self.evaluator.results[name].get(metric, 0) for name in matcher_names]\n",
    "                plt.bar(x + (i - len(metrics)/2 + 0.5) * width, values, width, label=metric.replace('_', ' ').title())\n",
    "            \n",
    "            # Add labels and legend\n",
    "            plt.xlabel('Matcher')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title('Performance Metric Comparison')\n",
    "            plt.xticks(x, matcher_names, rotation=45, ha='right')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save if requested\n",
    "            if save:\n",
    "                save_path = os.path.join(self.output_dir, filename)\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                self.logger.info(f\"Metric comparison saved to {save_path}\")\n",
    "            \n",
    "            return plt.gcf()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error plotting metric comparison: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_confusion_matrices(self, save=True, filename_prefix='confusion_matrix_', figsize=(8, 6)):\n",
    "        \"\"\"\n",
    "        Plot confusion matrices for all evaluated matchers\n",
    "        \n",
    "        Args:\n",
    "            save (bool): Whether to save the plots to files\n",
    "            filename_prefix (str): Prefix for saved plot filenames\n",
    "            figsize (tuple): Figure size (width, height) in inches\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of matcher name to figure object\n",
    "        \"\"\"\n",
    "        if not self.matplotlib_available or not self.evaluator or not self.evaluator.results:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            \n",
    "            figures = {}\n",
    "            \n",
    "            # For each matcher\n",
    "            for name, metrics in self.evaluator.results.items():\n",
    "                # Check if confusion matrix components are available\n",
    "                required_keys = ['true_positives', 'false_positives', 'true_negatives', 'false_negatives']\n",
    "                if not all(key in metrics for key in required_keys):\n",
    "                    continue\n",
    "                \n",
    "                # Create confusion matrix\n",
    "                cm = np.array([\n",
    "                    [metrics['true_negatives'], metrics['false_positives']],\n",
    "                    [metrics['false_negatives'], metrics['true_positives']]\n",
    "                ])\n",
    "                \n",
    "                # Create figure\n",
    "                plt.figure(figsize=figsize)\n",
    "                \n",
    "                # Plot confusion matrix\n",
    "                plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "                plt.title(f'Confusion Matrix: {name}')\n",
    "                plt.colorbar()\n",
    "                \n",
    "                # Add labels\n",
    "                classes = ['Non-match', 'Match']\n",
    "                tick_marks = np.arange(len(classes))\n",
    "                plt.xticks(tick_marks, classes)\n",
    "                plt.yticks(tick_marks, classes)\n",
    "                \n",
    "                # Add text annotations\n",
    "                thresh = cm.max() / 2.0\n",
    "                for i in range(cm.shape[0]):\n",
    "                    for j in range(cm.shape[1]):\n",
    "                        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                                horizontalalignment=\"center\",\n",
    "                                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "                \n",
    "                plt.ylabel('True Label')\n",
    "                plt.xlabel('Predicted Label')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save if requested\n",
    "                if save:\n",
    "                    filename = f\"{filename_prefix}{name.lower().replace(' ', '_')}.png\"\n",
    "                    save_path = os.path.join(self.output_dir, filename)\n",
    "                    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                    self.logger.info(f\"Confusion matrix for {name} saved to {save_path}\")\n",
    "                \n",
    "                figures[name] = plt.gcf()\n",
    "            \n",
    "            return figures\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error plotting confusion matrices: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_cross_validation_results(self, matcher_name=\"Primary\", save=True, \n",
    "                                   filename='cross_validation_results.png', figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Plot cross-validation results for a matcher\n",
    "        \n",
    "        Args:\n",
    "            matcher_name (str): Name of the matcher to visualize\n",
    "            save (bool): Whether to save the plot to file\n",
    "            filename (str): Filename for saved plot\n",
    "            figsize (tuple): Figure size (width, height) in inches\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure or None: Figure object or None if plotting failed\n",
    "        \"\"\"\n",
    "        if (not self.matplotlib_available or not self.evaluator or \n",
    "            not hasattr(self.evaluator, 'cross_validation_results') or \n",
    "            matcher_name not in self.evaluator.cross_validation_results):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            \n",
    "            # Get cross-validation results\n",
    "            cv_results = self.evaluator.cross_validation_results[matcher_name]\n",
    "            \n",
    "            # Create figure\n",
    "            fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "            \n",
    "            # Plot metric values across folds\n",
    "            metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']\n",
    "            \n",
    "            # Extract fold metrics\n",
    "            fold_metrics = cv_results['fold_metrics']\n",
    "            folds = range(1, len(fold_metrics) + 1)\n",
    "            \n",
    "            # Plot metrics\n",
    "            for metric in metrics_to_plot:\n",
    "                values = [fold.get(metric, 0) for fold in fold_metrics]\n",
    "                axs[0].plot(folds, values, 'o-', label=metric.replace('_', ' ').title())\n",
    "            \n",
    "            axs[0].set_xlabel('Fold')\n",
    "            axs[0].set_ylabel('Score')\n",
    "            axs[0].set_title('Metrics Across Folds')\n",
    "            axs[0].set_xticks(folds)\n",
    "            axs[0].legend()\n",
    "            axs[0].grid(alpha=0.3)\n",
    "            \n",
    "            # Plot aggregate metrics with error bars\n",
    "            aggregate_metrics = cv_results['aggregate_metrics']\n",
    "            metrics = list(aggregate_metrics.keys())\n",
    "            values = [aggregate_metrics[metric]['mean'] for metric in metrics]\n",
    "            errors = [aggregate_metrics[metric]['std'] for metric in metrics]\n",
    "            \n",
    "            # Format metric names for display\n",
    "            formatted_metrics = [metric.replace('_', ' ').title() for metric in metrics]\n",
    "            \n",
    "            axs[1].barh(formatted_metrics, values, xerr=errors, alpha=0.7)\n",
    "            \n",
    "            for i, value in enumerate(values):\n",
    "                axs[1].text(value, i, f\" {value:.3f}  {errors[i]:.3f}\", va='center')\n",
    "            \n",
    "            axs[1].set_title('Aggregate Metrics (Mean  Std)')\n",
    "            axs[1].set_xlim(0, 1.0)\n",
    "            axs[1].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save if requested\n",
    "            if save:\n",
    "                save_path = os.path.join(self.output_dir, filename)\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                self.logger.info(f\"Cross-validation results saved to {save_path}\")\n",
    "            \n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error plotting cross-validation results: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_domain_performance(self, matcher_name=\"Primary\", save=True,\n",
    "                              filename='domain_performance.png', figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Plot performance across different domains for a matcher\n",
    "        \n",
    "        Args:\n",
    "            matcher_name (str): Name of the matcher to visualize\n",
    "            save (bool): Whether to save the plot to file\n",
    "            filename (str): Filename for saved plot\n",
    "            figsize (tuple): Figure size (width, height) in inches\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure or None: Figure object or None if plotting failed\n",
    "        \"\"\"\n",
    "        if not self.matplotlib_available or not self.evaluator or not self.evaluator.results:\n",
    "            return None\n",
    "        \n",
    "        # Check if domain metrics are available\n",
    "        if (matcher_name not in self.evaluator.results or \n",
    "            'domain_metrics' not in self.evaluator.results[matcher_name]):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            \n",
    "            # Get domain metrics\n",
    "            domain_metrics = self.evaluator.results[matcher_name]['domain_metrics']\n",
    "            \n",
    "            # Skip if empty\n",
    "            if not domain_metrics:\n",
    "                return None\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=figsize)\n",
    "            \n",
    "            # Get domains and metrics\n",
    "            domains = list(domain_metrics.keys())\n",
    "            metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "            \n",
    "            # Set up positions\n",
    "            x = np.arange(len(domains))\n",
    "            width = 0.2  # width of bars\n",
    "            \n",
    "            # Plot each metric\n",
    "            for i, metric in enumerate(metrics):\n",
    "                values = [domain_metrics[domain].get(metric, 0) for domain in domains]\n",
    "                plt.bar(x + (i - len(metrics)/2 + 0.5) * width, values, width, label=metric.replace('_', ' ').title())\n",
    "            \n",
    "            # Add labels and legend\n",
    "            plt.xlabel('Domain')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title(f'Performance by Domain: {matcher_name}')\n",
    "            plt.xticks(x, domains, rotation=45, ha='right')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save if requested\n",
    "            if save:\n",
    "                save_path = os.path.join(self.output_dir, filename)\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                self.logger.info(f\"Domain performance saved to {save_path}\")\n",
    "            \n",
    "            return plt.gcf()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error plotting domain performance: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_error_analysis(self, matcher_name=\"Primary\", save=True,\n",
    "                          filename='error_analysis.png', figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Plot error analysis results for a matcher\n",
    "        \n",
    "        Args:\n",
    "            matcher_name (str): Name of the matcher to visualize\n",
    "            save (bool): Whether to save the plot to file\n",
    "            filename (str): Filename for saved plot\n",
    "            figsize (tuple): Figure size (width, height) in inches\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure or None: Figure object or None if plotting failed\n",
    "        \"\"\"\n",
    "        if (not self.matplotlib_available or not self.evaluator or \n",
    "            not hasattr(self.evaluator, 'error_analysis') or\n",
    "            matcher_name not in self.evaluator.error_analysis):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            \n",
    "            # Get error analysis results\n",
    "            error_analysis = self.evaluator.error_analysis[matcher_name]\n",
    "            \n",
    "            # Create figure\n",
    "            fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "            \n",
    "            # Plot error counts\n",
    "            error_types = ['false_positives', 'false_negatives']\n",
    "            error_counts = [error_analysis['error_counts'][et] for et in error_types]\n",
    "            \n",
    "            axs[0].bar(error_types, error_counts, color=['#ff9999', '#99ccff'])\n",
    "            \n",
    "            for i, count in enumerate(error_counts):\n",
    "                axs[0].text(i, count, str(count), ha='center', va='bottom')\n",
    "            \n",
    "            axs[0].set_ylabel('Count')\n",
    "            axs[0].set_title('Error Counts')\n",
    "            axs[0].set_xticklabels([et.replace('_', ' ').title() for et in error_types])\n",
    "            \n",
    "            # Plot error patterns\n",
    "            patterns = error_analysis['error_patterns']\n",
    "            \n",
    "            if patterns:\n",
    "                # Get top patterns by count\n",
    "                top_patterns = sorted(\n",
    "                    patterns.items(),\n",
    "                    key=lambda x: x[1]['count'],\n",
    "                    reverse=True\n",
    "                )[:5]\n",
    "                \n",
    "                pattern_names = [p[1]['name'] for p in top_patterns]\n",
    "                pattern_counts = [p[1]['count'] for p in top_patterns]\n",
    "                \n",
    "                # Plot horizontal bar chart\n",
    "                axs[1].barh(pattern_names, pattern_counts, color='#99cc99')\n",
    "                \n",
    "                for i, count in enumerate(pattern_counts):\n",
    "                    axs[1].text(count, i, str(count), va='center')\n",
    "                \n",
    "                axs[1].set_xlabel('Count')\n",
    "                axs[1].set_title('Top Error Patterns')\n",
    "            else:\n",
    "                axs[1].text(0.5, 0.5, \"No error patterns identified\", ha='center', va='center')\n",
    "                axs[1].set_title('Error Patterns')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save if requested\n",
    "            if save:\n",
    "                save_path = os.path.join(self.output_dir, filename)\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                self.logger.info(f\"Error analysis saved to {save_path}\")\n",
    "            \n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error plotting error analysis: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_significance_tests(self, save=True, filename='significance_tests.png', figsize=(10, 6)):\n",
    "        \"\"\"\n",
    "        Plot statistical significance test results\n",
    "        \n",
    "        Args:\n",
    "            save (bool): Whether to save the plot to file\n",
    "            filename (str): Filename for saved plot\n",
    "            figsize (tuple): Figure size (width, height) in inches\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure or None: Figure object or None if plotting failed\n",
    "        \"\"\"\n",
    "        if (not self.matplotlib_available or not self.evaluator or \n",
    "            not hasattr(self.evaluator, 'statistical_tests') or\n",
    "            not self.evaluator.statistical_tests):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            \n",
    "            # Get significance test results\n",
    "            tests = self.evaluator.statistical_tests\n",
    "            \n",
    "            # Filter to just the McNemar tests (not the Wilcoxon tests)\n",
    "            mcnemar_tests = {k: v for k, v in tests.items() if 'wilcoxon' not in k}\n",
    "            \n",
    "            if not mcnemar_tests:\n",
    "                return None\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=figsize)\n",
    "            \n",
    "            # Get baseline names and p-values\n",
    "            baselines = list(mcnemar_tests.keys())\n",
    "            p_values = [mcnemar_tests[b]['p_value'] for b in baselines]\n",
    "            \n",
    "            # Sort by p-value\n",
    "            sorted_indices = np.argsort(p_values)\n",
    "            baselines = [baselines[i] for i in sorted_indices]\n",
    "            p_values = [p_values[i] for i in sorted_indices]\n",
    "            \n",
    "            # Plot bar chart of p-values\n",
    "            bars = plt.bar(baselines, p_values, color=['green' if p < 0.05 else 'red' for p in p_values])\n",
    "            \n",
    "            # Add significance threshold line\n",
    "            plt.axhline(y=0.05, color='black', linestyle='--', alpha=0.7, label='Significance threshold (p=0.05)')\n",
    "            \n",
    "            # Add labels\n",
    "            for i, (bar, p) in enumerate(zip(bars, p_values)):\n",
    "                plt.text(\n",
    "                    bar.get_x() + bar.get_width()/2,\n",
    "                    0.01,\n",
    "                    f\"p={p:.4f}\",\n",
    "                    ha='center',\n",
    "                    rotation=90,\n",
    "                    color='white' if p < 0.05 else 'black'\n",
    "                )\n",
    "                \n",
    "                # Add significance indicator\n",
    "                if p < 0.05:\n",
    "                    plt.text(\n",
    "                        bar.get_x() + bar.get_width()/2,\n",
    "                        p + 0.02,\n",
    "                        '*',\n",
    "                        ha='center',\n",
    "                        fontsize=16\n",
    "                    )\n",
    "            \n",
    "            plt.xlabel('Baseline Matcher')\n",
    "            plt.ylabel('p-value')\n",
    "            plt.title(\"Statistical Significance Tests (McNemar's Test)\")\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save if requested\n",
    "            if save:\n",
    "                save_path = os.path.join(self.output_dir, filename)\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                self.logger.info(f\"Significance tests saved to {save_path}\")\n",
    "            \n",
    "            return plt.gcf()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error plotting significance tests: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_html_report(self, title=\"Merchant Matching Evaluation Report\", \n",
    "                            filename='merchant_matching_report.html'):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive HTML report with all visualizations and results\n",
    "        \n",
    "        Args:\n",
    "            title (str): Report title\n",
    "            filename (str): Filename for saved report\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to the generated HTML report\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Save all plots first\n",
    "            self.plot_roc_curves()\n",
    "            self.plot_precision_recall_curves()\n",
    "            self.plot_metric_comparison()\n",
    "            self.plot_confusion_matrices()\n",
    "            self.plot_cross_validation_results()\n",
    "            self.plot_domain_performance()\n",
    "            self.plot_error_analysis()\n",
    "            self.plot_significance_tests()\n",
    "            \n",
    "            # Generate HTML content\n",
    "            html_content = f\"\"\"\n",
    "            <!DOCTYPE html>\n",
    "            <html>\n",
    "            <head>\n",
    "                <title>{title}</title>\n",
    "                <style>\n",
    "                    body {{\n",
    "                        font-family: Arial, sans-serif;\n",
    "                        margin: 20px;\n",
    "                        line-height: 1.6;\n",
    "                    }}\n",
    "                    h1, h2, h3 {{\n",
    "                        color: #2c3e50;\n",
    "                    }}\n",
    "                    .section {{\n",
    "                        margin-bottom: 30px;\n",
    "                        border-bottom: 1px solid #eee;\n",
    "                        padding-bottom: 20px;\n",
    "                    }}\n",
    "                    table {{\n",
    "                        border-collapse: collapse;\n",
    "                        width: 100%;\n",
    "                    }}\n",
    "                    th, td {{\n",
    "                        text-align: left;\n",
    "                        padding: 8px;\n",
    "                        border-bottom: 1px solid #ddd;\n",
    "                    }}\n",
    "                    th {{\n",
    "                        background-color: #f2f2f2;\n",
    "                    }}\n",
    "                    tr:hover {{\n",
    "                        background-color: #f5f5f5;\n",
    "                    }}\n",
    "                    .figure {{\n",
    "                        margin: 20px 0;\n",
    "                        text-align: center;\n",
    "                    }}\n",
    "                    .figure img {{\n",
    "                        max-width: 100%;\n",
    "                        box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "                    }}\n",
    "                    .caption {{\n",
    "                        font-style: italic;\n",
    "                        color: #666;\n",
    "                        margin-top: 10px;\n",
    "                    }}\n",
    "                    .metric-good {{\n",
    "                        color: green;\n",
    "                        font-weight: bold;\n",
    "                    }}\n",
    "                    .metric-medium {{\n",
    "                        color: orange;\n",
    "                    }}\n",
    "                    .metric-poor {{\n",
    "                        color: red;\n",
    "                    }}\n",
    "                </style>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1>{title}</h1>\n",
    "                <p>Generated on {time.strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <h2>Overview</h2>\n",
    "                    <p>This report presents the evaluation results of various merchant name matching algorithms.</p>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Add summary table\n",
    "            if self.evaluator and self.evaluator.results:\n",
    "                html_content += \"\"\"\n",
    "                    <h3>Performance Summary</h3>\n",
    "                    <table>\n",
    "                        <tr>\n",
    "                            <th>Matcher</th>\n",
    "                            <th>Accuracy</th>\n",
    "                            <th>Precision</th>\n",
    "                            <th>Recall</th>\n",
    "                            <th>F1 Score</th>\n",
    "                            <th>AUC-ROC</th>\n",
    "                        </tr>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Sort by F1 score\n",
    "                sorted_matchers = sorted(\n",
    "                    self.evaluator.results.items(),\n",
    "                    key=lambda x: x[1].get('f1_score', 0),\n",
    "                    reverse=True\n",
    "                )\n",
    "                \n",
    "                for name, metrics in sorted_matchers:\n",
    "                    html_content += f\"\"\"\n",
    "                        <tr>\n",
    "                            <td>{name}</td>\n",
    "                            <td class=\"{'metric-good' if metrics.get('accuracy', 0) > 0.8 else 'metric-medium' if metrics.get('accuracy', 0) > 0.6 else 'metric-poor'}\">{metrics.get('accuracy', 0):.4f}</td>\n",
    "                            <td class=\"{'metric-good' if metrics.get('precision', 0) > 0.8 else 'metric-medium' if metrics.get('precision', 0) > 0.6 else 'metric-poor'}\">{metrics.get('precision', 0):.4f}</td>\n",
    "                            <td class=\"{'metric-good' if metrics.get('recall', 0) > 0.8 else 'metric-medium' if metrics.get('recall', 0) > 0.6 else 'metric-poor'}\">{metrics.get('recall', 0):.4f}</td>\n",
    "                            <td class=\"{'metric-good' if metrics.get('f1_score', 0) > 0.8 else 'metric-medium' if metrics.get('f1_score', 0) > 0.6 else 'metric-poor'}\">{metrics.get('f1_score', 0):.4f}</td>\n",
    "                            <td class=\"{'metric-good' if metrics.get('auc_roc', 0) > 0.8 else 'metric-medium' if metrics.get('auc_roc', 0) > 0.6 else 'metric-poor'}\">{metrics.get('auc_roc', 0):.4f}</td>\n",
    "                        </tr>\n",
    "                    \"\"\"\n",
    "                \n",
    "                html_content += \"\"\"\n",
    "                    </table>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Add ROC curves\n",
    "            html_content += \"\"\"\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <h2>ROC Curves</h2>\n",
    "                    <div class=\"figure\">\n",
    "                        <img src=\"roc_curves.png\" alt=\"ROC Curves\">\n",
    "                        <p class=\"caption\">Receiver Operating Characteristic (ROC) curves for all matchers.</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <h2>Precision-Recall Curves</h2>\n",
    "                    <div class=\"figure\">\n",
    "                        <img src=\"pr_curves.png\" alt=\"Precision-Recall Curves\">\n",
    "                        <p class=\"caption\">Precision-Recall curves for all matchers.</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <h2>Metric Comparison</h2>\n",
    "                    <div class=\"figure\">\n",
    "                        <img src=\"metric_comparison.png\" alt=\"Metric Comparison\">\n",
    "                        <p class=\"caption\">Comparison of performance metrics across all matchers.</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Add cross-validation results if available\n",
    "            if hasattr(self.evaluator, 'cross_validation_results') and self.evaluator.cross_validation_results:\n",
    "                html_content += \"\"\"\n",
    "                <div class=\"section\">\n",
    "                    <h2>Cross-Validation Results</h2>\n",
    "                    <div class=\"figure\">\n",
    "                        <img src=\"cross_validation_results.png\" alt=\"Cross-Validation Results\">\n",
    "                        <p class=\"caption\">Cross-validation results showing performance stability across folds.</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Add domain performance if available\n",
    "            if (self.evaluator and self.evaluator.results and \n",
    "                any('domain_metrics' in metrics for metrics in self.evaluator.results.values())):\n",
    "                html_content += \"\"\"\n",
    "                <div class=\"section\">\n",
    "                    <h2>Domain-Specific Performance</h2>\n",
    "                    <div class=\"figure\">\n",
    "                        <img src=\"domain_performance.png\" alt=\"Domain Performance\">\n",
    "                        <p class=\"caption\">Performance metrics across different domains.</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Add error analysis if available\n",
    "            if hasattr(self.evaluator, 'error_analysis') and self.evaluator.error_analysis:\n",
    "                html_content += \"\"\"\n",
    "                <div class=\"section\">\n",
    "                    <h2>Error Analysis</h2>\n",
    "                    <div class=\"figure\">\n",
    "                        <img src=\"error_analysis.png\" alt=\"Error Analysis\">\n",
    "                        <p class=\"caption\">Analysis of error types and patterns.</p>\n",
    "                    </div>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Add example errors\n",
    "                if \"Primary\" in self.evaluator.error_analysis:\n",
    "                    error_analysis = self.evaluator.error_analysis[\"Primary\"]\n",
    "                    \n",
    "                    # Add false positives\n",
    "                    if error_analysis['false_positives']:\n",
    "                        html_content += \"\"\"\n",
    "                        <h3>Example False Positives</h3>\n",
    "                        <table>\n",
    "                            <tr>\n",
    "                                <th>First Name</th>\n",
    "                                <th>Second Name</th>\n",
    "                                <th>Score</th>\n",
    "                                <th>Domain</th>\n",
    "                            </tr>\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        for fp in error_analysis['false_positives'][:5]:  # Show top 5\n",
    "                            html_content += f\"\"\"\n",
    "                            <tr>\n",
    "                                <td>{fp['s1']}</td>\n",
    "                                <td>{fp['s2']}</td>\n",
    "                                <td>{fp['score']:.4f}</td>\n",
    "                                <td>{fp.get('domain', 'N/A')}</td>\n",
    "                            </tr>\n",
    "                            \"\"\"\n",
    "                        \n",
    "                        html_content += \"\"\"\n",
    "                        </table>\n",
    "                        \"\"\"\n",
    "                    \n",
    "                    # Add false negatives\n",
    "                    if error_analysis['false_negatives']:\n",
    "                        html_content += \"\"\"\n",
    "                        <h3>Example False Negatives</h3>\n",
    "                        <table>\n",
    "                            <tr>\n",
    "                                <th>First Name</th>\n",
    "                                <th>Second Name</th>\n",
    "                                <th>Score</th>\n",
    "                                <th>Domain</th>\n",
    "                            </tr>\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        for fn in error_analysis['false_negatives'][:5]:  # Show top 5\n",
    "                            html_content += f\"\"\"\n",
    "                            <tr>\n",
    "                                <td>{fn['s1']}</td>\n",
    "                                <td>{fn['s2']}</td>\n",
    "                                <td>{fn['score']:.4f}</td>\n",
    "                                <td>{fn.get('domain', 'N/A')}</td>\n",
    "                            </tr>\n",
    "                            \"\"\"\n",
    "                        \n",
    "                        html_content += \"\"\"\n",
    "                        </table>\n",
    "                        \"\"\"\n",
    "                \n",
    "                html_content += \"\"\"\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Add significance tests if available\n",
    "            if hasattr(self.evaluator, 'statistical_tests') and self.evaluator.statistical_tests:\n",
    "                html_content += \"\"\"\n",
    "                <div class=\"section\">\n",
    "                    <h2>Statistical Significance</h2>\n",
    "                    <div class=\"figure\">\n",
    "                        <img src=\"significance_tests.png\" alt=\"Significance Tests\">\n",
    "                        <p class=\"caption\">Statistical significance of performance improvements.</p>\n",
    "                    </div>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Add significance test results table\n",
    "                html_content += \"\"\"\n",
    "                    <h3>Significance Test Results</h3>\n",
    "                    <table>\n",
    "                        <tr>\n",
    "                            <th>Baseline</th>\n",
    "                            <th>Test</th>\n",
    "                            <th>p-value</th>\n",
    "                            <th>Significant</th>\n",
    "                        </tr>\n",
    "                \"\"\"\n",
    "                \n",
    "                for name, test in self.evaluator.statistical_tests.items():\n",
    "                    if 'wilcoxon' in name:  # Skip Wilcoxon tests for simplicity\n",
    "                        continue\n",
    "                        \n",
    "                    html_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>{name}</td>\n",
    "                        <td>{test['test_name']}</td>\n",
    "                        <td>{test['p_value']:.4f}</td>\n",
    "                        <td class=\"{'metric-good' if test['significant'] else 'metric-poor'}\">{test['significant']}</td>\n",
    "                    </tr>\n",
    "                    \"\"\"\n",
    "                \n",
    "                html_content += \"\"\"\n",
    "                    </table>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Close HTML\n",
    "            html_content += \"\"\"\n",
    "                <div class=\"section\">\n",
    "                    <h2>Conclusion</h2>\n",
    "                    <p>This evaluation demonstrates the performance characteristics of various merchant name matching algorithms.</p>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Add overall recommendation\n",
    "            if self.evaluator and self.evaluator.results and \"Primary\" in self.evaluator.results:\n",
    "                # Check if Primary is the best\n",
    "                is_best = True\n",
    "                primary_f1 = self.evaluator.results[\"Primary\"].get('f1_score', 0)\n",
    "                \n",
    "                for name, metrics in self.evaluator.results.items():\n",
    "                    if name != \"Primary\" and metrics.get('f1_score', 0) > primary_f1:\n",
    "                        is_best = False\n",
    "                        break\n",
    "                \n",
    "                if is_best:\n",
    "                    html_content += \"\"\"\n",
    "                    <p class=\"metric-good\">The primary hybrid algorithm outperforms all baseline algorithms, demonstrating superior merchant name matching capability.</p>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    html_content += \"\"\"\n",
    "                    <p class=\"metric-medium\">The primary algorithm shows competitive performance, but some baseline algorithms may perform better in specific scenarios.</p>\n",
    "                    \"\"\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "                </div>\n",
    "                \n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Write to file\n",
    "            report_path = os.path.join(self.output_dir, filename)\n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write(html_content)\n",
    "            \n",
    "            self.logger.info(f\"HTML report generated at {report_path}\")\n",
    "            \n",
    "            return report_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating HTML report: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a78e01c3-9b16-4feb-aaa1-bc57591f1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.3: Comprehensive Evaluation Pipeline\n",
    "class MerchantMatchingEvaluationPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for rigorous evaluation of merchant name matching algorithms\n",
    "    with comprehensive analytics, comparison, and reporting capabilities.\n",
    "    \n",
    "    This pipeline integrates data loading, preprocessing, evaluation, validation,\n",
    "    and reporting into a unified workflow, ensuring consistent and thorough\n",
    "    assessment of matching performance.\n",
    "    \n",
    "    Key features:\n",
    "    - Automated test suite generation from real-world data\n",
    "    - Comprehensive performance evaluation across matchers\n",
    "    - Cross-validation for robust assessment\n",
    "    - Domain-specific analysis\n",
    "    - Error pattern analysis and classification\n",
    "    - Detailed reports with actionable insights\n",
    "    - Performance visualization and comparison tools\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, matchers=None, data_path=None, test_set=None, \n",
    "                 domains=None, metrics=None, k_folds=5, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the evaluation pipeline with matchers and configuration\n",
    "        \n",
    "        Args:\n",
    "            matchers (dict): Dictionary mapping matcher names to matcher instances\n",
    "            data_path (str, optional): Path to evaluation dataset\n",
    "            test_set (DataFrame, optional): Predefined test set\n",
    "            domains (list, optional): List of domains to evaluate\n",
    "            metrics (list, optional): List of evaluation metrics to use\n",
    "            k_folds (int): Number of folds for cross-validation\n",
    "            random_state (int): Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize matcher registry\n",
    "        self.matchers = matchers or {}\n",
    "        \n",
    "        # Set evaluation parameters\n",
    "        self.data_path = data_path\n",
    "        self.test_set = test_set\n",
    "        self.domains = domains or ['general', 'banking', 'retail', 'restaurant', 'hotel']\n",
    "        \n",
    "        # Set default metrics if not provided\n",
    "        self.metrics = metrics or [\n",
    "            'precision', 'recall', 'f1_score', 'accuracy', \n",
    "            'roc_auc', 'confusion_matrix', 'precision_at_k'\n",
    "        ]\n",
    "        \n",
    "        # Cross-validation parameters\n",
    "        self.k_folds = k_folds\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize results storage\n",
    "        self.results = {}\n",
    "        self.domain_results = {}\n",
    "        self.error_analysis = {}\n",
    "        self.cross_val_results = {}\n",
    "        \n",
    "        # Load test data if path provided\n",
    "        if data_path and not test_set:\n",
    "            self.test_set = self._load_test_data(data_path)\n",
    "        \n",
    "        # Register standard error types for classification\n",
    "        self.error_types = {\n",
    "            'acronym_errors': 'Failures in matching acronyms to full forms',\n",
    "            'spelling_errors': 'Failures due to misspellings or typos',\n",
    "            'structural_errors': 'Failures due to word order or structural differences',\n",
    "            'semantic_errors': 'Failures in understanding semantic relationships',\n",
    "            'domain_specific_errors': 'Failures due to domain-specific terminology',\n",
    "            'false_positives': 'Incorrect matches between different merchants',\n",
    "            'false_negatives': 'Missed matches between same merchants'\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"Evaluation pipeline initialized with {len(self.matchers)} matchers\")\n",
    "    \n",
    "    def register_matcher(self, name, matcher):\n",
    "        \"\"\"\n",
    "        Register a matcher for evaluation\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name identifier for the matcher\n",
    "            matcher: Matcher instance with match_merchants method\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        if name in self.matchers:\n",
    "            self.logger.warning(f\"Matcher '{name}' already registered. Overwriting.\")\n",
    "            \n",
    "        # Validate matcher interface\n",
    "        if not hasattr(matcher, 'match_merchants'):\n",
    "            self.logger.error(f\"Invalid matcher '{name}': missing match_merchants method\")\n",
    "            return False\n",
    "            \n",
    "        self.matchers[name] = matcher\n",
    "        self.logger.info(f\"Registered matcher '{name}'\")\n",
    "        return True\n",
    "    \n",
    "    def _load_test_data(self, data_path):\n",
    "        \"\"\"\n",
    "        Load test data from file\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to test data file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Loaded test data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Determine file type and load accordingly\n",
    "            if data_path.endswith('.csv'):\n",
    "                data = pd.read_csv(data_path)\n",
    "            elif data_path.endswith('.xlsx'):\n",
    "                data = pd.read_excel(data_path)\n",
    "            elif data_path.endswith('.json'):\n",
    "                data = pd.read_json(data_path)\n",
    "            else:\n",
    "                self.logger.error(f\"Unsupported file format: {data_path}\")\n",
    "                return None\n",
    "                \n",
    "            # Validate required columns\n",
    "            required_cols = ['s1', 's2', 'is_match']\n",
    "            if not all(col in data.columns for col in required_cols):\n",
    "                self.logger.error(f\"Test data missing required columns: {required_cols}\")\n",
    "                return None\n",
    "                \n",
    "            self.logger.info(f\"Loaded test data from {data_path} with {len(data)} examples\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading test data: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_test_suite(self, source_data, sample_size=1000, match_ratio=0.5,\n",
    "                            include_edge_cases=True, stratify_by_domain=True):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive test suite from source data\n",
    "        \n",
    "        Args:\n",
    "            source_data (DataFrame): Source data with merchant names\n",
    "            sample_size (int): Size of generated test suite\n",
    "            match_ratio (float): Ratio of matching pairs to non-matching pairs\n",
    "            include_edge_cases (bool): Whether to include challenging edge cases\n",
    "            stratify_by_domain (bool): Whether to stratify sampling by domain\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Generated test suite\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Generating test suite with {sample_size} examples\")\n",
    "        \n",
    "        try:\n",
    "            # Check for required columns\n",
    "            if 'merchant_name' not in source_data.columns:\n",
    "                self.logger.error(\"Source data missing required 'merchant_name' column\")\n",
    "                return None\n",
    "                \n",
    "            # Create positive (matching) pairs\n",
    "            pos_pairs = []\n",
    "            \n",
    "            # Group by domain if stratification requested\n",
    "            if stratify_by_domain and 'domain' in source_data.columns:\n",
    "                domains = source_data['domain'].unique()\n",
    "                pos_per_domain = int((sample_size * match_ratio) / len(domains))\n",
    "                \n",
    "                for domain in domains:\n",
    "                    domain_merchants = source_data[source_data['domain'] == domain]['merchant_name']\n",
    "                    domain_pairs = self._generate_positive_pairs(domain_merchants, pos_per_domain)\n",
    "                    \n",
    "                    for pair in domain_pairs:\n",
    "                        pos_pairs.append((*pair, 1, domain))\n",
    "            else:\n",
    "                # Generate without stratification\n",
    "                merchants = source_data['merchant_name'].unique()\n",
    "                pos_count = int(sample_size * match_ratio)\n",
    "                all_pos_pairs = self._generate_positive_pairs(merchants, pos_count)\n",
    "                \n",
    "                for pair in all_pos_pairs:\n",
    "                    domain = 'general'\n",
    "                    if 'domain' in source_data.columns:\n",
    "                        domains = source_data[source_data['merchant_name'].isin(pair)]['domain'].unique()\n",
    "                        if len(domains) > 0:\n",
    "                            domain = domains[0]\n",
    "                    \n",
    "                    pos_pairs.append((*pair, 1, domain))\n",
    "            \n",
    "            # Create negative (non-matching) pairs\n",
    "            neg_pairs = []\n",
    "            neg_count = sample_size - len(pos_pairs)\n",
    "            \n",
    "            if stratify_by_domain and 'domain' in source_data.columns:\n",
    "                domains = source_data['domain'].unique()\n",
    "                neg_per_domain = int(neg_count / len(domains))\n",
    "                \n",
    "                for domain in domains:\n",
    "                    domain_merchants = source_data[source_data['domain'] == domain]['merchant_name']\n",
    "                    domain_pairs = self._generate_negative_pairs(domain_merchants, neg_per_domain)\n",
    "                    \n",
    "                    for pair in domain_pairs:\n",
    "                        neg_pairs.append((*pair, 0, domain))\n",
    "            else:\n",
    "                # Generate without stratification\n",
    "                merchants = source_data['merchant_name'].unique()\n",
    "                all_neg_pairs = self._generate_negative_pairs(merchants, neg_count)\n",
    "                \n",
    "                for pair in all_neg_pairs:\n",
    "                    domain = 'general'\n",
    "                    if 'domain' in source_data.columns:\n",
    "                        domains = source_data[source_data['merchant_name'] == pair[0]]['domain'].unique()\n",
    "                        if len(domains) > 0:\n",
    "                            domain = domains[0]\n",
    "                    \n",
    "                    neg_pairs.append((*pair, 0, domain))\n",
    "            \n",
    "            # Combine positive and negative pairs\n",
    "            all_pairs = pos_pairs + neg_pairs\n",
    "            \n",
    "            # Add edge cases if requested\n",
    "            if include_edge_cases:\n",
    "                edge_cases = self._generate_edge_cases(source_data)\n",
    "                all_pairs.extend(edge_cases)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            test_suite = pd.DataFrame(all_pairs, columns=['s1', 's2', 'is_match', 'domain'])\n",
    "            \n",
    "            # Shuffle the data\n",
    "            test_suite = test_suite.sample(frac=1, random_state=self.random_state).reset_index(drop=True)\n",
    "            \n",
    "            self.test_set = test_suite\n",
    "            self.logger.info(f\"Generated test suite with {len(test_suite)} examples\")\n",
    "            \n",
    "            return test_suite\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating test suite: {str(e)}\")\n",
    "            import traceback\n",
    "            self.logger.debug(traceback.format_exc())\n",
    "            return None\n",
    "    \n",
    "    def _generate_positive_pairs(self, merchants, count):\n",
    "        \"\"\"Generate matching pairs through systematic variation\"\"\"\n",
    "        import random\n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        pairs = []\n",
    "        merchants = list(merchants)\n",
    "        \n",
    "        if len(merchants) < 10:\n",
    "            self.logger.warning(\"Too few merchants to generate diverse positive pairs\")\n",
    "            return pairs\n",
    "            \n",
    "        # Generate pairs through various transformations\n",
    "        while len(pairs) < count:\n",
    "            if not merchants:\n",
    "                break\n",
    "                \n",
    "            # Select a random merchant\n",
    "            merchant = random.choice(merchants)\n",
    "            \n",
    "            # Skip very short names\n",
    "            if len(merchant) < 3:\n",
    "                continue\n",
    "                \n",
    "            # Apply transformations to create matching variants\n",
    "            variant = None\n",
    "            transform_type = random.randint(1, 6)\n",
    "            \n",
    "            if transform_type == 1:\n",
    "                # Change business suffix\n",
    "                suffixes = [' Inc', ' LLC', ' Corp', ' Co', ' Ltd']\n",
    "                if any(merchant.endswith(suffix) for suffix in suffixes):\n",
    "                    # Remove suffix\n",
    "                    for suffix in suffixes:\n",
    "                        if merchant.endswith(suffix):\n",
    "                            variant = merchant[:-len(suffix)]\n",
    "                            break\n",
    "                else:\n",
    "                    # Add suffix\n",
    "                    variant = merchant + random.choice(suffixes)\n",
    "                    \n",
    "            elif transform_type == 2:\n",
    "                # Add/remove spaces or hyphens\n",
    "                if ' ' in merchant:\n",
    "                    variant = merchant.replace(' ', '')\n",
    "                elif '-' in merchant:\n",
    "                    variant = merchant.replace('-', ' ')\n",
    "                else:\n",
    "                    # Insert spaces between words (heuristic)\n",
    "                    words = []\n",
    "                    current_word = \"\"\n",
    "                    for c in merchant:\n",
    "                        if c.isupper() and current_word and not current_word[-1].isupper():\n",
    "                            words.append(current_word)\n",
    "                            current_word = c\n",
    "                        else:\n",
    "                            current_word += c\n",
    "                    if current_word:\n",
    "                        words.append(current_word)\n",
    "                    variant = ' '.join(words)\n",
    "                    \n",
    "            elif transform_type == 3:\n",
    "                # Abbreviate or expand\n",
    "                words = merchant.split()\n",
    "                if len(words) >= 3:\n",
    "                    # Create acronym\n",
    "                    variant = ''.join(word[0].upper() for word in words)\n",
    "                elif len(words) == 1 and len(merchant) <= 3:\n",
    "                    # Expand common acronyms\n",
    "                    expansions = {\n",
    "                        'BOA': 'Bank of America',\n",
    "                        'WF': 'Wells Fargo',\n",
    "                        'JPM': 'JPMorgan Chase',\n",
    "                        'GS': 'Goldman Sachs',\n",
    "                        'MS': 'Morgan Stanley',\n",
    "                        'WM': 'Walmart',\n",
    "                        'TGT': 'Target',\n",
    "                        'HD': 'Home Depot',\n",
    "                        'MCD': 'McDonalds'\n",
    "                    }\n",
    "                    variant = expansions.get(merchant.upper())\n",
    "                    \n",
    "            elif transform_type == 4:\n",
    "                # Change case\n",
    "                if merchant.isupper():\n",
    "                    variant = merchant.lower()\n",
    "                elif merchant.islower():\n",
    "                    variant = merchant.upper()\n",
    "                else:\n",
    "                    variant = merchant.upper()\n",
    "                    \n",
    "            elif transform_type == 5:\n",
    "                # Add/remove \"The\" prefix\n",
    "                if merchant.startswith('The '):\n",
    "                    variant = merchant[4:]\n",
    "                else:\n",
    "                    variant = 'The ' + merchant\n",
    "                    \n",
    "            elif transform_type == 6:\n",
    "                # Introduce minor misspellings\n",
    "                variant = merchant\n",
    "                if len(merchant) > 5:\n",
    "                    pos = random.randint(1, len(merchant)-2)\n",
    "                    chars = list(merchant)\n",
    "                    # Either swap adjacent chars or replace with a similar char\n",
    "                    if random.random() < 0.5:\n",
    "                        chars[pos], chars[pos+1] = chars[pos+1], chars[pos]\n",
    "                    else:\n",
    "                        similar_chars = {\n",
    "                            'a': 'e', 'e': 'a', 'i': 'y', 'o': 'u', 'u': 'o',\n",
    "                            's': 'z', 'c': 'k', 'm': 'n', 'b': 'p', 'g': 'j'\n",
    "                        }\n",
    "                        if chars[pos].lower() in similar_chars:\n",
    "                            chars[pos] = similar_chars[chars[pos].lower()]\n",
    "                    variant = ''.join(chars)\n",
    "            \n",
    "            # Add pair if valid variant was created\n",
    "            if variant and variant != merchant and (merchant, variant) not in pairs and (variant, merchant) not in pairs:\n",
    "                pairs.append((merchant, variant))\n",
    "                \n",
    "            # Break if we've tried too many merchants without success\n",
    "            if len(pairs) < count and len(pairs) < len(merchants) // 2:\n",
    "                merchants.remove(merchant)\n",
    "        \n",
    "        return pairs[:count]\n",
    "    \n",
    "    def _generate_negative_pairs(self, merchants, count):\n",
    "        \"\"\"Generate non-matching pairs with varying similarity levels\"\"\"\n",
    "        import random\n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        pairs = []\n",
    "        merchants = list(merchants)\n",
    "        \n",
    "        if len(merchants) < 10:\n",
    "            self.logger.warning(\"Too few merchants to generate diverse negative pairs\")\n",
    "            return pairs\n",
    "            \n",
    "        # Generate random pairs with different similarity levels\n",
    "        while len(pairs) < count:\n",
    "            if len(merchants) < 2:\n",
    "                break\n",
    "                \n",
    "            # Select two different merchants\n",
    "            m1 = random.choice(merchants)\n",
    "            remaining = [m for m in merchants if m != m1]\n",
    "            if not remaining:\n",
    "                continue\n",
    "                \n",
    "            m2 = random.choice(remaining)\n",
    "            \n",
    "            # Skip very short names\n",
    "            if len(m1) < 3 or len(m2) < 3:\n",
    "                continue\n",
    "                \n",
    "            # Check if pair already exists\n",
    "            if (m1, m2) in pairs or (m2, m1) in pairs:\n",
    "                continue\n",
    "                \n",
    "            # Add the pair\n",
    "            pairs.append((m1, m2))\n",
    "        \n",
    "        return pairs[:count]\n",
    "    \n",
    "    def _generate_edge_cases(self, source_data):\n",
    "        \"\"\"Generate challenging edge cases for testing\"\"\"\n",
    "        edge_cases = []\n",
    "        \n",
    "        # Example edge cases (these would be more sophisticated in a real implementation)\n",
    "        # 1. Acronyms with same letters but different order\n",
    "        # 2. Names with same words but different order\n",
    "        # 3. Names with high character overlap but different meanings\n",
    "        # 4. Very similar names but semantically different merchants\n",
    "        \n",
    "        # For this implementation, we'll just add a few hand-crafted examples\n",
    "        banking_cases = [\n",
    "            # Positive cases (is_match=1)\n",
    "            ('Bank of America', 'Bank of America, N.A.', 1, 'banking'),\n",
    "            ('Bank of America', 'BoA', 1, 'banking'),\n",
    "            ('JPMorgan Chase', 'J.P. Morgan', 1, 'banking'),\n",
    "            # Negative cases (is_match=0)\n",
    "            ('Bank of America', 'Bank of American Express', 0, 'banking'),\n",
    "            ('First National Bank', 'First National Bancorp', 0, 'banking'),\n",
    "        ]\n",
    "        \n",
    "        retail_cases = [\n",
    "            # Positive cases\n",
    "            ('Walmart', 'Wal-Mart Supercenter', 1, 'retail'),\n",
    "            ('Target', 'Target Corp', 1, 'retail'),\n",
    "            ('Best Buy', 'BestBuy', 1, 'retail'),\n",
    "            # Negative cases\n",
    "            ('Ross Dress for Less', 'Ross Stores', 0, 'retail'),\n",
    "            ('TJ Maxx', 'T.J. Max', 0, 'retail'),\n",
    "        ]\n",
    "        \n",
    "        restaurant_cases = [\n",
    "            # Positive cases\n",
    "            ('McDonald\\'s', 'McDonalds', 1, 'restaurant'),\n",
    "            ('Starbucks Coffee', 'SBUX', 1, 'restaurant'),\n",
    "            ('Chipotle Mexican Grill', 'Chipotle', 1, 'restaurant'),\n",
    "            # Negative cases\n",
    "            ('Panda Express', 'Panda Inn', 0, 'restaurant'),\n",
    "            ('Five Guys', 'Five Guys Burgers', 0, 'restaurant'),\n",
    "        ]\n",
    "        \n",
    "        edge_cases.extend(banking_cases + retail_cases + restaurant_cases)\n",
    "        return edge_cases\n",
    "    \n",
    "    def evaluate_all(self, use_cross_val=True):\n",
    "        \"\"\"\n",
    "        Evaluate all registered matchers on the test set\n",
    "        \n",
    "        Args:\n",
    "            use_cross_val (bool): Whether to use cross-validation\n",
    "            \n",
    "        Returns:\n",
    "            dict: Evaluation results for all matchers\n",
    "        \"\"\"\n",
    "        if not self.test_set is not None:\n",
    "            self.logger.error(\"No test set available for evaluation\")\n",
    "            return None\n",
    "            \n",
    "        if not self.matchers:\n",
    "            self.logger.error(\"No matchers registered for evaluation\")\n",
    "            return None\n",
    "            \n",
    "        self.logger.info(f\"Evaluating {len(self.matchers)} matchers on {len(self.test_set)} examples\")\n",
    "        \n",
    "        # Initialize results storage\n",
    "        results = {}\n",
    "        \n",
    "        # Evaluate each matcher\n",
    "        for name, matcher in self.matchers.items():\n",
    "            self.logger.info(f\"Evaluating matcher: {name}\")\n",
    "            \n",
    "            # Standard evaluation\n",
    "            matcher_results = self.evaluate_matcher(matcher, name)\n",
    "            results[name] = matcher_results\n",
    "            \n",
    "            # Cross-validation if requested\n",
    "            if use_cross_val:\n",
    "                cv_results = self.cross_validate(matcher, name)\n",
    "                results[name]['cross_validation'] = cv_results\n",
    "        \n",
    "        # Store results\n",
    "        self.results = results\n",
    "        \n",
    "        # Generate comparative analysis\n",
    "        comparison = self._compare_matchers(results)\n",
    "        results['comparison'] = comparison\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_matcher(self, matcher, matcher_name):\n",
    "        \"\"\"\n",
    "        Evaluate a single matcher on the test set\n",
    "        \n",
    "        Args:\n",
    "            matcher: Matcher instance to evaluate\n",
    "            matcher_name (str): Name identifier for the matcher\n",
    "            \n",
    "        Returns:\n",
    "            dict: Evaluation results\n",
    "        \"\"\"\n",
    "        if self.test_set is None:\n",
    "            self.logger.error(\"No test set available for evaluation\")\n",
    "            return None\n",
    "            \n",
    "        # Extract test pairs\n",
    "        s1_values = self.test_set['s1'].values\n",
    "        s2_values = self.test_set['s2'].values\n",
    "        y_true = self.test_set['is_match'].values\n",
    "        domains = self.test_set['domain'].values if 'domain' in self.test_set.columns else None\n",
    "        \n",
    "        # Get match scores\n",
    "        self.logger.info(f\"Processing {len(s1_values)} test pairs for {matcher_name}\")\n",
    "        \n",
    "        y_scores = []\n",
    "        y_pred = []\n",
    "        \n",
    "        # Process in batches for large test sets\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(s1_values), batch_size):\n",
    "            batch_end = min(i + batch_size, len(s1_values))\n",
    "            self.logger.info(f\"Processing batch {i//batch_size + 1}/{(len(s1_values)-1)//batch_size + 1}\")\n",
    "            \n",
    "            batch_scores = []\n",
    "            for j in range(i, batch_end):\n",
    "                s1 = s1_values[j]\n",
    "                s2 = s2_values[j]\n",
    "                domain = domains[j] if domains is not None else None\n",
    "                \n",
    "                try:\n",
    "                    # Get match score\n",
    "                    if hasattr(matcher, 'match_merchants'):\n",
    "                        score = matcher.match_merchants(s1, s2, domain)\n",
    "                    else:\n",
    "                        # Fallback for simplistic matchers\n",
    "                        score = matcher(s1, s2)\n",
    "                        \n",
    "                    batch_scores.append(float(score))\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error matching '{s1}' and '{s2}': {e}\")\n",
    "                    batch_scores.append(0.0)\n",
    "            \n",
    "            y_scores.extend(batch_scores)\n",
    "        \n",
    "        # Convert scores to array\n",
    "        y_scores = np.array(y_scores)\n",
    "        \n",
    "        # Calculate optimal threshold using Youden's J statistic\n",
    "        from sklearn.metrics import roc_curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \n",
    "        # Generate predictions based on optimal threshold\n",
    "        y_pred = (y_scores >= optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self._calculate_metrics(y_true, y_pred, y_scores)\n",
    "        metrics['optimal_threshold'] = optimal_threshold\n",
    "        \n",
    "        # Calculate domain-specific performance if domains available\n",
    "        domain_performance = {}\n",
    "        if domains is not None:\n",
    "            unique_domains = np.unique(domains)\n",
    "            \n",
    "            for domain in unique_domains:\n",
    "                domain_mask = domains == domain\n",
    "                if sum(domain_mask) < 10:  # Skip domains with too few samples\n",
    "                    continue\n",
    "                    \n",
    "                domain_y_true = y_true[domain_mask]\n",
    "                domain_y_pred = y_pred[domain_mask]\n",
    "                domain_y_scores = y_scores[domain_mask]\n",
    "                \n",
    "                domain_metrics = self._calculate_metrics(domain_y_true, domain_y_pred, domain_y_scores)\n",
    "                domain_performance[domain] = domain_metrics\n",
    "        \n",
    "        # Identify error patterns\n",
    "        error_analysis = self._analyze_errors(\n",
    "            s1_values, s2_values, y_true, y_pred, y_scores, domains\n",
    "        )\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'matcher_name': matcher_name,\n",
    "            'overall_metrics': metrics,\n",
    "            'domain_performance': domain_performance,\n",
    "            'error_analysis': error_analysis,\n",
    "            'test_size': len(y_true),\n",
    "            'positive_examples': int(sum(y_true)),\n",
    "            'negative_examples': int(len(y_true) - sum(y_true)),\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        # Store in instance results\n",
    "        self.results[matcher_name] = results\n",
    "        self.domain_results[matcher_name] = domain_performance\n",
    "        self.error_analysis[matcher_name] = error_analysis\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred, y_scores):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        from sklearn.metrics import (\n",
    "            precision_score, recall_score, f1_score, accuracy_score, \n",
    "            roc_auc_score, confusion_matrix, precision_recall_curve\n",
    "        )\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Basic classification metrics\n",
    "        metrics['precision'] = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "        metrics['recall'] = float(recall_score(y_true, y_pred, zero_division=0))\n",
    "        metrics['f1_score'] = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "        metrics['accuracy'] = float(accuracy_score(y_true, y_pred))\n",
    "        \n",
    "        # ROC and AUC\n",
    "        try:\n",
    "            metrics['roc_auc'] = float(roc_auc_score(y_true, y_scores))\n",
    "        except:\n",
    "            metrics['roc_auc'] = 0.5  # Default for failed calculation\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        metrics['confusion_matrix'] = cm.tolist()\n",
    "        \n",
    "        # True/False Positives/Negatives\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        metrics['true_positives'] = int(tp)\n",
    "        metrics['false_positives'] = int(fp)\n",
    "        metrics['true_negatives'] = int(tn)\n",
    "        metrics['false_negatives'] = int(fn)\n",
    "        \n",
    "        # Precision-Recall curve\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_scores)\n",
    "        # Store selected points from the curve (100 points max)\n",
    "        step = max(1, len(precision) // 100)\n",
    "        metrics['pr_curve'] = {\n",
    "            'precision': precision[::step].tolist(),\n",
    "            'recall': recall[::step].tolist()\n",
    "        }\n",
    "        \n",
    "        # Additional metrics\n",
    "        if tp + fp > 0:\n",
    "            metrics['precision_at_k'] = float(tp / (tp + fp))\n",
    "        else:\n",
    "            metrics['precision_at_k'] = 0.0\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def _analyze_errors(self, s1_values, s2_values, y_true, y_pred, y_scores, domains=None):\n",
    "        \"\"\"\n",
    "        Analyze error patterns in matching results\n",
    "        \n",
    "        Args:\n",
    "            s1_values: Array of first merchant names\n",
    "            s2_values: Array of second merchant names\n",
    "            y_true: Array of true labels\n",
    "            y_pred: Array of predicted labels\n",
    "            y_scores: Array of match scores\n",
    "            domains: Array of domains (optional)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Error analysis results\n",
    "        \"\"\"\n",
    "        # Find incorrect predictions\n",
    "        errors = (y_true != y_pred)\n",
    "        error_indices = np.where(errors)[0]\n",
    "        \n",
    "        if len(error_indices) == 0:\n",
    "            return {'total_errors': 0, 'error_examples': []}\n",
    "            \n",
    "        # Collect error examples\n",
    "        error_examples = []\n",
    "        for idx in error_indices[:min(100, len(error_indices))]:  # Limit to 100 examples\n",
    "            s1 = s1_values[idx]\n",
    "            s2 = s2_values[idx]\n",
    "            true_label = int(y_true[idx])\n",
    "            pred_label = int(y_pred[idx])\n",
    "            score = float(y_scores[idx])\n",
    "            domain = domains[idx] if domains is not None else 'unknown'\n",
    "            \n",
    "            error_type = 'false_positive' if pred_label == 1 and true_label == 0 else 'false_negative'\n",
    "            \n",
    "            error_examples.append({\n",
    "                'index': int(idx),\n",
    "                's1': s1,\n",
    "                's2': s2,\n",
    "                'true_label': true_label,\n",
    "                'pred_label': pred_label,\n",
    "                'score': score,\n",
    "                'domain': domain,\n",
    "                'error_type': error_type,\n",
    "                'error_subtype': self._classify_error(s1, s2, true_label, pred_label)\n",
    "            })\n",
    "        \n",
    "        # Classify errors by type\n",
    "        error_counts = {\n",
    "            'false_positives': int(sum((y_pred == 1) & (y_true == 0))),\n",
    "            'false_negatives': int(sum((y_pred == 0) & (y_true == 1)))\n",
    "        }\n",
    "        \n",
    "        # Classify errors by domain if available\n",
    "        domain_errors = {}\n",
    "        if domains is not None:\n",
    "            unique_domains = np.unique(domains)\n",
    "            for domain in unique_domains:\n",
    "                domain_mask = domains == domain\n",
    "                if sum(domain_mask) < 5:  # Skip domains with too few samples\n",
    "                    continue\n",
    "                    \n",
    "                domain_errors[domain] = {\n",
    "                    'total': int(sum(errors & domain_mask)),\n",
    "                    'false_positives': int(sum((y_pred == 1) & (y_true == 0) & domain_mask)),\n",
    "                    'false_negatives': int(sum((y_pred == 0) & (y_true == 1) & domain_mask))\n",
    "                }\n",
    "        \n",
    "        # Classify error subtypes\n",
    "        error_subtypes = {}\n",
    "        for example in error_examples:\n",
    "            subtype = example['error_subtype']\n",
    "            if subtype not in error_subtypes:\n",
    "                error_subtypes[subtype] = 0\n",
    "            error_subtypes[subtype] += 1\n",
    "        \n",
    "        return {\n",
    "            'total_errors': int(sum(errors)),\n",
    "            'error_rate': float(sum(errors) / len(y_true)),\n",
    "            'error_counts': error_counts,\n",
    "            'domain_errors': domain_errors,\n",
    "            'error_subtypes': error_subtypes,\n",
    "            'error_examples': error_examples\n",
    "        }\n",
    "    \n",
    "    def _classify_error(self, s1, s2, true_label, pred_label):\n",
    "        \"\"\"Classify the error type based on string analysis\"\"\"\n",
    "        # Initialize potential error types\n",
    "        error_types = []\n",
    "        \n",
    "        # Check for acronym pattern\n",
    "        if (len(s1) <= 5 and s1.isupper() and len(s2) > 10) or \\\n",
    "           (len(s2) <= 5 and s2.isupper() and len(s1) > 10):\n",
    "            error_types.append('acronym_error')\n",
    "        \n",
    "        # Check for spelling differences\n",
    "        if len(s1) > 3 and len(s2) > 3:\n",
    "            if self._levenshtein_distance(s1.lower(), s2.lower()) <= 2:\n",
    "                error_types.append('spelling_error')\n",
    "        \n",
    "        # Check for word order differences\n",
    "        s1_words = set(s1.lower().split())\n",
    "        s2_words = set(s2.lower().split())\n",
    "        if s1_words == s2_words and s1.lower() != s2.lower():\n",
    "            error_types.append('structural_error')\n",
    "        \n",
    "        # Check for partial containment\n",
    "        if s1.lower() in s2.lower() or s2.lower() in s1.lower():\n",
    "            error_types.append('partial_match_error')\n",
    "        \n",
    "        # Default error types based on prediction type\n",
    "        if true_label == 1 and pred_label == 0:\n",
    "            if not error_types:\n",
    "                error_types.append('semantic_error')  # Failed to recognize semantic relationship\n",
    "        else:  # true_label == 0 and pred_label == 1\n",
    "            if not error_types:\n",
    "                error_types.append('false_similar_error')  # Incorrectly identified as similar\n",
    "        \n",
    "        # Return primary error type\n",
    "        return error_types[0] if error_types else 'unknown_error'\n",
    "    \n",
    "    def _levenshtein_distance(self, s1, s2):\n",
    "        \"\"\"Calculate Levenshtein distance between two strings\"\"\"\n",
    "        if len(s1) < len(s2):\n",
    "            return self._levenshtein_distance(s2, s1)\n",
    "            \n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "            \n",
    "        previous_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "            \n",
    "        return previous_row[-1]\n",
    "    \n",
    "    def cross_validate(self, matcher, matcher_name, n_folds=None):\n",
    "        \"\"\"\n",
    "        Perform cross-validation on the matcher\n",
    "        \n",
    "        Args:\n",
    "            matcher: Matcher instance to evaluate\n",
    "            matcher_name (str): Name identifier for the matcher\n",
    "            n_folds (int, optional): Number of folds (defaults to self.k_folds)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Cross-validation results\n",
    "        \"\"\"\n",
    "        if self.test_set is None:\n",
    "            self.logger.error(\"No test set available for cross-validation\")\n",
    "            return None\n",
    "            \n",
    "        # Use default folds if not specified\n",
    "        n_folds = n_folds or self.k_folds\n",
    "        \n",
    "        self.logger.info(f\"Performing {n_folds}-fold cross-validation for {matcher_name}\")\n",
    "        \n",
    "        # Initialize result storage\n",
    "        cv_results = {\n",
    "            'folds': [],\n",
    "            'avg_metrics': {},\n",
    "            'std_metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Prepare data\n",
    "        data = self.test_set.copy()\n",
    "        \n",
    "        # Create fold indices\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=self.random_state)\n",
    "        fold_indices = list(skf.split(data, data['is_match']))\n",
    "        \n",
    "        # Evaluate each fold\n",
    "        all_metrics = []\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(fold_indices):\n",
    "            self.logger.info(f\"Processing fold {fold_idx+1}/{n_folds}\")\n",
    "            \n",
    "            # Split data\n",
    "            train_data = data.iloc[train_idx]\n",
    "            test_data = data.iloc[test_idx]\n",
    "            \n",
    "            # Train matcher if it supports training\n",
    "            if hasattr(matcher, 'train') and callable(getattr(matcher, 'train')):\n",
    "                try:\n",
    "                    matcher.train(train_data)\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error training matcher on fold {fold_idx+1}: {e}\")\n",
    "            \n",
    "            # Evaluate on test split\n",
    "            s1_values = test_data['s1'].values\n",
    "            s2_values = test_data['s2'].values\n",
    "            y_true = test_data['is_match'].values\n",
    "            \n",
    "            y_scores = []\n",
    "            for s1, s2 in zip(s1_values, s2_values):\n",
    "                try:\n",
    "                    score = matcher.match_merchants(s1, s2)\n",
    "                    y_scores.append(float(score))\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error matching '{s1}' and '{s2}': {e}\")\n",
    "                    y_scores.append(0.0)\n",
    "            \n",
    "            y_scores = np.array(y_scores)\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            from sklearn.metrics import roc_curve\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "            optimal_idx = np.argmax(tpr - fpr)\n",
    "            optimal_threshold = thresholds[optimal_idx]\n",
    "            \n",
    "            # Generate predictions\n",
    "            y_pred = (y_scores >= optimal_threshold).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            fold_metrics = self._calculate_metrics(y_true, y_pred, y_scores)\n",
    "            fold_metrics['optimal_threshold'] = optimal_threshold\n",
    "            \n",
    "            # Store fold results\n",
    "            cv_results['folds'].append({\n",
    "                'fold_idx': fold_idx,\n",
    "                'metrics': fold_metrics,\n",
    "                'train_size': len(train_data),\n",
    "                'test_size': len(test_data)\n",
    "            })\n",
    "            \n",
    "            all_metrics.append(fold_metrics)\n",
    "        \n",
    "        # Calculate aggregate statistics\n",
    "        metric_names = ['precision', 'recall', 'f1_score', 'accuracy', 'roc_auc']\n",
    "        for metric in metric_names:\n",
    "            values = [fold['metrics'][metric] for fold in cv_results['folds']]\n",
    "            cv_results['avg_metrics'][metric] = float(np.mean(values))\n",
    "            cv_results['std_metrics'][metric] = float(np.std(values))\n",
    "        \n",
    "        # Store cross-validation results\n",
    "        self.cross_val_results[matcher_name] = cv_results\n",
    "        \n",
    "        return cv_results\n",
    "    \n",
    "    def _compare_matchers(self, results):\n",
    "        \"\"\"Generate comparative analysis of matcher performance\"\"\"\n",
    "        if not results:\n",
    "            return {}\n",
    "            \n",
    "        comparison = {\n",
    "            'overall_ranking': {},\n",
    "            'domain_ranking': {},\n",
    "            'metric_comparison': {},\n",
    "            'error_comparison': {},\n",
    "            'statistical_significance': {}\n",
    "        }\n",
    "        \n",
    "        # Extract matcher names\n",
    "        matcher_names = list(results.keys())\n",
    "        \n",
    "        # Compare overall metrics\n",
    "        metric_names = ['precision', 'recall', 'f1_score', 'accuracy', 'roc_auc']\n",
    "        for metric in metric_names:\n",
    "            comparison['metric_comparison'][metric] = {}\n",
    "            \n",
    "            # Get metric values for all matchers\n",
    "            values = {}\n",
    "            for name in matcher_names:\n",
    "                if name in results and 'overall_metrics' in results[name]:\n",
    "                    values[name] = results[name]['overall_metrics'].get(metric, 0)\n",
    "            \n",
    "            # Rank matchers by the metric\n",
    "            ranked_matchers = sorted(values.items(), key=lambda x: x[1], reverse=True)\n",
    "            comparison['metric_comparison'][metric]['values'] = values\n",
    "            comparison['metric_comparison'][metric]['ranking'] = [name for name, _ in ranked_matchers]\n",
    "        \n",
    "        # Generate overall ranking based on F1 score\n",
    "        f1_scores = {}\n",
    "        for name in matcher_names:\n",
    "            if name in results and 'overall_metrics' in results[name]:\n",
    "                f1_scores[name] = results[name]['overall_metrics'].get('f1_score', 0)\n",
    "                \n",
    "        overall_ranking = sorted(f1_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        comparison['overall_ranking'] = {\n",
    "            'metric': 'f1_score',\n",
    "            'ranking': [name for name, _ in overall_ranking],\n",
    "            'scores': f1_scores\n",
    "        }\n",
    "        \n",
    "        # Compare domain performance if available\n",
    "        domain_data = {}\n",
    "        for name in matcher_names:\n",
    "            if name in results and 'domain_performance' in results[name]:\n",
    "                for domain, metrics in results[name]['domain_performance'].items():\n",
    "                    if domain not in domain_data:\n",
    "                        domain_data[domain] = {}\n",
    "                    domain_data[domain][name] = metrics.get('f1_score', 0)\n",
    "        \n",
    "        # Rank by domain\n",
    "        for domain, scores in domain_data.items():\n",
    "            ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            comparison['domain_ranking'][domain] = {\n",
    "                'ranking': [name for name, _ in ranked],\n",
    "                'scores': scores\n",
    "            }\n",
    "        \n",
    "        # Compare error patterns\n",
    "        error_data = {}\n",
    "        for name in matcher_names:\n",
    "            if name in results and 'error_analysis' in results[name]:\n",
    "                error_analysis = results[name]['error_analysis']\n",
    "                error_data[name] = {\n",
    "                    'total_errors': error_analysis.get('total_errors', 0),\n",
    "                    'error_rate': error_analysis.get('error_rate', 0),\n",
    "                    'false_positives': error_analysis.get('error_counts', {}).get('false_positives', 0),\n",
    "                    'false_negatives': error_analysis.get('error_counts', {}).get('false_negatives', 0)\n",
    "                }\n",
    "        \n",
    "        comparison['error_comparison'] = error_data\n",
    "        \n",
    "        # Perform statistical significance testing (McNemar's test)\n",
    "        if len(matcher_names) > 1:\n",
    "            try:\n",
    "                from statsmodels.stats.contingency_tables import mcnemar\n",
    "                \n",
    "                # Get prediction arrays for each matcher\n",
    "                predictions = {}\n",
    "                for name in matcher_names:\n",
    "                    if name in self.results:\n",
    "                        # Reconstruct predictions from error analysis\n",
    "                        y_true = self.test_set['is_match'].values\n",
    "                        error_indices = [ex['index'] for ex in self.results[name]['error_analysis'].get('error_examples', [])]\n",
    "                        y_pred = np.array(y_true)  # Start with correct predictions\n",
    "                        y_pred[error_indices] = 1 - y_true[error_indices]  # Flip predictions at error indices\n",
    "                        predictions[name] = y_pred\n",
    "                \n",
    "                # Perform pairwise McNemar tests\n",
    "                significance_results = {}\n",
    "                for i, name1 in enumerate(matcher_names):\n",
    "                    for j, name2 in enumerate(matcher_names):\n",
    "                        if i < j and name1 in predictions and name2 in predictions:\n",
    "                            # Create contingency table\n",
    "                            pred1 = predictions[name1]\n",
    "                            pred2 = predictions[name2]\n",
    "                            \n",
    "                            # Count contingency table cells\n",
    "                            both_correct = sum((pred1 == self.test_set['is_match']) & (pred2 == self.test_set['is_match']))\n",
    "                            name1_correct = sum((pred1 == self.test_set['is_match']) & (pred2 != self.test_set['is_match']))\n",
    "                            name2_correct = sum((pred1 != self.test_set['is_match']) & (pred2 == self.test_set['is_match']))\n",
    "                            both_wrong = sum((pred1 != self.test_set['is_match']) & (pred2 != self.test_set['is_match']))\n",
    "                            \n",
    "                            table = np.array([[both_correct, name2_correct], \n",
    "                                             [name1_correct, both_wrong]])\n",
    "                            \n",
    "                            # Perform McNemar test\n",
    "                            result = mcnemar(table, exact=True)\n",
    "                            p_value = result.pvalue\n",
    "                            \n",
    "                            pair_key = f\"{name1}_vs_{name2}\"\n",
    "                            significance_results[pair_key] = {\n",
    "                                'p_value': float(p_value),\n",
    "                                'significant': p_value < 0.05,\n",
    "                                'better_matcher': name1 if name1_correct > name2_correct else name2\n",
    "                            }\n",
    "                \n",
    "                comparison['statistical_significance'] = significance_results\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error performing statistical significance tests: {e}\")\n",
    "                comparison['statistical_significance'] = {'error': str(e)}\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def generate_report(self, output_path=None, format='html'):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive evaluation report\n",
    "        \n",
    "        Args:\n",
    "            output_path (str, optional): Path to save the report\n",
    "            format (str): Report format ('html', 'json', 'pdf')\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to the generated report or HTML string\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            self.logger.error(\"No evaluation results available for reporting\")\n",
    "            return None\n",
    "            \n",
    "        self.logger.info(f\"Generating {format} evaluation report\")\n",
    "        \n",
    "        # Generate report content based on format\n",
    "        if format == 'html':\n",
    "            report_content = self._generate_html_report()\n",
    "        elif format == 'json':\n",
    "            import json\n",
    "            report_content = json.dumps(self.results, indent=2)\n",
    "        else:\n",
    "            self.logger.error(f\"Unsupported report format: {format}\")\n",
    "            return None\n",
    "        \n",
    "        # Save to file if output path provided\n",
    "        if output_path:\n",
    "            try:\n",
    "                with open(output_path, 'w') as f:\n",
    "                    f.write(report_content)\n",
    "                self.logger.info(f\"Report saved to {output_path}\")\n",
    "                return output_path\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error saving report: {e}\")\n",
    "                return report_content\n",
    "        \n",
    "        return report_content\n",
    "    \n",
    "    def _generate_html_report(self):\n",
    "        \"\"\"Generate an HTML evaluation report\"\"\"\n",
    "        # Basic HTML template\n",
    "        html_template = '''\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Merchant Matching Evaluation Report</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; max-width: 1200px; margin: 0 auto; }\n",
    "                h1, h2, h3 { color: #333; }\n",
    "                table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }\n",
    "                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
    "                th { background-color: #f2f2f2; }\n",
    "                tr:nth-child(even) { background-color: #f9f9f9; }\n",
    "                .chart-container { width: 600px; height: 400px; margin: 20px 0; }\n",
    "                .matcher-section { border: 1px solid #ddd; padding: 15px; margin-bottom: 20px; }\n",
    "                .metric-good { color: green; }\n",
    "                .metric-medium { color: orange; }\n",
    "                .metric-bad { color: red; }\n",
    "                .summary-box { background-color: #f8f8f8; border-left: 4px solid #4CAF50; padding: 10px; margin-bottom: 20px; }\n",
    "            </style>\n",
    "            <!-- Add Chart.js for visualizations -->\n",
    "            <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Merchant Matching Evaluation Report</h1>\n",
    "            <div class=\"summary-box\">\n",
    "                <h2>Executive Summary</h2>\n",
    "                {executive_summary}\n",
    "            </div>\n",
    "            \n",
    "            <h2>Overall Comparison</h2>\n",
    "            {overall_comparison}\n",
    "            \n",
    "            <h2>Detailed Matcher Results</h2>\n",
    "            {matcher_results}\n",
    "            \n",
    "            <h2>Domain-Specific Performance</h2>\n",
    "            {domain_performance}\n",
    "            \n",
    "            <h2>Error Analysis</h2>\n",
    "            {error_analysis}\n",
    "            \n",
    "            <h2>Statistical Significance</h2>\n",
    "            {statistical_significance}\n",
    "            \n",
    "            <footer>\n",
    "                <p>Generated on {generation_date}</p>\n",
    "            </footer>\n",
    "            \n",
    "            <!-- Charts initialization -->\n",
    "            <script>\n",
    "            {chart_scripts}\n",
    "            </script>\n",
    "        </body>\n",
    "        </html>\n",
    "        '''\n",
    "        \n",
    "        # Generate sections\n",
    "        executive_summary = self._generate_executive_summary_html()\n",
    "        overall_comparison = self._generate_overall_comparison_html()\n",
    "        matcher_results = self._generate_matcher_results_html()\n",
    "        domain_performance = self._generate_domain_performance_html()\n",
    "        error_analysis = self._generate_error_analysis_html()\n",
    "        statistical_significance = self._generate_statistical_significance_html()\n",
    "        chart_scripts = self._generate_chart_scripts()\n",
    "        \n",
    "        # Format the template\n",
    "        formatted_html = html_template.format(\n",
    "            executive_summary=executive_summary,\n",
    "            overall_comparison=overall_comparison,\n",
    "            matcher_results=matcher_results,\n",
    "            domain_performance=domain_performance,\n",
    "            error_analysis=error_analysis,\n",
    "            statistical_significance=statistical_significance,\n",
    "            chart_scripts=chart_scripts,\n",
    "            generation_date=time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        )\n",
    "        \n",
    "        return formatted_html\n",
    "    \n",
    "    def _generate_executive_summary_html(self):\n",
    "        \"\"\"Generate HTML executive summary section\"\"\"\n",
    "        # Find the best performing matcher\n",
    "        best_matcher = None\n",
    "        best_f1 = -1\n",
    "        \n",
    "        for name, results in self.results.items():\n",
    "            if 'overall_metrics' in results:\n",
    "                f1 = results['overall_metrics'].get('f1_score', 0)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_matcher = name\n",
    "        \n",
    "        html = f'''\n",
    "        <p>This report evaluates {len(self.results)} merchant name matching algorithms on a test set of \n",
    "        {len(self.test_set) if self.test_set is not None else 0} examples.</p>\n",
    "        \n",
    "        <p>The best performing matcher is <strong>{best_matcher}</strong> with an F1 score of {best_f1:.3f}.</p>\n",
    "        \n",
    "        <p>Key findings:</p>\n",
    "        <ul>\n",
    "        '''\n",
    "        \n",
    "        # Add key findings\n",
    "        if best_matcher and 'overall_metrics' in self.results[best_matcher]:\n",
    "            metrics = self.results[best_matcher]['overall_metrics']\n",
    "            precision = metrics.get('precision', 0)\n",
    "            recall = metrics.get('recall', 0)\n",
    "            \n",
    "            if precision > 0.9:\n",
    "                html += f'<li>{best_matcher} achieved high precision ({precision:.3f}), making it suitable for applications requiring accurate matches.</li>'\n",
    "            if recall > 0.9:\n",
    "                html += f'<li>{best_matcher} achieved high recall ({recall:.3f}), making it effective at finding all potential matches.</li>'\n",
    "        \n",
    "        # Add domain-specific insights\n",
    "        if best_matcher and 'domain_performance' in self.results[best_matcher]:\n",
    "            domain_metrics = self.results[best_matcher]['domain_performance']\n",
    "            \n",
    "            # Find best and worst performing domains\n",
    "            if domain_metrics:\n",
    "                domain_f1 = {domain: metrics.get('f1_score', 0) for domain, metrics in domain_metrics.items()}\n",
    "                best_domain = max(domain_f1.items(), key=lambda x: x[1])\n",
    "                worst_domain = min(domain_f1.items(), key=lambda x: x[1])\n",
    "                \n",
    "                html += f'<li>Best performance in the <strong>{best_domain[0]}</strong> domain (F1: {best_domain[1]:.3f}).</li>'\n",
    "                html += f'<li>Potential improvement needed in the <strong>{worst_domain[0]}</strong> domain (F1: {worst_domain[1]:.3f}).</li>'\n",
    "        \n",
    "        html += '</ul>'\n",
    "        return html\n",
    "    \n",
    "    def _generate_overall_comparison_html(self):\n",
    "        \"\"\"Generate HTML overall comparison section\"\"\"\n",
    "        # Extract overall metrics for all matchers\n",
    "        html = '''\n",
    "        <div class=\"chart-container\">\n",
    "            <canvas id=\"overallChart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Matcher</th>\n",
    "                <th>Precision</th>\n",
    "                <th>Recall</th>\n",
    "                <th>F1 Score</th>\n",
    "                <th>Accuracy</th>\n",
    "                <th>ROC AUC</th>\n",
    "            </tr>\n",
    "        '''\n",
    "        \n",
    "        # Add rows for each matcher\n",
    "        for name, results in self.results.items():\n",
    "            if 'overall_metrics' in results:\n",
    "                metrics = results['overall_metrics']\n",
    "                html += f'''\n",
    "                <tr>\n",
    "                    <td>{name}</td>\n",
    "                    <td>{metrics.get('precision', 0):.3f}</td>\n",
    "                    <td>{metrics.get('recall', 0):.3f}</td>\n",
    "                    <td>{metrics.get('f1_score', 0):.3f}</td>\n",
    "                    <td>{metrics.get('accuracy', 0):.3f}</td>\n",
    "                    <td>{metrics.get('roc_auc', 0):.3f}</td>\n",
    "                </tr>\n",
    "                '''\n",
    "        \n",
    "        html += '</table>'\n",
    "        return html\n",
    "    \n",
    "    def _generate_matcher_results_html(self):\n",
    "        \"\"\"Generate HTML sections for individual matcher results\"\"\"\n",
    "        html = ''\n",
    "        \n",
    "        for name, results in self.results.items():\n",
    "            # Skip the comparison results\n",
    "            if name == 'comparison':\n",
    "                continue\n",
    "                \n",
    "            if 'overall_metrics' not in results:\n",
    "                continue\n",
    "                \n",
    "            metrics = results['overall_metrics']\n",
    "            test_size = results.get('test_size', 0)\n",
    "            html += f'''\n",
    "            <div class=\"matcher-section\">\n",
    "                <h3>{name}</h3>\n",
    "                <p>Evaluated on {test_size} examples.</p>\n",
    "                \n",
    "                <h4>Overall Performance</h4>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Metric</th>\n",
    "                        <th>Value</th>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Precision</td>\n",
    "                        <td>{metrics.get('precision', 0):.3f}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Recall</td>\n",
    "                        <td>{metrics.get('recall', 0):.3f}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>F1 Score</td>\n",
    "                        <td>{metrics.get('f1_score', 0):.3f}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Accuracy</td>\n",
    "                        <td>{metrics.get('accuracy', 0):.3f}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>ROC AUC</td>\n",
    "                        <td>{metrics.get('roc_auc', 0):.3f}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Optimal Threshold</td>\n",
    "                        <td>{metrics.get('optimal_threshold', 0):.3f}</td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "                \n",
    "                <h4>Confusion Matrix</h4>\n",
    "                <div class=\"chart-container\" style=\"width: 400px; height: 300px;\">\n",
    "                    <canvas id=\"confusionMatrix_{name.replace(' ', '_')}\"></canvas>\n",
    "                </div>\n",
    "            '''\n",
    "            \n",
    "            # Add cross-validation results if available\n",
    "            if 'cross_validation' in results:\n",
    "                cv = results['cross_validation']\n",
    "                html += f'''\n",
    "                <h4>Cross-Validation Results ({len(cv.get('folds', []))} folds)</h4>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Metric</th>\n",
    "                        <th>Average</th>\n",
    "                        <th>Std Dev</th>\n",
    "                    </tr>\n",
    "                '''\n",
    "                \n",
    "                for metric, value in cv.get('avg_metrics', {}).items():\n",
    "                    std = cv.get('std_metrics', {}).get(metric, 0)\n",
    "                    html += f'''\n",
    "                    <tr>\n",
    "                        <td>{metric}</td>\n",
    "                        <td>{value:.3f}</td>\n",
    "                        <td>{std:.3f}</td>\n",
    "                    </tr>\n",
    "                    '''\n",
    "                    \n",
    "                html += '</table>'\n",
    "            \n",
    "            html += '</div>'\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def _generate_domain_performance_html(self):\n",
    "        \"\"\"Generate HTML for domain-specific performance\"\"\"\n",
    "        html = '''\n",
    "        <div class=\"chart-container\">\n",
    "            <canvas id=\"domainChart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Matcher</th>\n",
    "                <th>Domain</th>\n",
    "                <th>F1 Score</th>\n",
    "                <th>Precision</th>\n",
    "                <th>Recall</th>\n",
    "            </tr>\n",
    "        '''\n",
    "        \n",
    "        # Add rows for each matcher and domain\n",
    "        for name, results in self.results.items():\n",
    "            if name == 'comparison' or 'domain_performance' not in results:\n",
    "                continue\n",
    "                \n",
    "            domain_results = results['domain_performance']\n",
    "            for domain, metrics in domain_results.items():\n",
    "                html += f'''\n",
    "                <tr>\n",
    "                    <td>{name}</td>\n",
    "                    <td>{domain}</td>\n",
    "                    <td>{metrics.get('f1_score', 0):.3f}</td>\n",
    "                    <td>{metrics.get('precision', 0):.3f}</td>\n",
    "                    <td>{metrics.get('recall', 0):.3f}</td>\n",
    "                </tr>\n",
    "                '''\n",
    "        \n",
    "        html += '</table>'\n",
    "        return html\n",
    "    \n",
    "    def _generate_error_analysis_html(self):\n",
    "        \"\"\"Generate HTML for error analysis\"\"\"\n",
    "        html = '''\n",
    "        <div class=\"chart-container\">\n",
    "            <canvas id=\"errorChart\"></canvas>\n",
    "        </div>\n",
    "        '''\n",
    "        \n",
    "        # Add error examples for each matcher\n",
    "        for name, results in self.results.items():\n",
    "            if name == 'comparison' or 'error_analysis' not in results:\n",
    "                continue\n",
    "                \n",
    "            error_analysis = results['error_analysis']\n",
    "            total_errors = error_analysis.get('total_errors', 0)\n",
    "            error_rate = error_analysis.get('error_rate', 0)\n",
    "            \n",
    "            html += f'''\n",
    "            <h3>Errors for {name}</h3>\n",
    "            <p>Total Errors: {total_errors} (Error Rate: {error_rate:.2%})</p>\n",
    "            '''\n",
    "            \n",
    "            # Error subtype breakdown\n",
    "            if 'error_subtypes' in error_analysis and error_analysis['error_subtypes']:\n",
    "                html += '''\n",
    "                <h4>Error Type Distribution</h4>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Error Type</th>\n",
    "                        <th>Count</th>\n",
    "                        <th>Percentage</th>\n",
    "                    </tr>\n",
    "                '''\n",
    "                \n",
    "                for subtype, count in error_analysis['error_subtypes'].items():\n",
    "                    percentage = count / total_errors if total_errors > 0 else 0\n",
    "                    html += f'''\n",
    "                    <tr>\n",
    "                        <td>{subtype}</td>\n",
    "                        <td>{count}</td>\n",
    "                        <td>{percentage:.2%}</td>\n",
    "                    </tr>\n",
    "                    '''\n",
    "                    \n",
    "                html += '</table>'\n",
    "            \n",
    "            # Show sample error examples\n",
    "            if 'error_examples' in error_analysis and error_analysis['error_examples']:\n",
    "                examples = error_analysis['error_examples'][:10]  # Limit to 10 examples\n",
    "                \n",
    "                html += '''\n",
    "                <h4>Sample Error Examples</h4>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Merchant 1</th>\n",
    "                        <th>Merchant 2</th>\n",
    "                        <th>True Label</th>\n",
    "                        <th>Predicted</th>\n",
    "                        <th>Score</th>\n",
    "                        <th>Error Type</th>\n",
    "                    </tr>\n",
    "                '''\n",
    "                \n",
    "                for example in examples:\n",
    "                    true_label = \"Match\" if example['true_label'] == 1 else \"No Match\"\n",
    "                    pred_label = \"Match\" if example['pred_label'] == 1 else \"No Match\"\n",
    "                    \n",
    "                    html += f'''\n",
    "                    <tr>\n",
    "                        <td>{example['s1']}</td>\n",
    "                        <td>{example['s2']}</td>\n",
    "                        <td>{true_label}</td>\n",
    "                        <td>{pred_label}</td>\n",
    "                        <td>{example['score']:.3f}</td>\n",
    "                        <td>{example.get('error_subtype', 'Unknown')}</td>\n",
    "                    </tr>\n",
    "                    '''\n",
    "                    \n",
    "                html += '</table>'\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def _generate_statistical_significance_html(self):\n",
    "        \"\"\"Generate HTML for statistical significance tests\"\"\"\n",
    "        # Check if comparison results exist\n",
    "        if 'comparison' not in self.results or 'statistical_significance' not in self.results['comparison']:\n",
    "            return '<p>No statistical significance tests available.</p>'\n",
    "            \n",
    "        significance_tests = self.results['comparison']['statistical_significance']\n",
    "        \n",
    "        if not significance_tests or isinstance(significance_tests, dict) and 'error' in significance_tests:\n",
    "            return '<p>No statistical significance tests available.</p>'\n",
    "            \n",
    "        html = '''\n",
    "        <p>McNemar's test was used to determine if differences between matchers are statistically significant.</p>\n",
    "        \n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Comparison</th>\n",
    "                <th>p-value</th>\n",
    "                <th>Significant (p < 0.05)</th>\n",
    "                <th>Better Matcher</th>\n",
    "            </tr>\n",
    "        '''\n",
    "        \n",
    "        for pair, results in significance_tests.items():\n",
    "            significant = \"Yes\" if results['significant'] else \"No\"\n",
    "            \n",
    "            html += f'''\n",
    "            <tr>\n",
    "                <td>{pair}</td>\n",
    "                <td>{results['p_value']:.4f}</td>\n",
    "                <td>{significant}</td>\n",
    "                <td>{results.get('better_matcher', 'N/A')}</td>\n",
    "            </tr>\n",
    "            '''\n",
    "            \n",
    "        html += '</table>'\n",
    "        return html\n",
    "    \n",
    "    def _generate_chart_scripts(self):\n",
    "        \"\"\"Generate JavaScript for charts\"\"\"\n",
    "        # Extract data for charts\n",
    "        matcher_names = [name for name in self.results.keys() if name != 'comparison']\n",
    "        \n",
    "        # Overall metrics data\n",
    "        precision_data = []\n",
    "        recall_data = []\n",
    "        f1_data = []\n",
    "        \n",
    "        for name in matcher_names:\n",
    "            if 'overall_metrics' in self.results[name]:\n",
    "                metrics = self.results[name]['overall_metrics']\n",
    "                precision_data.append(metrics.get('precision', 0))\n",
    "                recall_data.append(metrics.get('recall', 0))\n",
    "                f1_data.append(metrics.get('f1_score', 0))\n",
    "        \n",
    "        # Domain performance data\n",
    "        domain_data = {}\n",
    "        for name in matcher_names:\n",
    "            if 'domain_performance' in self.results[name]:\n",
    "                domain_results = self.results[name]['domain_performance']\n",
    "                for domain, metrics in domain_results.items():\n",
    "                    if domain not in domain_data:\n",
    "                        domain_data[domain] = {}\n",
    "                    domain_data[domain][name] = metrics.get('f1_score', 0)\n",
    "        \n",
    "        # Build chart scripts\n",
    "        scripts = []\n",
    "        \n",
    "        # Overall metrics chart\n",
    "        scripts.append(f'''\n",
    "        const overallCtx = document.getElementById('overallChart').getContext('2d');\n",
    "        new Chart(overallCtx, {{\n",
    "            type: 'bar',\n",
    "            data: {{\n",
    "                labels: {json.dumps(matcher_names)},\n",
    "                datasets: [\n",
    "                    {{\n",
    "                        label: 'Precision',\n",
    "                        data: {json.dumps(precision_data)},\n",
    "                        backgroundColor: 'rgba(54, 162, 235, 0.5)',\n",
    "                        borderColor: 'rgb(54, 162, 235)',\n",
    "                        borderWidth: 1\n",
    "                    }},\n",
    "                    {{\n",
    "                        label: 'Recall',\n",
    "                        data: {json.dumps(recall_data)},\n",
    "                        backgroundColor: 'rgba(255, 99, 132, 0.5)',\n",
    "                        borderColor: 'rgb(255, 99, 132)',\n",
    "                        borderWidth: 1\n",
    "                    }},\n",
    "                    {{\n",
    "                        label: 'F1 Score',\n",
    "                        data: {json.dumps(f1_data)},\n",
    "                        backgroundColor: 'rgba(75, 192, 192, 0.5)',\n",
    "                        borderColor: 'rgb(75, 192, 192)',\n",
    "                        borderWidth: 1\n",
    "                    }}\n",
    "                ]\n",
    "            }},\n",
    "            options: {{\n",
    "                responsive: true,\n",
    "                title: {{\n",
    "                    display: true,\n",
    "                    text: 'Overall Matcher Performance'\n",
    "                }},\n",
    "                scales: {{\n",
    "                    y: {{\n",
    "                        beginAtZero: true,\n",
    "                        max: 1\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }});\n",
    "        ''')\n",
    "        \n",
    "        # Domain performance chart\n",
    "        domain_chart_data = []\n",
    "        domains = list(domain_data.keys())\n",
    "        \n",
    "        for name in matcher_names:\n",
    "            dataset = {\n",
    "                'label': name,\n",
    "                'data': [domain_data.get(domain, {}).get(name, 0) for domain in domains]\n",
    "            }\n",
    "            domain_chart_data.append(dataset)\n",
    "            \n",
    "        if domains and domain_chart_data:\n",
    "            scripts.append(f'''\n",
    "            const domainCtx = document.getElementById('domainChart').getContext('2d');\n",
    "            new Chart(domainCtx, {{\n",
    "                type: 'radar',\n",
    "                data: {{\n",
    "                    labels: {json.dumps(domains)},\n",
    "                    datasets: {json.dumps(domain_chart_data)}\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    title: {{\n",
    "                        display: true,\n",
    "                        text: 'Domain-Specific Performance (F1 Score)'\n",
    "                    }},\n",
    "                    scale: {{\n",
    "                        ticks: {{\n",
    "                            beginAtZero: true,\n",
    "                            max: 1\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "            ''')\n",
    "        \n",
    "        # Confusion matrix charts for each matcher\n",
    "        for name in matcher_names:\n",
    "            if 'overall_metrics' in self.results[name] and 'confusion_matrix' in self.results[name]['overall_metrics']:\n",
    "                cm = self.results[name]['overall_metrics']['confusion_matrix']\n",
    "                \n",
    "                if isinstance(cm, list) and len(cm) == 2 and len(cm[0]) == 2:\n",
    "                    # Extract values from 2x2 confusion matrix\n",
    "                    tn, fp = cm[0]\n",
    "                    fn, tp = cm[1]\n",
    "                    \n",
    "                    scripts.append(f'''\n",
    "                    const cmCtx_{name.replace(' ', '_')} = document.getElementById('confusionMatrix_{name.replace(' ', '_')}').getContext('2d');\n",
    "                    new Chart(cmCtx_{name.replace(' ', '_')}, {{\n",
    "                        type: 'pie',\n",
    "                        data: {{\n",
    "                            labels: ['True Positives', 'False Positives', 'True Negatives', 'False Negatives'],\n",
    "                            datasets: [{{\n",
    "                                data: [{tp}, {fp}, {tn}, {fn}],\n",
    "                                backgroundColor: [\n",
    "                                    'rgba(75, 192, 192, 0.5)',  // TP: Teal\n",
    "                                    'rgba(255, 99, 132, 0.5)',  // FP: Red\n",
    "                                    'rgba(54, 162, 235, 0.5)',  // TN: Blue\n",
    "                                    'rgba(255, 206, 86, 0.5)'   // FN: Yellow\n",
    "                                ],\n",
    "                                borderColor: [\n",
    "                                    'rgb(75, 192, 192)',\n",
    "                                    'rgb(255, 99, 132)',\n",
    "                                    'rgb(54, 162, 235)',\n",
    "                                    'rgb(255, 206, 86)'\n",
    "                                ],\n",
    "                                borderWidth: 1\n",
    "                            }}]\n",
    "                        }},\n",
    "                        options: {{\n",
    "                            responsive: true,\n",
    "                            title: {{\n",
    "                                display: true,\n",
    "                                text: 'Confusion Matrix'\n",
    "                            }}\n",
    "                        }}\n",
    "                    }});\n",
    "                    ''')\n",
    "        \n",
    "        # Error analysis chart\n",
    "        error_labels = []\n",
    "        error_data = []\n",
    "        \n",
    "        for name in matcher_names:\n",
    "            if 'error_analysis' in self.results[name]:\n",
    "                error_analysis = self.results[name]['error_analysis']\n",
    "                error_labels.append(name)\n",
    "                error_data.append(error_analysis.get('error_rate', 0))\n",
    "                \n",
    "        if error_labels and error_data:\n",
    "            scripts.append(f'''\n",
    "            const errorCtx = document.getElementById('errorChart').getContext('2d');\n",
    "            new Chart(errorCtx, {{\n",
    "                type: 'horizontalBar',\n",
    "                data: {{\n",
    "                    labels: {json.dumps(error_labels)},\n",
    "                    datasets: [{{\n",
    "                        label: 'Error Rate',\n",
    "                        data: {json.dumps(error_data)},\n",
    "                        backgroundColor: 'rgba(255, 99, 132, 0.5)',\n",
    "                        borderColor: 'rgb(255, 99, 132)',\n",
    "                        borderWidth: 1\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    title: {{\n",
    "                        display: true,\n",
    "                        text: 'Error Rate by Matcher'\n",
    "                    }},\n",
    "                    scales: {{\n",
    "                        x: {{\n",
    "                            beginAtZero: true,\n",
    "                            max: 1\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "            ''')\n",
    "            \n",
    "        return '\\n'.join(scripts)\n",
    "    \n",
    "    def visualize_errors(self, matcher_name, n_examples=10):\n",
    "        \"\"\"\n",
    "        Visualize representative error examples for a matcher\n",
    "        \n",
    "        Args:\n",
    "            matcher_name (str): Name of the matcher to visualize\n",
    "            n_examples (int): Number of examples to display\n",
    "            \n",
    "        Returns:\n",
    "            str: HTML visualization\n",
    "        \"\"\"\n",
    "        if matcher_name not in self.results or 'error_analysis' not in self.results[matcher_name]:\n",
    "            return f\"No error analysis available for matcher '{matcher_name}'\"\n",
    "            \n",
    "        error_analysis = self.results[matcher_name]['error_analysis']\n",
    "        if 'error_examples' not in error_analysis or not error_analysis['error_examples']:\n",
    "            return f\"No error examples available for matcher '{matcher_name}'\"\n",
    "            \n",
    "        # Get representative examples (mixed error types)\n",
    "        examples = error_analysis['error_examples'][:n_examples]\n",
    "        \n",
    "        # Generate HTML visualization\n",
    "        html = f'''\n",
    "        <h2>Error Analysis for {matcher_name}</h2>\n",
    "        <p>Total Errors: {error_analysis.get('total_errors', 0)} (Error Rate: {error_analysis.get('error_rate', 0):.2%})</p>\n",
    "        \n",
    "        <h3>Representative Error Examples</h3>\n",
    "        <table style=\"width:100%; border-collapse: collapse; margin-bottom: 20px;\">\n",
    "            <tr style=\"background-color: #f2f2f2;\">\n",
    "                <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Merchant 1</th>\n",
    "                <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Merchant 2</th>\n",
    "                <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">True</th>\n",
    "                <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Predicted</th>\n",
    "                <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Score</th>\n",
    "                <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Error Type</th>\n",
    "            </tr>\n",
    "        '''\n",
    "        \n",
    "        for example in examples:\n",
    "            true_label = \"Match\" if example['true_label'] == 1 else \"No Match\"\n",
    "            pred_label = \"Match\" if example['pred_label'] == 1 else \"No Match\"\n",
    "            \n",
    "            # Color-code based on error type\n",
    "            bg_color = \"#ffebee\" if example['error_type'] == 'false_positive' else \"#e8f5e9\"\n",
    "            \n",
    "            html += f'''\n",
    "            <tr style=\"background-color: {bg_color};\">\n",
    "                <td style=\"border: 1px solid #ddd; padding: 8px;\">{example['s1']}</td>\n",
    "                <td style=\"border: 1px solid #ddd; padding: 8px;\">{example['s2']}</td>\n",
    "                <td style=\"border: 1px solid #ddd; padding: 8px;\">{true_label}</td>\n",
    "                <td style=\"border: 1px solid #ddd; padding: 8px;\">{pred_label}</td>\n",
    "                <td style=\"border: 1px solid #ddd; padding: 8px;\">{example['score']:.3f}</td>\n",
    "                <td style=\"border: 1px solid #ddd; padding: 8px;\">{example.get('error_subtype', 'Unknown')}</td>\n",
    "            </tr>\n",
    "            '''\n",
    "            \n",
    "        html += '</table>'\n",
    "        \n",
    "        # Add error type distribution\n",
    "        if 'error_subtypes' in error_analysis and error_analysis['error_subtypes']:\n",
    "            html += '''\n",
    "            <h3>Error Type Distribution</h3>\n",
    "            <table style=\"width:60%; border-collapse: collapse; margin-bottom: 20px;\">\n",
    "                <tr style=\"background-color: #f2f2f2;\">\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Error Type</th>\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Count</th>\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Percentage</th>\n",
    "                </tr>\n",
    "            '''\n",
    "            \n",
    "            total_errors = error_analysis.get('total_errors', 0)\n",
    "            for subtype, count in error_analysis['error_subtypes'].items():\n",
    "                percentage = count / total_errors if total_errors > 0 else 0\n",
    "                html += f'''\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">{subtype}</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">{count}</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">{percentage:.2%}</td>\n",
    "                </tr>\n",
    "                '''\n",
    "                \n",
    "            html += '</table>'\n",
    "        \n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce8769e7-b6fc-4319-95de-062b8d0ab809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Comprehensive Testing and Analysis Process\n",
    "\n",
    "class MerchantMatcherTester:\n",
    "    \"\"\"\n",
    "    Comprehensive testing and analysis framework for the merchant name matching system.\n",
    "    This class provides end-to-end functionality for validating the matching system,\n",
    "    generating test cases, analyzing performance, optimizing parameters, and generating\n",
    "    detailed reports with explanations and confidence metrics.\n",
    "    \n",
    "    Key features:\n",
    "    - Input validation and preprocessing\n",
    "    - Test suite generation with diverse edge cases\n",
    "    - Automated threshold optimization\n",
    "    - Performance analysis with detailed metrics\n",
    "    - Output generation with explanations\n",
    "    - Confidence scoring mechanism\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline=None, config_path=None, domain=None, \n",
    "                 output_dir=\"./merchant_matcher_results\"):\n",
    "        \"\"\"\n",
    "        Initialize the tester with the matching pipeline and configuration\n",
    "        \n",
    "        Args:\n",
    "            pipeline (GMARTMerchantMatchingPipeline): Existing matcher pipeline\n",
    "            config_path (str): Path to configuration file\n",
    "            domain (str): Default domain for testing\n",
    "            output_dir (str): Directory for output files\n",
    "        \"\"\"\n",
    "        # Use existing pipeline or create a new one\n",
    "        if pipeline:\n",
    "            self.pipeline = pipeline\n",
    "        else:\n",
    "            self.pipeline = GMARTMerchantMatchingPipeline(\n",
    "                config_path=config_path,\n",
    "                domain=domain,\n",
    "                debug_mode=True,\n",
    "                log_level='INFO'\n",
    "            )\n",
    "        \n",
    "        # Set up output directory\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize performance metrics tracking\n",
    "        self.performance_metrics = {}\n",
    "        self.test_results = pd.DataFrame()\n",
    "        self.optimal_thresholds = {}\n",
    "        \n",
    "        # Tracking for confidence score calibration\n",
    "        self.score_distribution = {\n",
    "            'true_positives': [],\n",
    "            'false_positives': [],\n",
    "            'true_negatives': [],\n",
    "            'false_negatives': []\n",
    "        }\n",
    "        \n",
    "        # Set up logging for test results\n",
    "        self._setup_logging()\n",
    "        \n",
    "        # Load or initialize confusion matrix\n",
    "        self.confusion_matrix = {\n",
    "            'true_positives': 0,\n",
    "            'false_positives': 0,\n",
    "            'false_negatives': 0,\n",
    "            'true_negatives': 0\n",
    "        }\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Set up logging for test results\"\"\"\n",
    "        self.logger = logging.getLogger('merchant_matcher_tester')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Create file handler\n",
    "        log_file = os.path.join(self.output_dir, \"test_results.log\")\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        \n",
    "        # Create formatter and add to handler\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        \n",
    "        # Add handler to logger\n",
    "        self.logger.addHandler(file_handler)\n",
    "        \n",
    "        self.logger.info(\"MerchantMatcherTester initialized\")\n",
    "    \n",
    "    def _validate_test_data(self, test_data):\n",
    "        \"\"\"\n",
    "        Validate test data and preprocess for testing\n",
    "        \n",
    "        Args:\n",
    "            test_data (DataFrame): Test data with merchant pairs\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Validated and preprocessed test data\n",
    "        \"\"\"\n",
    "        # Check if DataFrame\n",
    "        if not isinstance(test_data, pd.DataFrame):\n",
    "            raise ValueError(\"Test data must be a pandas DataFrame\")\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = ['s1', 's2']\n",
    "        if not all(col in test_data.columns for col in required_cols):\n",
    "            # Try to adapt common column formats\n",
    "            if 'Acronym' in test_data.columns and 'Full_Name' in test_data.columns:\n",
    "                self.logger.info(\"Adapting Acronym/Full_Name format to s1/s2\")\n",
    "                test_data = test_data.rename(columns={'Acronym': 's1', 'Full_Name': 's2'})\n",
    "            elif 'input_name' in test_data.columns and 'matched_name' in test_data.columns:\n",
    "                self.logger.info(\"Adapting input_name/matched_name format to s1/s2\")\n",
    "                test_data = test_data.rename(columns={'input_name': 's1', 'matched_name': 's2'})\n",
    "            else:\n",
    "                raise ValueError(f\"Test data must contain 's1' and 's2' columns. Found: {test_data.columns.tolist()}\")\n",
    "        \n",
    "        # Check if ground truth exists\n",
    "        if 'is_match' not in test_data.columns and 'expected_match' not in test_data.columns:\n",
    "            self.logger.warning(\"No ground truth column found. Adding placeholder 'is_match' column.\")\n",
    "            test_data['is_match'] = None\n",
    "        elif 'expected_match' in test_data.columns and 'is_match' not in test_data.columns:\n",
    "            self.logger.info(\"Renaming 'expected_match' to 'is_match'\")\n",
    "            test_data = test_data.rename(columns={'expected_match': 'is_match'})\n",
    "        \n",
    "        # Handle missing values\n",
    "        test_data = test_data.dropna(subset=['s1', 's2'])\n",
    "        self.logger.info(f\"Validated test data with {len(test_data)} valid pairs\")\n",
    "        \n",
    "        return test_data\n",
    "    \n",
    "    def generate_test_suite(self, base_data=None, num_cases=100, include_edge_cases=True,\n",
    "                            domain=None, output_file=None):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive test suite with diverse test cases\n",
    "        \n",
    "        Args:\n",
    "            base_data (DataFrame): Base data to augment with test cases\n",
    "            num_cases (int): Number of test cases to generate\n",
    "            include_edge_cases (bool): Whether to include edge cases\n",
    "            domain (str): Domain for test cases\n",
    "            output_file (str): Path to save test suite\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Generated test suite\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Generating test suite with {num_cases} cases\")\n",
    "        \n",
    "        # Initialize test suite\n",
    "        if base_data is not None:\n",
    "            test_suite = self._validate_test_data(base_data).copy()\n",
    "        else:\n",
    "            test_suite = pd.DataFrame(columns=['s1', 's2', 'is_match', 'case_type'])\n",
    "        \n",
    "        # Get merchant matcher for generation\n",
    "        matcher = self.pipeline._get_merchant_matcher()\n",
    "        preprocessor = self.pipeline._initialized_components.get('preprocessor')\n",
    "        \n",
    "        if not preprocessor:\n",
    "            raise ValueError(\"Preprocessor not initialized in pipeline\")\n",
    "        \n",
    "        # Generate positive pairs (matching merchants)\n",
    "        positive_pairs = []\n",
    "        \n",
    "        # Common merchant names for test generation\n",
    "        common_merchants = {\n",
    "            'banking': [\n",
    "                'Bank of America', 'Chase Bank', 'Wells Fargo', 'Citibank', \n",
    "                'JP Morgan', 'Capital One', 'HSBC Bank', 'TD Bank', 'PNC Bank',\n",
    "                'US Bank', 'Barclays', 'Santander', 'Bank of the West', 'Regions Bank'\n",
    "            ],\n",
    "            'retail': [\n",
    "                'Walmart', 'Target', 'Costco', 'Amazon', 'Best Buy', 'Home Depot', \n",
    "                'Lowe\\'s', 'Macy\\'s', 'Kohl\\'s', 'Nordstrom', 'Staples', 'Office Depot',\n",
    "                'CVS Pharmacy', 'Walgreens', 'Dollar General'\n",
    "            ],\n",
    "            'restaurant': [\n",
    "                'McDonald\\'s', 'Starbucks', 'Subway', 'Taco Bell', 'Burger King',\n",
    "                'Wendy\\'s', 'KFC', 'Pizza Hut', 'Domino\\'s Pizza', 'Chipotle',\n",
    "                'Panera Bread', 'Olive Garden', 'Chili\\'s', 'Applebee\\'s', 'Outback'\n",
    "            ],\n",
    "            'gas_station': [\n",
    "                'Shell', 'Exxon', 'BP', 'Chevron', 'Texaco', 'Mobil', 'Sunoco',\n",
    "                'Valero', 'Marathon', 'Phillips 66', 'Circle K', '7-Eleven',\n",
    "                'QuikTrip', 'RaceTrac', 'Speedway'\n",
    "            ],\n",
    "            'hotel': [\n",
    "                'Marriott', 'Hilton', 'Hyatt', 'Holiday Inn', 'Sheraton', 'Westin',\n",
    "                'Hampton Inn', 'Comfort Inn', 'Best Western', 'Courtyard Marriott',\n",
    "                'Embassy Suites', 'Doubletree', 'Four Seasons', 'Ritz-Carlton'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Select domain merchants\n",
    "        if domain and domain.lower() in common_merchants:\n",
    "            merchants = common_merchants[domain.lower()]\n",
    "        else:\n",
    "            # Combine all merchants\n",
    "            merchants = []\n",
    "            for domain_merchants in common_merchants.values():\n",
    "                merchants.extend(domain_merchants)\n",
    "        \n",
    "        # Generate positive pairs with variations\n",
    "        num_positive = min(num_cases // 2, 50)\n",
    "        for i in range(num_positive):\n",
    "            if len(merchants) > 0:\n",
    "                # Select random merchant\n",
    "                merchant = random.choice(merchants)\n",
    "                \n",
    "                # Create variations\n",
    "                variation_type = random.choice([\n",
    "                    'suffix', 'prefix', 'abbreviation', 'typo', 'spacing',\n",
    "                    'punctuation', 'capitalization', 'word_order'\n",
    "                ])\n",
    "                \n",
    "                if variation_type == 'suffix':\n",
    "                    # Add business suffix\n",
    "                    suffixes = [' Inc', ' LLC', ' Corp', ' Company', ' Co', ' Ltd']\n",
    "                    variation = merchant + random.choice(suffixes)\n",
    "                elif variation_type == 'prefix':\n",
    "                    # Add location prefix\n",
    "                    prefixes = ['Downtown ', 'North ', 'South ', 'East ', 'West ', 'Central ']\n",
    "                    variation = random.choice(prefixes) + merchant\n",
    "                elif variation_type == 'abbreviation':\n",
    "                    # Create abbreviation or acronym\n",
    "                    if ' ' in merchant:\n",
    "                        words = merchant.split()\n",
    "                        variation = ''.join(word[0] for word in words)\n",
    "                    else:\n",
    "                        # Abbreviate by removing vowels\n",
    "                        variation = ''.join(c for c in merchant if c.lower() not in 'aeiou')\n",
    "                elif variation_type == 'typo':\n",
    "                    # Introduce typo\n",
    "                    chars = list(merchant)\n",
    "                    if len(chars) > 3:\n",
    "                        pos = random.randint(1, len(chars)-2)\n",
    "                        chars[pos] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "                        variation = ''.join(chars)\n",
    "                    else:\n",
    "                        variation = merchant\n",
    "                elif variation_type == 'spacing':\n",
    "                    # Modify spacing\n",
    "                    if ' ' in merchant:\n",
    "                        variation = merchant.replace(' ', '')\n",
    "                    else:\n",
    "                        # Insert random space\n",
    "                        pos = random.randint(1, len(merchant)-1)\n",
    "                        variation = merchant[:pos] + ' ' + merchant[pos:]\n",
    "                elif variation_type == 'punctuation':\n",
    "                    # Add/modify punctuation\n",
    "                    if \"'\" in merchant:\n",
    "                        variation = merchant.replace(\"'\", \"\")\n",
    "                    else:\n",
    "                        # Add apostrophe or hyphen\n",
    "                        if random.random() < 0.5:\n",
    "                            # Add apostrophe\n",
    "                            pos = random.randint(1, len(merchant)-1)\n",
    "                            variation = merchant[:pos] + \"'\" + merchant[pos:]\n",
    "                        else:\n",
    "                            # Add hyphen if spaces\n",
    "                            variation = merchant.replace(' ', '-')\n",
    "                elif variation_type == 'capitalization':\n",
    "                    # Change capitalization\n",
    "                    options = [merchant.upper(), merchant.lower(), merchant.title()]\n",
    "                    variation = random.choice(options)\n",
    "                elif variation_type == 'word_order':\n",
    "                    # Change word order if multiple words\n",
    "                    if ' ' in merchant:\n",
    "                        words = merchant.split()\n",
    "                        if len(words) > 1:\n",
    "                            random.shuffle(words)\n",
    "                            variation = ' '.join(words)\n",
    "                        else:\n",
    "                            variation = merchant\n",
    "                    else:\n",
    "                        variation = merchant\n",
    "                \n",
    "                positive_pairs.append({\n",
    "                    's1': merchant,\n",
    "                    's2': variation,\n",
    "                    'is_match': 1,\n",
    "                    'case_type': f'positive_{variation_type}'\n",
    "                })\n",
    "        \n",
    "        # Generate negative pairs (non-matching merchants)\n",
    "        negative_pairs = []\n",
    "        num_negative = min(num_cases - num_positive, 50)\n",
    "        for i in range(num_negative):\n",
    "            if len(merchants) > 1:\n",
    "                # Select two different merchants\n",
    "                merchant1 = random.choice(merchants)\n",
    "                merchant2 = random.choice([m for m in merchants if m != merchant1])\n",
    "                \n",
    "                # Create different types of negative pairs\n",
    "                case_type = random.choice([\n",
    "                    'completely_different', 'partial_match', 'similar_industry', \n",
    "                    'similar_word', 'substring', 'acronym_confusion'\n",
    "                ])\n",
    "                \n",
    "                if case_type == 'completely_different':\n",
    "                    # Use merchants as is\n",
    "                    pass\n",
    "                elif case_type == 'partial_match':\n",
    "                    # Create partial word match\n",
    "                    if ' ' in merchant1 and ' ' in merchant2:\n",
    "                        words1 = merchant1.split()\n",
    "                        words2 = merchant2.split()\n",
    "                        # Share one word\n",
    "                        shared_word = random.choice(words1)\n",
    "                        pos = random.randint(0, len(words2)-1)\n",
    "                        words2[pos] = shared_word\n",
    "                        merchant2 = ' '.join(words2)\n",
    "                elif case_type == 'similar_industry':\n",
    "                    # Ensure from same industry but different\n",
    "                    for domain_name, domain_merchants in common_merchants.items():\n",
    "                        if merchant1 in domain_merchants:\n",
    "                            other_merchants = [m for m in domain_merchants \n",
    "                                             if m != merchant1]\n",
    "                            if other_merchants:\n",
    "                                merchant2 = random.choice(other_merchants)\n",
    "                            break\n",
    "                elif case_type == 'similar_word':\n",
    "                    # Find similar sounding merchant\n",
    "                    similar_merchants = sorted(merchants, \n",
    "                                             key=lambda m: jaro_winkler(merchant1, m))\n",
    "                    if len(similar_merchants) > 2:\n",
    "                        # Pick second most similar (most similar would be itself)\n",
    "                        merchant2 = similar_merchants[-2]\n",
    "                elif case_type == 'substring':\n",
    "                    # One is substring of other\n",
    "                    if len(merchant1) > 5:\n",
    "                        merchant2 = merchant1[:len(merchant1)//2]\n",
    "                    else:\n",
    "                        # If too short, just use different merchant\n",
    "                        merchant2 = random.choice([m for m in merchants if m != merchant1])\n",
    "                elif case_type == 'acronym_confusion':\n",
    "                    # Create confusing acronym\n",
    "                    if ' ' in merchant1:\n",
    "                        words = merchant1.split()\n",
    "                        acronym = ''.join(word[0] for word in words)\n",
    "                        # Find another merchant with same acronym\n",
    "                        for m in merchants:\n",
    "                            if m != merchant1 and ' ' in m:\n",
    "                                m_words = m.split()\n",
    "                                m_acronym = ''.join(word[0] for word in m_words)\n",
    "                                if m_acronym == acronym:\n",
    "                                    merchant2 = m\n",
    "                                    break\n",
    "                \n",
    "                # Add negative pair\n",
    "                negative_pairs.append({\n",
    "                    's1': merchant1,\n",
    "                    's2': merchant2,\n",
    "                    'is_match': 0,\n",
    "                    'case_type': f'negative_{case_type}'\n",
    "                })\n",
    "        \n",
    "        # Add edge cases if requested\n",
    "        edge_cases = []\n",
    "        if include_edge_cases:\n",
    "            # Empty strings\n",
    "            edge_cases.append({\n",
    "                's1': '', 's2': 'Valid Merchant', 'is_match': 0, 'case_type': 'edge_empty_s1'\n",
    "            })\n",
    "            edge_cases.append({\n",
    "                's1': 'Valid Merchant', 's2': '', 'is_match': 0, 'case_type': 'edge_empty_s2'\n",
    "            })\n",
    "            \n",
    "            # Very short names\n",
    "            edge_cases.append({\n",
    "                's1': 'IBM', 's2': 'IBM Corp', 'is_match': 1, 'case_type': 'edge_short_name'\n",
    "            })\n",
    "            \n",
    "            # Special characters\n",
    "            edge_cases.append({\n",
    "                's1': 'AT&T', 's2': 'AT and T', 'is_match': 1, 'case_type': 'edge_special_chars'\n",
    "            })\n",
    "            \n",
    "            # Numbers\n",
    "            edge_cases.append({\n",
    "                's1': '7-Eleven', 's2': 'Seven Eleven', 'is_match': 1, 'case_type': 'edge_numbers'\n",
    "            })\n",
    "            \n",
    "            # Extreme length difference\n",
    "            edge_cases.append({\n",
    "                's1': 'IBM', \n",
    "                's2': 'International Business Machines Corporation Worldwide', \n",
    "                'is_match': 1, \n",
    "                'case_type': 'edge_length_diff'\n",
    "            })\n",
    "            \n",
    "            # Repeated words\n",
    "            edge_cases.append({\n",
    "                's1': 'Buffalo Buffalo', \n",
    "                's2': 'Buffalo', \n",
    "                'is_match': 1, \n",
    "                'case_type': 'edge_repeated_words'\n",
    "            })\n",
    "            \n",
    "            # Non-English characters\n",
    "            edge_cases.append({\n",
    "                's1': 'Caf Coffee', \n",
    "                's2': 'Cafe Coffee', \n",
    "                'is_match': 1, \n",
    "                'case_type': 'edge_non_english'\n",
    "            })\n",
    "        \n",
    "        # Combine all test cases\n",
    "        all_cases = pd.DataFrame(positive_pairs + negative_pairs + edge_cases)\n",
    "        \n",
    "        # Append to existing test suite if provided\n",
    "        if not test_suite.empty:\n",
    "            test_suite = pd.concat([test_suite, all_cases], ignore_index=True)\n",
    "        else:\n",
    "            test_suite = all_cases\n",
    "        \n",
    "        # Save to file if requested\n",
    "        if output_file:\n",
    "            output_path = os.path.join(self.output_dir, output_file)\n",
    "            test_suite.to_csv(output_path, index=False)\n",
    "            self.logger.info(f\"Test suite saved to {output_path}\")\n",
    "        \n",
    "        return test_suite\n",
    "    \n",
    "    def run_tests(self, test_data, domain=None, threshold=None, save_results=True):\n",
    "        \"\"\"\n",
    "        Run comprehensive tests on the matcher with the provided test data\n",
    "        \n",
    "        Args:\n",
    "            test_data (DataFrame): Test data with merchant pairs and expected matches\n",
    "            domain (str): Domain for testing\n",
    "            threshold (float): Custom threshold for matching\n",
    "            save_results (bool): Whether to save detailed results\n",
    "            \n",
    "        Returns:\n",
    "            dict: Test results and performance metrics\n",
    "        \"\"\"\n",
    "        # Validate and preprocess test data\n",
    "        test_data = self._validate_test_data(test_data)\n",
    "        \n",
    "        # Check if ground truth exists\n",
    "        has_ground_truth = 'is_match' in test_data.columns and not test_data['is_match'].isna().all()\n",
    "        \n",
    "        # Get effective domain\n",
    "        effective_domain = domain or self.pipeline.domain\n",
    "        \n",
    "        # Run matcher on test data\n",
    "        self.logger.info(f\"Running tests on {len(test_data)} merchant pairs\")\n",
    "        \n",
    "        # Get matcher\n",
    "        matcher = self.pipeline._get_merchant_matcher()\n",
    "        \n",
    "        # Customize threshold if provided\n",
    "        original_thresholds = None\n",
    "        if threshold is not None:\n",
    "            original_thresholds = matcher.thresholds.copy()\n",
    "            matcher.thresholds = {\n",
    "                'high': threshold + 0.1,\n",
    "                'medium': threshold,\n",
    "                'low': threshold - 0.1\n",
    "            }\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 100\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(test_data), batch_size):\n",
    "            batch = test_data.iloc[i:i+batch_size]\n",
    "            self.logger.info(f\"Processing batch {i//batch_size + 1}/{(len(test_data)-1)//batch_size + 1}\")\n",
    "            \n",
    "            batch_results = []\n",
    "            for _, row in batch.iterrows():\n",
    "                s1 = row['s1']\n",
    "                s2 = row['s2']\n",
    "                \n",
    "                # Skip invalid pairs\n",
    "                if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "                    continue\n",
    "                \n",
    "                # Get detailed match result\n",
    "                match_result = matcher.match_merchants(\n",
    "                    s1, s2, effective_domain, return_details=True\n",
    "                )\n",
    "                \n",
    "                # Create result entry\n",
    "                result_entry = {\n",
    "                    's1': s1,\n",
    "                    's2': s2,\n",
    "                    'score': match_result['match_score'],\n",
    "                    'level': match_result['match_level'],\n",
    "                    'confidence': self._calculate_confidence(match_result),\n",
    "                    'expected_match': row.get('is_match') if has_ground_truth else None,\n",
    "                    'explanation': match_result['explanation'],\n",
    "                    'case_type': row.get('case_type', 'unknown')\n",
    "                }\n",
    "                \n",
    "                # Calculate match correctness if ground truth exists\n",
    "                if has_ground_truth and not pd.isna(row['is_match']):\n",
    "                    expected = bool(row['is_match'])\n",
    "                    actual = match_result['match_score'] >= (threshold or matcher.thresholds['medium'])\n",
    "                    result_entry['correct'] = expected == actual\n",
    "                    \n",
    "                    # Update confusion matrix\n",
    "                    if expected and actual:\n",
    "                        self.confusion_matrix['true_positives'] += 1\n",
    "                        self.score_distribution['true_positives'].append(match_result['match_score'])\n",
    "                    elif expected and not actual:\n",
    "                        self.confusion_matrix['false_negatives'] += 1\n",
    "                        self.score_distribution['false_negatives'].append(match_result['match_score'])\n",
    "                    elif not expected and actual:\n",
    "                        self.confusion_matrix['false_positives'] += 1\n",
    "                        self.score_distribution['false_positives'].append(match_result['match_score'])\n",
    "                    else:\n",
    "                        self.confusion_matrix['true_negatives'] += 1\n",
    "                        self.score_distribution['true_negatives'].append(match_result['match_score'])\n",
    "                \n",
    "                batch_results.append(result_entry)\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "        \n",
    "        # Restore original thresholds if modified\n",
    "        if original_thresholds:\n",
    "            matcher.thresholds = original_thresholds\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        self.test_results = results_df\n",
    "        \n",
    "        # Calculate performance metrics if ground truth exists\n",
    "        performance = {}\n",
    "        if has_ground_truth:\n",
    "            # Calculate accuracy, precision, recall, F1\n",
    "            tp = self.confusion_matrix['true_positives']\n",
    "            fp = self.confusion_matrix['false_positives']\n",
    "            fn = self.confusion_matrix['false_negatives']\n",
    "            tn = self.confusion_matrix['true_negatives']\n",
    "            \n",
    "            total = tp + fp + fn + tn\n",
    "            \n",
    "            if total > 0:\n",
    "                accuracy = (tp + tn) / total\n",
    "            else:\n",
    "                accuracy = 0.0\n",
    "                \n",
    "            if (tp + fp) > 0:\n",
    "                precision = tp / (tp + fp)\n",
    "            else:\n",
    "                precision = 0.0\n",
    "                \n",
    "            if (tp + fn) > 0:\n",
    "                recall = tp / (tp + fn)\n",
    "            else:\n",
    "                recall = 0.0\n",
    "                \n",
    "            if (precision + recall) > 0:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            else:\n",
    "                f1 = 0.0\n",
    "            \n",
    "            performance = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'true_positives': tp,\n",
    "                'false_positives': fp,\n",
    "                'false_negatives': fn,\n",
    "                'true_negatives': tn\n",
    "            }\n",
    "            \n",
    "            # Calculate performance by case type\n",
    "            case_types = results_df['case_type'].unique()\n",
    "            case_performance = {}\n",
    "            \n",
    "            for case_type in case_types:\n",
    "                case_results = results_df[results_df['case_type'] == case_type]\n",
    "                if not case_results.empty and 'correct' in case_results.columns:\n",
    "                    case_performance[case_type] = {\n",
    "                        'count': len(case_results),\n",
    "                        'accuracy': case_results['correct'].mean(),\n",
    "                        'avg_score': case_results['score'].mean(),\n",
    "                        'avg_confidence': case_results['confidence'].mean()\n",
    "                    }\n",
    "            \n",
    "            performance['case_type_performance'] = case_performance\n",
    "        \n",
    "        # Save the performance metrics\n",
    "        self.performance_metrics = performance\n",
    "        \n",
    "        # Save results if requested\n",
    "        if save_results:\n",
    "            # Save detailed results\n",
    "            results_path = os.path.join(self.output_dir, \"test_results.csv\")\n",
    "            results_df.to_csv(results_path, index=False)\n",
    "            \n",
    "            # Save performance metrics\n",
    "            if performance:\n",
    "                metrics_path = os.path.join(self.output_dir, \"performance_metrics.json\")\n",
    "                with open(metrics_path, 'w') as f:\n",
    "                    json.dump(performance, f, indent=2)\n",
    "            \n",
    "            self.logger.info(f\"Test results saved to {self.output_dir}\")\n",
    "        \n",
    "        # Log performance summary\n",
    "        if performance:\n",
    "            self.logger.info(f\"Performance Summary:\")\n",
    "            self.logger.info(f\"  Accuracy: {performance['accuracy']:.4f}\")\n",
    "            self.logger.info(f\"  Precision: {performance['precision']:.4f}\")\n",
    "            self.logger.info(f\"  Recall: {performance['recall']:.4f}\")\n",
    "            self.logger.info(f\"  F1 Score: {performance['f1_score']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'results': results_df,\n",
    "            'performance': performance,\n",
    "            'domain': effective_domain,\n",
    "            'threshold': threshold\n",
    "        }\n",
    "    \n",
    "    def _calculate_confidence(self, match_result):\n",
    "        \"\"\"\n",
    "        Calculate confidence score for a match result\n",
    "        \n",
    "        Args:\n",
    "            match_result (dict): Match result from matcher\n",
    "            \n",
    "        Returns:\n",
    "            float: Confidence score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Extract key metrics\n",
    "        score = match_result['match_score']\n",
    "        features = match_result.get('features', {})\n",
    "        patterns = match_result.get('patterns', {})\n",
    "        \n",
    "        # Base confidence on match score\n",
    "        confidence = score\n",
    "        \n",
    "        # Adjust confidence based on feature consistency\n",
    "        feature_values = [v for k, v in features.items() \n",
    "                       if k in ['string_similarity', 'token_set_similarity', \n",
    "                              'semantic_similarity', 'contains_score']]\n",
    "        \n",
    "        if feature_values:\n",
    "            # Reduce confidence if features disagree significantly\n",
    "            feature_std = np.std(feature_values) if len(feature_values) > 1 else 0\n",
    "            confidence *= (1 - 0.5 * min(feature_std, 0.5))\n",
    "        \n",
    "        # Boost confidence if patterns detected\n",
    "        if patterns:\n",
    "            confidence = min(confidence + 0.1, 1.0)\n",
    "        \n",
    "        # Penalize confidence for extreme length differences\n",
    "        s1_clean = match_result.get('processed_s1', '')\n",
    "        s2_clean = match_result.get('processed_s2', '')\n",
    "        if s1_clean and s2_clean:\n",
    "            len_ratio = min(len(s1_clean), len(s2_clean)) / max(len(s1_clean), len(s2_clean)) if max(len(s1_clean), len(s2_clean)) > 0 else 1\n",
    "            if len_ratio < 0.3:\n",
    "                confidence *= max(0.5, len_ratio + 0.2)\n",
    "        \n",
    "        return confidence\n",
    "    \n",
    "    def optimize_thresholds(self, test_data=None, domain=None):\n",
    "        \"\"\"\n",
    "        Optimize matching thresholds based on test results\n",
    "        \n",
    "        Args:\n",
    "            test_data (DataFrame): Test data with merchant pairs\n",
    "            domain (str): Domain for optimization\n",
    "            \n",
    "        Returns:\n",
    "            dict: Optimized thresholds and performance\n",
    "        \"\"\"\n",
    "        # Use existing test results if no data provided\n",
    "        if test_data is None:\n",
    "            if self.test_results.empty:\n",
    "                raise ValueError(\"No test data provided and no existing test results\")\n",
    "            test_data = self.test_results\n",
    "        else:\n",
    "            test_data = self._validate_test_data(test_data)\n",
    "        \n",
    "        # Ensure ground truth exists\n",
    "        if 'is_match' not in test_data.columns or test_data['is_match'].isna().all():\n",
    "            raise ValueError(\"Test data must contain 'is_match' ground truth for threshold optimization\")\n",
    "        \n",
    "        # Get effective domain\n",
    "        effective_domain = domain or self.pipeline.domain\n",
    "        \n",
    "        self.logger.info(f\"Optimizing thresholds for domain: {effective_domain or 'general'}\")\n",
    "        \n",
    "        # Try multiple thresholds and measure performance\n",
    "        thresholds_to_try = np.arange(0.5, 0.95, 0.05)\n",
    "        threshold_results = []\n",
    "        \n",
    "        for threshold in thresholds_to_try:\n",
    "            # Run test with this threshold\n",
    "            result = self.run_tests(\n",
    "                test_data, \n",
    "                domain=effective_domain,\n",
    "                threshold=threshold,\n",
    "                save_results=False\n",
    "            )\n",
    "            \n",
    "            # Store performance\n",
    "            performance = result['performance']\n",
    "            threshold_results.append({\n",
    "                'threshold': threshold,\n",
    "                'accuracy': performance.get('accuracy', 0),\n",
    "                'precision': performance.get('precision', 0),\n",
    "                'recall': performance.get('recall', 0),\n",
    "                'f1_score': performance.get('f1_score', 0)\n",
    "            })\n",
    "        \n",
    "        # Create thresholds DataFrame\n",
    "        thresholds_df = pd.DataFrame(threshold_results)\n",
    "        \n",
    "        # Find best thresholds for different metrics\n",
    "        best_accuracy = thresholds_df.loc[thresholds_df['accuracy'].idxmax()]\n",
    "        best_f1 = thresholds_df.loc[thresholds_df['f1_score'].idxmax()]\n",
    "        best_precision = thresholds_df.loc[thresholds_df['precision'].idxmax()]\n",
    "        best_recall = thresholds_df.loc[thresholds_df['recall'].idxmax()]\n",
    "        \n",
    "        # Choose balanced threshold (best F1)\n",
    "        optimal_threshold = best_f1['threshold']\n",
    "        \n",
    "        # Set optimal thresholds\n",
    "        optimal_thresholds = {\n",
    "            'high': optimal_threshold + 0.1,\n",
    "            'medium': optimal_threshold,\n",
    "            'low': optimal_threshold - 0.1\n",
    "        }\n",
    "        \n",
    "        # Update matcher thresholds\n",
    "        matcher = self.pipeline._get_merchant_matcher()\n",
    "        matcher.thresholds = optimal_thresholds\n",
    "        \n",
    "        # Save thresholds\n",
    "        self.optimal_thresholds = {\n",
    "            'threshold': optimal_threshold,\n",
    "            'accuracy': best_accuracy['accuracy'],\n",
    "            'f1': best_f1['f1_score'],\n",
    "            'precision': best_precision['precision'],\n",
    "            'recall': best_recall['recall'],\n",
    "            'thresholds': optimal_thresholds,\n",
    "            'all_results': thresholds_df.to_dict('records')\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        thresholds_path = os.path.join(self.output_dir, \"optimal_thresholds.json\")\n",
    "        with open(thresholds_path, 'w') as f:\n",
    "            json.dump(self.optimal_thresholds, f, indent=2)\n",
    "        \n",
    "        # Generate visualization if matplotlib available\n",
    "        try:\n",
    "            self._visualize_threshold_optimization(thresholds_df)\n",
    "        except:\n",
    "            self.logger.warning(\"Could not generate threshold visualization\")\n",
    "        \n",
    "        # Log results\n",
    "        self.logger.info(f\"Optimal thresholds determined:\")\n",
    "        self.logger.info(f\"  High: {optimal_thresholds['high']:.2f}\")\n",
    "        self.logger.info(f\"  Medium: {optimal_thresholds['medium']:.2f}\")\n",
    "        self.logger.info(f\"  Low: {optimal_thresholds['low']:.2f}\")\n",
    "        self.logger.info(f\"  Best F1 Score: {best_f1['f1_score']:.4f}\")\n",
    "        \n",
    "        return self.optimal_thresholds\n",
    "    \n",
    "    def _visualize_threshold_optimization(self, thresholds_df):\n",
    "        \"\"\"Generate visualization for threshold optimization\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # Plot metrics vs thresholds\n",
    "            plt.plot(thresholds_df['threshold'], thresholds_df['accuracy'], 'b-', label='Accuracy')\n",
    "            plt.plot(thresholds_df['threshold'], thresholds_df['precision'], 'g-', label='Precision')\n",
    "            plt.plot(thresholds_df['threshold'], thresholds_df['recall'], 'r-', label='Recall')\n",
    "            plt.plot(thresholds_df['threshold'], thresholds_df['f1_score'], 'k-', label='F1 Score')\n",
    "            \n",
    "            # Mark optimal threshold\n",
    "            optimal_threshold = self.optimal_thresholds['threshold']\n",
    "            optimal_f1 = self.optimal_thresholds['f1']\n",
    "            plt.axvline(x=optimal_threshold, color='m', linestyle='--', alpha=0.5)\n",
    "            plt.scatter([optimal_threshold], [optimal_f1], color='m', s=100, \n",
    "                      label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "            \n",
    "            # Add labels and legend\n",
    "            plt.xlabel('Threshold')\n",
    "            plt.ylabel('Metric Value')\n",
    "            plt.title('Performance Metrics vs. Threshold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Save figure\n",
    "            plt.savefig(os.path.join(self.output_dir, \"threshold_optimization.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not generate threshold visualization: {e}\")\n",
    "    \n",
    "    def analyze_performance(self, by_domain=False, by_case_type=True, by_confidence=True):\n",
    "        \"\"\"\n",
    "        Analyze matcher performance in depth\n",
    "        \n",
    "        Args:\n",
    "            by_domain (bool): Whether to analyze by domain\n",
    "            by_case_type (bool): Whether to analyze by case type\n",
    "            by_confidence (bool): Whether to analyze by confidence level\n",
    "            \n",
    "        Returns:\n",
    "            dict: Detailed performance analysis\n",
    "        \"\"\"\n",
    "        if self.test_results.empty or not self.performance_metrics:\n",
    "            raise ValueError(\"No test results available for analysis\")\n",
    "        \n",
    "        self.logger.info(\"Analyzing matcher performance\")\n",
    "        \n",
    "        analysis = {\n",
    "            'overall': self.performance_metrics,\n",
    "            'by_domain': {},\n",
    "            'by_case_type': {},\n",
    "            'by_confidence': {},\n",
    "            'error_analysis': {}\n",
    "        }\n",
    "        \n",
    "        # Analyze by domain if requested\n",
    "        if by_domain and 'domain' in self.test_results.columns:\n",
    "            domains = self.test_results['domain'].unique()\n",
    "            for domain in domains:\n",
    "                if not pd.isna(domain):\n",
    "                    domain_results = self.test_results[self.test_results['domain'] == domain]\n",
    "                    if 'correct' in domain_results.columns:\n",
    "                        analysis['by_domain'][domain] = {\n",
    "                            'count': len(domain_results),\n",
    "                            'accuracy': domain_results['correct'].mean(),\n",
    "                            'avg_score': domain_results['score'].mean(),\n",
    "                            'avg_confidence': domain_results['confidence'].mean()\n",
    "                        }\n",
    "                        \n",
    "                        # Calculate precision, recall, F1 if expected matches are present\n",
    "                        if 'expected_match' in domain_results.columns:\n",
    "                            tp = sum((domain_results['expected_match'] == 1) & (domain_results['score'] >= 0.7))\n",
    "                            fp = sum((domain_results['expected_match'] == 0) & (domain_results['score'] >= 0.7))\n",
    "                            fn = sum((domain_results['expected_match'] == 1) & (domain_results['score'] < 0.7))\n",
    "                            tn = sum((domain_results['expected_match'] == 0) & (domain_results['score'] < 0.7))\n",
    "                            \n",
    "                            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                            \n",
    "                            analysis['by_domain'][domain].update({\n",
    "                                'precision': precision,\n",
    "                                'recall': recall,\n",
    "                                'f1_score': f1,\n",
    "                                'confusion_matrix': {\n",
    "                                    'true_positives': int(tp),\n",
    "                                    'false_positives': int(fp),\n",
    "                                    'false_negatives': int(fn),\n",
    "                                    'true_negatives': int(tn)\n",
    "                                }\n",
    "                            })\n",
    "        \n",
    "        # Analyze by case type if requested\n",
    "        if by_case_type and 'case_type' in self.test_results.columns:\n",
    "            case_types = self.test_results['case_type'].unique()\n",
    "            for case_type in case_types:\n",
    "                if not pd.isna(case_type):\n",
    "                    case_results = self.test_results[self.test_results['case_type'] == case_type]\n",
    "                    if 'correct' in case_results.columns:\n",
    "                        analysis['by_case_type'][case_type] = {\n",
    "                            'count': len(case_results),\n",
    "                            'accuracy': case_results['correct'].mean(),\n",
    "                            'avg_score': case_results['score'].mean(),\n",
    "                            'avg_confidence': case_results['confidence'].mean()\n",
    "                        }\n",
    "                        \n",
    "                        # Identify common failure patterns\n",
    "                        if len(case_results) > 0 and 'correct' in case_results.columns:\n",
    "                            failures = case_results[~case_results['correct']]\n",
    "                            if len(failures) > 0:\n",
    "                                analysis['by_case_type'][case_type]['failure_analysis'] = {\n",
    "                                    'failure_count': len(failures),\n",
    "                                    'failure_rate': len(failures) / len(case_results),\n",
    "                                    'avg_failure_score': failures['score'].mean(),\n",
    "                                    'example_failures': failures.head(3)[['s1', 's2', 'score', 'expected_match']].to_dict('records')\n",
    "                                }\n",
    "        \n",
    "        # Analyze by confidence level if requested\n",
    "        if by_confidence and 'confidence' in self.test_results.columns:\n",
    "            # Create confidence bins\n",
    "            self.test_results['confidence_bin'] = pd.cut(\n",
    "                self.test_results['confidence'], \n",
    "                bins=[0, 0.5, 0.7, 0.85, 0.95, 1.0], \n",
    "                labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "            )\n",
    "            \n",
    "            confidence_bins = self.test_results['confidence_bin'].unique()\n",
    "            for confidence_bin in confidence_bins:\n",
    "                if not pd.isna(confidence_bin):\n",
    "                    bin_results = self.test_results[self.test_results['confidence_bin'] == confidence_bin]\n",
    "                    if 'correct' in bin_results.columns:\n",
    "                        analysis['by_confidence'][str(confidence_bin)] = {\n",
    "                            'count': len(bin_results),\n",
    "                            'accuracy': bin_results['correct'].mean(),\n",
    "                            'avg_score': bin_results['score'].mean()\n",
    "                        }\n",
    "                        \n",
    "                        # Calculate reliability ratio (how well confidence predicts accuracy)\n",
    "                        if len(bin_results) > 0 and 'correct' in bin_results.columns:\n",
    "                            avg_confidence = bin_results['confidence'].mean()\n",
    "                            accuracy = bin_results['correct'].mean()\n",
    "                            reliability_ratio = min(avg_confidence, accuracy) / max(avg_confidence, accuracy) if max(avg_confidence, accuracy) > 0 else 0\n",
    "                            \n",
    "                            analysis['by_confidence'][str(confidence_bin)]['reliability_ratio'] = reliability_ratio\n",
    "        \n",
    "        # Error analysis - identify common patterns in false positives and negatives\n",
    "        if 'expected_match' in self.test_results.columns and 'score' in self.test_results.columns:\n",
    "            # Get false positives (matched but shouldn't have)\n",
    "            false_positives = self.test_results[\n",
    "                (self.test_results['expected_match'] == 0) & \n",
    "                (self.test_results['score'] >= 0.7)\n",
    "            ]\n",
    "            \n",
    "            # Get false negatives (didn't match but should have)\n",
    "            false_negatives = self.test_results[\n",
    "                (self.test_results['expected_match'] == 1) & \n",
    "                (self.test_results['score'] < 0.7)\n",
    "            ]\n",
    "            \n",
    "            # Analyze false positives\n",
    "            if len(false_positives) > 0:\n",
    "                analysis['error_analysis']['false_positives'] = {\n",
    "                    'count': len(false_positives),\n",
    "                    'avg_score': false_positives['score'].mean(),\n",
    "                    'avg_confidence': false_positives['confidence'].mean(),\n",
    "                    'examples': false_positives.head(5)[['s1', 's2', 'score', 'confidence']].to_dict('records')\n",
    "                }\n",
    "                \n",
    "                # Analyze by case type if available\n",
    "                if 'case_type' in false_positives.columns:\n",
    "                    type_counts = false_positives['case_type'].value_counts()\n",
    "                    analysis['error_analysis']['false_positives']['case_type_distribution'] = type_counts.to_dict()\n",
    "            \n",
    "            # Analyze false negatives\n",
    "            if len(false_negatives) > 0:\n",
    "                analysis['error_analysis']['false_negatives'] = {\n",
    "                    'count': len(false_negatives),\n",
    "                    'avg_score': false_negatives['score'].mean(),\n",
    "                    'avg_confidence': false_negatives['confidence'].mean(),\n",
    "                    'examples': false_negatives.head(5)[['s1', 's2', 'score', 'confidence']].to_dict('records')\n",
    "                }\n",
    "                \n",
    "                # Analyze by case type if available\n",
    "                if 'case_type' in false_negatives.columns:\n",
    "                    type_counts = false_negatives['case_type'].value_counts()\n",
    "                    analysis['error_analysis']['false_negatives']['case_type_distribution'] = type_counts.to_dict()\n",
    "        \n",
    "        # Save analysis results\n",
    "        analysis_path = os.path.join(self.output_dir, \"performance_analysis.json\")\n",
    "        with open(analysis_path, 'w') as f:\n",
    "            json.dump(analysis, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Performance analysis saved to {analysis_path}\")\n",
    "        return analysis\n",
    "    \n",
    "    def generate_report(self, output_format='html', include_visualizations=True):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive report of test results and analysis\n",
    "        \n",
    "        Args:\n",
    "            output_format (str): Report format ('html', 'pdf', 'markdown')\n",
    "            include_visualizations (bool): Include visualizations in report\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to generated report\n",
    "        \"\"\"\n",
    "        if self.test_results.empty:\n",
    "            raise ValueError(\"No test results available for report generation\")\n",
    "        \n",
    "        self.logger.info(f\"Generating {output_format} report\")\n",
    "        \n",
    "        # Prepare report data\n",
    "        report_data = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'test_summary': {\n",
    "                'total_tests': len(self.test_results),\n",
    "                'performance_metrics': self.performance_metrics,\n",
    "                'optimal_thresholds': self.optimal_thresholds\n",
    "            },\n",
    "            'test_results': self.test_results.to_dict('records')[:100]  # Limit to 100 examples\n",
    "        }\n",
    "        \n",
    "        # Generate visualizations if requested\n",
    "        if include_visualizations:\n",
    "            visualization_paths = self._generate_visualizations()\n",
    "            report_data['visualizations'] = visualization_paths\n",
    "        \n",
    "        # Create report based on format\n",
    "        if output_format == 'html':\n",
    "            report_path = self._generate_html_report(report_data)\n",
    "        elif output_format == 'markdown':\n",
    "            report_path = self._generate_markdown_report(report_data)\n",
    "        elif output_format == 'pdf':\n",
    "            try:\n",
    "                # Try to convert HTML to PDF if library available\n",
    "                report_path = self._generate_html_report(report_data)\n",
    "                pdf_path = report_path.replace('.html', '.pdf')\n",
    "                \n",
    "                try:\n",
    "                    import weasyprint\n",
    "                    html = weasyprint.HTML(filename=report_path)\n",
    "                    html.write_pdf(pdf_path)\n",
    "                    report_path = pdf_path\n",
    "                except ImportError:\n",
    "                    self.logger.warning(\"WeasyPrint not available. Generated HTML report instead.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to generate PDF report: {e}\")\n",
    "                report_path = self._generate_html_report(report_data)\n",
    "        else:\n",
    "            # Default to HTML\n",
    "            report_path = self._generate_html_report(report_data)\n",
    "        \n",
    "        self.logger.info(f\"Report generated at {report_path}\")\n",
    "        return report_path\n",
    "    \n",
    "    def _generate_visualizations(self):\n",
    "        \"\"\"Generate visualizations for report\"\"\"\n",
    "        visualization_paths = []\n",
    "        \n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import seaborn as sns\n",
    "            \n",
    "            # Create output directory for visualizations\n",
    "            viz_dir = os.path.join(self.output_dir, \"visualizations\")\n",
    "            os.makedirs(viz_dir, exist_ok=True)\n",
    "            \n",
    "            # 1. Score distribution plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            if 'score' in self.test_results.columns and 'expected_match' in self.test_results.columns:\n",
    "                sns.histplot(\n",
    "                    data=self.test_results, \n",
    "                    x='score', \n",
    "                    hue='expected_match',\n",
    "                    bins=20,\n",
    "                    kde=True\n",
    "                )\n",
    "                plt.title('Score Distribution by Expected Match')\n",
    "                plt.xlabel('Match Score')\n",
    "                plt.ylabel('Count')\n",
    "                plt.legend(['Not a Match', 'Match'])\n",
    "                \n",
    "                score_dist_path = os.path.join(viz_dir, \"score_distribution.png\")\n",
    "                plt.savefig(score_dist_path)\n",
    "                plt.close()\n",
    "                visualization_paths.append(score_dist_path)\n",
    "            \n",
    "            # 2. Confusion matrix heatmap\n",
    "            if self.confusion_matrix:\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                cm_data = np.array([\n",
    "                    [self.confusion_matrix['true_negatives'], self.confusion_matrix['false_positives']],\n",
    "                    [self.confusion_matrix['false_negatives'], self.confusion_matrix['true_positives']]\n",
    "                ])\n",
    "                \n",
    "                sns.heatmap(\n",
    "                    cm_data, \n",
    "                    annot=True, \n",
    "                    fmt='d', \n",
    "                    cmap='Blues',\n",
    "                    xticklabels=['Predicted No Match', 'Predicted Match'],\n",
    "                    yticklabels=['Actual No Match', 'Actual Match']\n",
    "                )\n",
    "                plt.title('Confusion Matrix')\n",
    "                \n",
    "                cm_path = os.path.join(viz_dir, \"confusion_matrix.png\")\n",
    "                plt.savefig(cm_path)\n",
    "                plt.close()\n",
    "                visualization_paths.append(cm_path)\n",
    "            \n",
    "            # 3. Confidence vs. Accuracy plot\n",
    "            if 'confidence' in self.test_results.columns and 'correct' in self.test_results.columns:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                # Group by confidence bins\n",
    "                self.test_results['confidence_bin'] = pd.cut(\n",
    "                    self.test_results['confidence'], \n",
    "                    bins=10\n",
    "                )\n",
    "                \n",
    "                conf_acc = self.test_results.groupby('confidence_bin')['correct'].mean().reset_index()\n",
    "                conf_acc['bin_center'] = conf_acc['confidence_bin'].apply(lambda x: x.mid)\n",
    "                \n",
    "                plt.plot(conf_acc['bin_center'], conf_acc['correct'], 'o-', linewidth=2)\n",
    "                plt.plot([0, 1], [0, 1], 'k--')  # Ideal calibration line\n",
    "                \n",
    "                plt.title('Confidence Calibration Plot')\n",
    "                plt.xlabel('Confidence')\n",
    "                plt.ylabel('Accuracy')\n",
    "                plt.grid(alpha=0.3)\n",
    "                \n",
    "                calib_path = os.path.join(viz_dir, \"confidence_calibration.png\")\n",
    "                plt.savefig(calib_path)\n",
    "                plt.close()\n",
    "                visualization_paths.append(calib_path)\n",
    "            \n",
    "            # 4. Case type performance\n",
    "            if 'case_type' in self.test_results.columns and 'correct' in self.test_results.columns:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                case_perf = self.test_results.groupby('case_type')['correct'].mean().sort_values()\n",
    "                \n",
    "                colors = ['g' if x > 0.8 else 'y' if x > 0.6 else 'r' for x in case_perf]\n",
    "                \n",
    "                case_perf.plot(kind='barh', color=colors)\n",
    "                plt.title('Accuracy by Case Type')\n",
    "                plt.xlabel('Accuracy')\n",
    "                plt.ylabel('Case Type')\n",
    "                plt.xlim(0, 1)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                case_path = os.path.join(viz_dir, \"case_type_performance.png\")\n",
    "                plt.savefig(case_path)\n",
    "                plt.close()\n",
    "                visualization_paths.append(case_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to generate visualizations: {e}\")\n",
    "        \n",
    "        return visualization_paths\n",
    "    \n",
    "    def _generate_html_report(self, report_data):\n",
    "        \"\"\"Generate HTML report from data\"\"\"\n",
    "        report_path = os.path.join(self.output_dir, \"merchant_matcher_report.html\")\n",
    "        \n",
    "        # Basic HTML template\n",
    "        html_template = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Merchant Matcher Test Report</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; line-height: 1.6; color: #333; max-width: 1200px; margin: 0 auto; padding: 20px; }\n",
    "                h1, h2, h3 { color: #2c3e50; }\n",
    "                .header { background-color: #2c3e50; color: white; padding: 20px; margin-bottom: 20px; }\n",
    "                .section { margin-bottom: 30px; background-color: #f9f9f9; padding: 20px; border-radius: 5px; }\n",
    "                table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }\n",
    "                th, td { border: 1px solid #ddd; padding: 12px; }\n",
    "                th { background-color: #f2f2f2; text-align: left; }\n",
    "                tr:nth-child(even) { background-color: #f9f9f9; }\n",
    "                .metrics { display: flex; flex-wrap: wrap; gap: 20px; margin-bottom: 20px; }\n",
    "                .metric-card { background-color: white; border-radius: 5px; padding: 15px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); flex: 1; min-width: 150px; }\n",
    "                .metric-value { font-size: 24px; font-weight: bold; color: #2c3e50; }\n",
    "                .visualizations { display: flex; flex-wrap: wrap; gap: 20px; }\n",
    "                .visualization { max-width: 100%; height: auto; }\n",
    "                .visualization img { max-width: 100%; height: auto; border: 1px solid #ddd; }\n",
    "                pre { background-color: #f5f5f5; padding: 10px; border-radius: 5px; overflow-x: auto; }\n",
    "                .good { color: green; }\n",
    "                .medium { color: orange; }\n",
    "                .poor { color: red; }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"header\">\n",
    "                <h1>Merchant Matcher Test Report</h1>\n",
    "                <p>Generated on: {timestamp}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>Summary</h2>\n",
    "                <div class=\"metrics\">\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div>Accuracy</div>\n",
    "                        <div class=\"metric-value {accuracy_class}\">{accuracy:.2%}</div>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div>Precision</div>\n",
    "                        <div class=\"metric-value {precision_class}\">{precision:.2%}</div>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div>Recall</div>\n",
    "                        <div class=\"metric-value {recall_class}\">{recall:.2%}</div>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div>F1 Score</div>\n",
    "                        <div class=\"metric-value {f1_class}\">{f1:.2%}</div>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div>Total Tests</div>\n",
    "                        <div class=\"metric-value\">{total_tests}</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <h3>Confusion Matrix</h3>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th></th>\n",
    "                        <th>Predicted No Match</th>\n",
    "                        <th>Predicted Match</th>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <th>Actual No Match</th>\n",
    "                        <td>{true_negatives}</td>\n",
    "                        <td>{false_positives}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <th>Actual Match</th>\n",
    "                        <td>{false_negatives}</td>\n",
    "                        <td>{true_positives}</td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "                \n",
    "                <h3>Optimal Thresholds</h3>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Level</th>\n",
    "                        <th>Threshold</th>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>High</td>\n",
    "                        <td>{high_threshold:.2f}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Medium</td>\n",
    "                        <td>{medium_threshold:.2f}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Low</td>\n",
    "                        <td>{low_threshold:.2f}</td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>Visualizations</h2>\n",
    "                <div class=\"visualizations\">\n",
    "                    {visualizations_html}\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>Performance by Case Type</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Case Type</th>\n",
    "                        <th>Count</th>\n",
    "                        <th>Accuracy</th>\n",
    "                        <th>Avg Score</th>\n",
    "                    </tr>\n",
    "                    {case_type_rows}\n",
    "                </table>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>Sample Test Results</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Merchant 1</th>\n",
    "                        <th>Merchant 2</th>\n",
    "                        <th>Score</th>\n",
    "                        <th>Confidence</th>\n",
    "                        <th>Expected</th>\n",
    "                        <th>Match Level</th>\n",
    "                    </tr>\n",
    "                    {test_result_rows}\n",
    "                </table>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>Error Analysis</h2>\n",
    "                <h3>False Positives (Incorrectly Matched)</h3>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Merchant 1</th>\n",
    "                        <th>Merchant 2</th>\n",
    "                        <th>Score</th>\n",
    "                        <th>Confidence</th>\n",
    "                    </tr>\n",
    "                    {false_positive_rows}\n",
    "                </table>\n",
    "                \n",
    "                <h3>False Negatives (Incorrectly Not Matched)</h3>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Merchant 1</th>\n",
    "                        <th>Merchant 2</th>\n",
    "                        <th>Score</th>\n",
    "                        <th>Confidence</th>\n",
    "                    </tr>\n",
    "                    {false_negative_rows}\n",
    "                </table>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract metrics\n",
    "        metrics = report_data['test_summary']['performance_metrics']\n",
    "        \n",
    "        accuracy = metrics.get('accuracy', 0)\n",
    "        precision = metrics.get('precision', 0)\n",
    "        recall = metrics.get('recall', 0)\n",
    "        f1 = metrics.get('f1_score', 0)\n",
    "        \n",
    "        accuracy_class = 'good' if accuracy >= 0.9 else 'medium' if accuracy >= 0.7 else 'poor'\n",
    "        precision_class = 'good' if precision >= 0.9 else 'medium' if precision >= 0.7 else 'poor'\n",
    "        recall_class = 'good' if recall >= 0.9 else 'medium' if recall >= 0.7 else 'poor'\n",
    "        f1_class = 'good' if f1 >= 0.9 else 'medium' if f1 >= 0.7 else 'poor'\n",
    "        \n",
    "        # Generate visualizations HTML\n",
    "        visualizations_html = \"\"\n",
    "        if 'visualizations' in report_data:\n",
    "            for viz_path in report_data['visualizations']:\n",
    "                # Get relative path\n",
    "                rel_path = os.path.relpath(viz_path, self.output_dir)\n",
    "                visualizations_html += f\"\"\"\n",
    "                <div class=\"visualization\">\n",
    "                    <img src=\"{rel_path}\" alt=\"Visualization\">\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        # Generate case type rows\n",
    "        case_type_rows = \"\"\n",
    "        if 'case_type_performance' in metrics:\n",
    "            for case_type, case_data in metrics['case_type_performance'].items():\n",
    "                acc_class = 'good' if case_data['accuracy'] >= 0.9 else 'medium' if case_data['accuracy'] >= 0.7 else 'poor'\n",
    "                case_type_rows += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{case_type}</td>\n",
    "                    <td>{case_data['count']}</td>\n",
    "                    <td class=\"{acc_class}\">{case_data['accuracy']:.2%}</td>\n",
    "                    <td>{case_data['avg_score']:.2f}</td>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        # Generate test result rows\n",
    "        test_result_rows = \"\"\n",
    "        for result in report_data['test_results'][:20]:  # Show first 20\n",
    "            expected = result.get('expected_match')\n",
    "            expected_str = 'Yes' if expected == 1 else 'No' if expected == 0 else 'Unknown'\n",
    "            test_result_rows += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{result['s1']}</td>\n",
    "                <td>{result['s2']}</td>\n",
    "                <td>{result['score']:.2f}</td>\n",
    "                <td>{result.get('confidence', 0):.2f}</td>\n",
    "                <td>{expected_str}</td>\n",
    "                <td>{result['level']}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Generate false positive and negative rows\n",
    "        false_positive_rows = \"\"\n",
    "        false_negative_rows = \"\"\n",
    "        \n",
    "        if 'error_analysis' in metrics and 'false_positives' in metrics['error_analysis']:\n",
    "            for fp in metrics['error_analysis']['false_positives']['examples'][:10]:\n",
    "                false_positive_rows += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{fp['s1']}</td>\n",
    "                    <td>{fp['s2']}</td>\n",
    "                    <td>{fp['score']:.2f}</td>\n",
    "                    <td>{fp.get('confidence', 0):.2f}</td>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        if 'error_analysis' in metrics and 'false_negatives' in metrics['error_analysis']:\n",
    "            for fn in metrics['error_analysis']['false_negatives']['examples'][:10]:\n",
    "                false_negative_rows += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{fn['s1']}</td>\n",
    "                    <td>{fn['s2']}</td>\n",
    "                    <td>{fn['score']:.2f}</td>\n",
    "                    <td>{fn.get('confidence', 0):.2f}</td>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        # Extract confusion matrix data\n",
    "        true_positives = metrics.get('true_positives', 0)\n",
    "        false_positives = metrics.get('false_positives', 0)\n",
    "        false_negatives = metrics.get('false_negatives', 0)\n",
    "        true_negatives = metrics.get('true_negatives', 0)\n",
    "        \n",
    "        # Extract threshold data\n",
    "        thresholds = report_data['test_summary'].get('optimal_thresholds', {}).get('thresholds', {})\n",
    "        high_threshold = thresholds.get('high', 0.85)\n",
    "        medium_threshold = thresholds.get('medium', 0.75)\n",
    "        low_threshold = thresholds.get('low', 0.60)\n",
    "        \n",
    "        # Format the HTML\n",
    "        formatted_html = html_template.format(\n",
    "            timestamp=report_data['timestamp'],\n",
    "            accuracy=accuracy,\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            f1=f1,\n",
    "            total_tests=report_data['test_summary']['total_tests'],\n",
    "            true_positives=true_positives,\n",
    "            false_positives=false_positives,\n",
    "            false_negatives=false_negatives,\n",
    "            true_negatives=true_negatives,\n",
    "            high_threshold=high_threshold,\n",
    "            medium_threshold=medium_threshold,\n",
    "            low_threshold=low_threshold,\n",
    "            visualizations_html=visualizations_html,\n",
    "            case_type_rows=case_type_rows,\n",
    "            test_result_rows=test_result_rows,\n",
    "            false_positive_rows=false_positive_rows,\n",
    "            false_negative_rows=false_negative_rows,\n",
    "            accuracy_class=accuracy_class,\n",
    "            precision_class=precision_class,\n",
    "            recall_class=recall_class,\n",
    "            f1_class=f1_class\n",
    "        )\n",
    "        \n",
    "        # Write to file\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(formatted_html)\n",
    "        \n",
    "        return report_path\n",
    "    \n",
    "    def _generate_markdown_report(self, report_data):\n",
    "        \"\"\"Generate Markdown report from data\"\"\"\n",
    "        report_path = os.path.join(self.output_dir, \"merchant_matcher_report.md\")\n",
    "        \n",
    "        # Extract metrics\n",
    "        metrics = report_data['test_summary']['performance_metrics']\n",
    "        \n",
    "        accuracy = metrics.get('accuracy', 0)\n",
    "        precision = metrics.get('precision', 0)\n",
    "        recall = metrics.get('recall', 0)\n",
    "        f1 = metrics.get('f1_score', 0)\n",
    "        \n",
    "        # Extract confusion matrix data\n",
    "        true_positives = metrics.get('true_positives', 0)\n",
    "        false_positives = metrics.get('false_positives', 0)\n",
    "        false_negatives = metrics.get('false_negatives', 0)\n",
    "        true_negatives = metrics.get('true_negatives', 0)\n",
    "        \n",
    "        # Extract threshold data\n",
    "        thresholds = report_data['test_summary'].get('optimal_thresholds', {}).get('thresholds', {})\n",
    "        high_threshold = thresholds.get('high', 0.85)\n",
    "        medium_threshold = thresholds.get('medium', 0.75)\n",
    "        low_threshold = thresholds.get('low', 0.60)\n",
    "        \n",
    "        # Markdown content\n",
    "        markdown_content = f\"\"\"\n",
    "        # Merchant Matcher Test Report\n",
    "    \n",
    "        *Generated on: {report_data['timestamp']}*\n",
    "    \n",
    "        ## Summary\n",
    "    \n",
    "        - **Total Tests:** {report_data['test_summary']['total_tests']}\n",
    "        - **Accuracy:** {accuracy:.2%}\n",
    "        - **Precision:** {precision:.2%}\n",
    "        - **Recall:** {recall:.2%}\n",
    "        - **F1 Score:** {f1:.2%}\n",
    "    \n",
    "        ### Confusion Matrix\n",
    "    \n",
    "        |                  | Predicted No Match | Predicted Match |\n",
    "        |------------------|------------------|----------------|\n",
    "        | **Actual No Match** | {true_negatives} | {false_positives} |\n",
    "        | **Actual Match**    | {false_negatives} | {true_positives} |\n",
    "    \n",
    "        ### Optimal Thresholds\n",
    "    \n",
    "        | Level  | Threshold |\n",
    "        |--------|-----------|\n",
    "        | High   | {high_threshold:.2f} |\n",
    "        | Medium | {medium_threshold:.2f} |\n",
    "        | Low    | {low_threshold:.2f} |\n",
    "    \n",
    "        ## Performance by Case Type\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        # Add case type performance\n",
    "        if 'case_type_performance' in metrics:\n",
    "            markdown_content += \"\"\"\n",
    "        | Case Type | Count | Accuracy | Avg Score |\n",
    "        |-----------|-------|----------|-----------|\n",
    "        \"\"\"\n",
    "            for case_type, case_data in metrics['case_type_performance'].items():\n",
    "                markdown_content += f\"| {case_type} | {case_data['count']} | {case_data['accuracy']:.2%} | {case_data['avg_score']:.2f} |\\n\"\n",
    "        \n",
    "        # Add sample test results\n",
    "        markdown_content += \"\"\"\n",
    "        ## Sample Test Results\n",
    "    \n",
    "        | Merchant 1 | Merchant 2 | Score | Confidence | Expected | Match Level |\n",
    "        |------------|------------|-------|------------|----------|-------------|\n",
    "        \"\"\"\n",
    "        \n",
    "        for result in report_data['test_results'][:10]:\n",
    "            expected = result.get('expected_match')\n",
    "            expected_str = 'Yes' if expected == 1 else 'No' if expected == 0 else 'Unknown'\n",
    "            markdown_content += f\"| {result['s1']} | {result['s2']} | {result['score']:.2f} | {result.get('confidence', 0):.2f} | {expected_str} | {result['level']} |\\n\"\n",
    "        \n",
    "        # Add error analysis\n",
    "        markdown_content += \"\"\"\n",
    "        ## Error Analysis\n",
    "    \n",
    "        ### False Positives (Incorrectly Matched)\n",
    "    \n",
    "        | Merchant 1 | Merchant 2 | Score | Confidence |\n",
    "        |------------|------------|-------|------------|\n",
    "        \"\"\"\n",
    "        \n",
    "        if 'error_analysis' in metrics and 'false_positives' in metrics['error_analysis']:\n",
    "            for fp in metrics['error_analysis']['false_positives']['examples'][:5]:\n",
    "                markdown_content += f\"| {fp['s1']} | {fp['s2']} | {fp['score']:.2f} | {fp.get('confidence', 0):.2f} |\\n\"\n",
    "        \n",
    "        markdown_content += \"\"\"\n",
    "        ### False Negatives (Incorrectly Not Matched)\n",
    "    \n",
    "        | Merchant 1 | Merchant 2 | Score | Confidence |\n",
    "        |------------|------------|-------|------------|\n",
    "        \"\"\"\n",
    "        \n",
    "        if 'error_analysis' in metrics and 'false_negatives' in metrics['error_analysis']:\n",
    "            for fn in metrics['error_analysis']['false_negatives']['examples'][:5]:\n",
    "                markdown_content += f\"| {fn['s1']} | {fn['s2']} | {fn['score']:.2f} | {fn.get('confidence', 0):.2f} |\\n\"\n",
    "        \n",
    "        # Write to file\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(markdown_content)\n",
    "        \n",
    "        return report_path\n",
    "    \n",
    "    def export_results(self, format='excel', include_explanations=True):\n",
    "        \"\"\"\n",
    "        Export test results to various formats\n",
    "        \n",
    "        Args:\n",
    "            format (str): Export format ('excel', 'csv', 'json')\n",
    "            include_explanations (bool): Include detailed explanations\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to exported results\n",
    "        \"\"\"\n",
    "        if self.test_results.empty:\n",
    "            raise ValueError(\"No test results available for export\")\n",
    "        \n",
    "        self.logger.info(f\"Exporting results to {format}\")\n",
    "        \n",
    "        # Prepare results for export\n",
    "        export_df = self.test_results.copy()\n",
    "        \n",
    "        # Truncate explanations if they are too long\n",
    "        if 'explanation' in export_df.columns and include_explanations:\n",
    "            export_df['explanation'] = export_df['explanation'].apply(\n",
    "                lambda x: x[:500] + '...' if isinstance(x, str) and len(x) > 500 else x\n",
    "            )\n",
    "        elif 'explanation' in export_df.columns and not include_explanations:\n",
    "            export_df = export_df.drop(columns=['explanation'])\n",
    "        \n",
    "        # Export based on format\n",
    "        if format == 'excel':\n",
    "            export_path = os.path.join(self.output_dir, \"merchant_matcher_results.xlsx\")\n",
    "            \n",
    "            # Create Excel writer\n",
    "            try:\n",
    "                import openpyxl\n",
    "                from openpyxl.styles import PatternFill, Font, Alignment\n",
    "                from openpyxl.utils import get_column_letter\n",
    "                \n",
    "                # Create writer\n",
    "                writer = pd.ExcelWriter(export_path, engine='openpyxl')\n",
    "                \n",
    "                # Write main results\n",
    "                export_df.to_excel(writer, sheet_name='Test Results', index=False)\n",
    "                \n",
    "                # Add summary sheet\n",
    "                summary_data = pd.DataFrame([{\n",
    "                    'Metric': 'Accuracy',\n",
    "                    'Value': self.performance_metrics.get('accuracy', 0)\n",
    "                }, {\n",
    "                    'Metric': 'Precision',\n",
    "                    'Value': self.performance_metrics.get('precision', 0)\n",
    "                }, {\n",
    "                    'Metric': 'Recall',\n",
    "                    'Value': self.performance_metrics.get('recall', 0)\n",
    "                }, {\n",
    "                    'Metric': 'F1 Score',\n",
    "                    'Value': self.performance_metrics.get('f1_score', 0)\n",
    "                }, {\n",
    "                    'Metric': 'Total Tests',\n",
    "                    'Value': len(export_df)\n",
    "                }])\n",
    "                \n",
    "                summary_data.to_excel(writer, sheet_name='Summary', index=False)\n",
    "                \n",
    "                # Access workbook\n",
    "                workbook = writer.book\n",
    "                \n",
    "                # Format Test Results sheet\n",
    "                if 'Test Results' in workbook.sheetnames:\n",
    "                    sheet = workbook['Test Results']\n",
    "                    \n",
    "                    # Format header\n",
    "                    for col in range(1, sheet.max_column + 1):\n",
    "                        cell = sheet.cell(row=1, column=col)\n",
    "                        cell.font = Font(bold=True)\n",
    "                        cell.fill = PatternFill(start_color=\"DDDDDD\", end_color=\"DDDDDD\", fill_type=\"solid\")\n",
    "                    \n",
    "                    # Auto-size columns\n",
    "                    for col in range(1, sheet.max_column + 1):\n",
    "                        column_letter = get_column_letter(col)\n",
    "                        sheet.column_dimensions[column_letter].width = 15\n",
    "                    \n",
    "                    # Add conditional formatting for score column\n",
    "                    score_col = None\n",
    "                    for col in range(1, sheet.max_column + 1):\n",
    "                        if sheet.cell(row=1, column=col).value == 'score':\n",
    "                            score_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if score_col:\n",
    "                        for row in range(2, sheet.max_row + 1):\n",
    "                            cell = sheet.cell(row=row, column=score_col)\n",
    "                            score = cell.value if cell.value is not None else 0\n",
    "                            \n",
    "                            if score >= 0.85:\n",
    "                                cell.fill = PatternFill(start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\")\n",
    "                            elif score >= 0.7:\n",
    "                                cell.fill = PatternFill(start_color=\"FFEB9C\", end_color=\"FFEB9C\", fill_type=\"solid\")\n",
    "                            else:\n",
    "                                cell.fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "                \n",
    "                # Format Summary sheet\n",
    "                if 'Summary' in workbook.sheetnames:\n",
    "                    sheet = workbook['Summary']\n",
    "                    \n",
    "                    # Format header\n",
    "                    for col in range(1, sheet.max_column + 1):\n",
    "                        cell = sheet.cell(row=1, column=col)\n",
    "                        cell.font = Font(bold=True)\n",
    "                        cell.fill = PatternFill(start_color=\"DDDDDD\", end_color=\"DDDDDD\", fill_type=\"solid\")\n",
    "                    \n",
    "                    # Format values\n",
    "                    for row in range(2, sheet.max_row + 1):\n",
    "                        metric_cell = sheet.cell(row=row, column=1)\n",
    "                        value_cell = sheet.cell(row=row, column=2)\n",
    "                        \n",
    "                        metric_cell.font = Font(bold=True)\n",
    "                        \n",
    "                        # Format percentage values\n",
    "                        if metric_cell.value in ['Accuracy', 'Precision', 'Recall', 'F1 Score']:\n",
    "                            value = value_cell.value if value_cell.value is not None else 0\n",
    "                            \n",
    "                            if value >= 0.9:\n",
    "                                value_cell.fill = PatternFill(start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\")\n",
    "                            elif value >= 0.7:\n",
    "                                value_cell.fill = PatternFill(start_color=\"FFEB9C\", end_color=\"FFEB9C\", fill_type=\"solid\")\n",
    "                            else:\n",
    "                                value_cell.fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "                \n",
    "                # Save workbook\n",
    "                writer.close()\n",
    "                \n",
    "            except ImportError:\n",
    "                # Fallback to basic Excel export\n",
    "                export_df.to_excel(export_path, index=False)\n",
    "                \n",
    "        elif format == 'csv':\n",
    "            export_path = os.path.join(self.output_dir, \"merchant_matcher_results.csv\")\n",
    "            export_df.to_csv(export_path, index=False)\n",
    "            \n",
    "        elif format == 'json':\n",
    "            export_path = os.path.join(self.output_dir, \"merchant_matcher_results.json\")\n",
    "            \n",
    "            # Create JSON with results and metrics\n",
    "            export_data = {\n",
    "                'results': export_df.to_dict('records'),\n",
    "                'performance_metrics': self.performance_metrics,\n",
    "                'optimal_thresholds': self.optimal_thresholds\n",
    "            }\n",
    "            \n",
    "            with open(export_path, 'w') as f:\n",
    "                json.dump(export_data, f, indent=2)\n",
    "        else:\n",
    "            # Default to CSV\n",
    "            export_path = os.path.join(self.output_dir, \"merchant_matcher_results.csv\")\n",
    "            export_df.to_csv(export_path, index=False)\n",
    "        \n",
    "        self.logger.info(f\"Results exported to {export_path}\")\n",
    "        return export_path\n",
    "    \n",
    "    def calibrate_confidence(self):\n",
    "        \"\"\"\n",
    "        Calibrate confidence scores based on test results\n",
    "        \n",
    "        Returns:\n",
    "            dict: Calibration parameters and metrics\n",
    "        \"\"\"\n",
    "        if self.test_results.empty or 'confidence' not in self.test_results.columns:\n",
    "            raise ValueError(\"Test results with confidence scores required for calibration\")\n",
    "        \n",
    "        self.logger.info(\"Calibrating confidence scores\")\n",
    "        \n",
    "        # Check if we have ground truth\n",
    "        has_ground_truth = 'expected_match' in self.test_results.columns and not self.test_results['expected_match'].isna().all()\n",
    "        \n",
    "        if not has_ground_truth:\n",
    "            self.logger.warning(\"Ground truth not available. Skipping calibration.\")\n",
    "            return None\n",
    "        \n",
    "        # Create confidence bins\n",
    "        self.test_results['confidence_bin'] = pd.cut(\n",
    "            self.test_results['confidence'], \n",
    "            bins=10\n",
    "        )\n",
    "        \n",
    "        # Calculate actual accuracy in each bin\n",
    "        confidence_calibration = self.test_results.groupby('confidence_bin').agg({\n",
    "            'confidence': 'mean',\n",
    "            'correct': 'mean',\n",
    "            'expected_match': 'count'\n",
    "        }).rename(columns={\n",
    "            'confidence': 'avg_confidence',\n",
    "            'correct': 'actual_accuracy',\n",
    "            'expected_match': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Calculate calibration metrics\n",
    "        calibration_error = np.mean(np.abs(confidence_calibration['avg_confidence'] - confidence_calibration['actual_accuracy']))\n",
    "        \n",
    "        # Calculate scaling parameters for logistic calibration\n",
    "        from scipy.optimize import curve_fit\n",
    "        \n",
    "        def logistic_function(x, a, b):\n",
    "            return 1 / (1 + np.exp(-(a * x + b)))\n",
    "        \n",
    "        try:\n",
    "            # Fit logistic function to confidence-accuracy relationship\n",
    "            params, _ = curve_fit(\n",
    "                logistic_function, \n",
    "                confidence_calibration['avg_confidence'], \n",
    "                confidence_calibration['actual_accuracy'],\n",
    "                p0=[1, 0]\n",
    "            )\n",
    "            \n",
    "            a, b = params\n",
    "            \n",
    "            # Calculate calibrated confidence\n",
    "            def calibrate(confidence):\n",
    "                return logistic_function(confidence, a, b)\n",
    "            \n",
    "            # Apply calibration to test results\n",
    "            self.test_results['calibrated_confidence'] = self.test_results['confidence'].apply(calibrate)\n",
    "            \n",
    "            # Calculate new calibration error\n",
    "            self.test_results['calibrated_confidence_bin'] = pd.cut(\n",
    "                self.test_results['calibrated_confidence'], \n",
    "                bins=10\n",
    "            )\n",
    "            \n",
    "            calibrated_metrics = self.test_results.groupby('calibrated_confidence_bin').agg({\n",
    "                'calibrated_confidence': 'mean',\n",
    "                'correct': 'mean',\n",
    "                'expected_match': 'count'\n",
    "            }).rename(columns={\n",
    "                'calibrated_confidence': 'avg_calibrated_confidence',\n",
    "                'correct': 'actual_accuracy',\n",
    "                'expected_match': 'count'\n",
    "            }).reset_index()\n",
    "            \n",
    "            new_calibration_error = np.mean(np.abs(calibrated_metrics['avg_calibrated_confidence'] - calibrated_metrics['actual_accuracy']))\n",
    "            \n",
    "            # Save calibration parameters\n",
    "            calibration_params = {\n",
    "                'method': 'logistic',\n",
    "                'parameters': {\n",
    "                    'a': float(a),\n",
    "                    'b': float(b)\n",
    "                },\n",
    "                'original_calibration_error': float(calibration_error),\n",
    "                'calibrated_error': float(new_calibration_error),\n",
    "                'improvement': float(calibration_error - new_calibration_error)\n",
    "            }\n",
    "            \n",
    "            # Save calibration results\n",
    "            calibration_path = os.path.join(self.output_dir, \"confidence_calibration.json\")\n",
    "            with open(calibration_path, 'w') as f:\n",
    "                json.dump(calibration_params, f, indent=2)\n",
    "            \n",
    "            self.logger.info(f\"Confidence calibration completed. Parameters saved to {calibration_path}\")\n",
    "            self.logger.info(f\"Calibration improvement: {calibration_params['improvement']:.4f}\")\n",
    "            \n",
    "            return calibration_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to calibrate confidence: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_with_baseline(self, baseline_thresholds=None):\n",
    "        \"\"\"\n",
    "        Compare matcher performance with baseline algorithms\n",
    "        \n",
    "        Args:\n",
    "            baseline_thresholds (dict): Thresholds for baseline algorithms\n",
    "            \n",
    "        Returns:\n",
    "            dict: Comparison results\n",
    "        \"\"\"\n",
    "        if self.test_results.empty:\n",
    "            raise ValueError(\"No test results available for comparison\")\n",
    "        \n",
    "        # Check if we have ground truth\n",
    "        has_ground_truth = 'expected_match' in self.test_results.columns and not self.test_results['expected_match'].isna().all()\n",
    "        \n",
    "        if not has_ground_truth:\n",
    "            self.logger.warning(\"Ground truth not available. Skipping baseline comparison.\")\n",
    "            return None\n",
    "        \n",
    "        self.logger.info(\"Comparing with baseline algorithms\")\n",
    "        \n",
    "        # Default baseline thresholds\n",
    "        if baseline_thresholds is None:\n",
    "            baseline_thresholds = {\n",
    "                'jaro_winkler': 0.85,\n",
    "                'levenshtein': 0.7,\n",
    "                'token_set': 0.8\n",
    "            }\n",
    "        \n",
    "        # Prepare comparison data\n",
    "        comparison_results = {\n",
    "            'enhanced_matcher': {\n",
    "                'accuracy': self.performance_metrics.get('accuracy', 0),\n",
    "                'precision': self.performance_metrics.get('precision', 0),\n",
    "                'recall': self.performance_metrics.get('recall', 0),\n",
    "                'f1_score': self.performance_metrics.get('f1_score', 0)\n",
    "            },\n",
    "            'baseline_algorithms': {}\n",
    "        }\n",
    "        \n",
    "        # Initialize similarity algorithms for baseline\n",
    "        similarity_algorithms = SimilarityAlgorithms(\n",
    "            preprocessor=self.pipeline._initialized_components.get('preprocessor')\n",
    "        )\n",
    "        \n",
    "        # Run baseline algorithm comparisons\n",
    "        baselines = [\n",
    "            ('jaro_winkler', similarity_algorithms.jaro_winkler_similarity),\n",
    "            ('levenshtein', lambda s1, s2, domain=None: 1 - levenshtein_distance(s1, s2) / max(len(s1), len(s2)) if max(len(s1), len(s2)) > 0 else 0),\n",
    "            ('token_set', similarity_algorithms.token_set_ratio)\n",
    "        ]\n",
    "        \n",
    "        for baseline_name, similarity_func in baselines:\n",
    "            # Get threshold for this baseline\n",
    "            threshold = baseline_thresholds.get(baseline_name, 0.7)\n",
    "            \n",
    "            # Calculate baseline scores\n",
    "            baseline_results = []\n",
    "            \n",
    "            for _, row in self.test_results.iterrows():\n",
    "                s1 = row['s1']\n",
    "                s2 = row['s2']\n",
    "                expected = row['expected_match']\n",
    "                \n",
    "                # Skip invalid entries\n",
    "                if not isinstance(s1, str) or not isinstance(s2, str) or pd.isna(expected):\n",
    "                    continue\n",
    "                \n",
    "                # Calculate similarity\n",
    "                similarity = similarity_func(s1, s2)\n",
    "                \n",
    "                # Determine match\n",
    "                predicted = similarity >= threshold\n",
    "                \n",
    "                baseline_results.append({\n",
    "                    'expected': bool(expected),\n",
    "                    'predicted': predicted,\n",
    "                    'score': similarity\n",
    "                })\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if baseline_results:\n",
    "                df = pd.DataFrame(baseline_results)\n",
    "                \n",
    "                tp = sum((df['expected'] == True) & (df['predicted'] == True))\n",
    "                fp = sum((df['expected'] == False) & (df['predicted'] == True))\n",
    "                fn = sum((df['expected'] == True) & (df['predicted'] == False))\n",
    "                tn = sum((df['expected'] == False) & (df['predicted'] == False))\n",
    "                \n",
    "                accuracy = (tp + tn) / len(df) if len(df) > 0 else 0\n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                comparison_results['baseline_algorithms'][baseline_name] = {\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1\n",
    "                }\n",
    "        \n",
    "        # Calculate improvements over baseline\n",
    "        if comparison_results['baseline_algorithms']:\n",
    "            # Find best baseline\n",
    "            best_baseline = max(\n",
    "                comparison_results['baseline_algorithms'].items(),\n",
    "                key=lambda x: x[1]['f1_score']\n",
    "            )\n",
    "            \n",
    "            best_baseline_name, best_baseline_metrics = best_baseline\n",
    "            \n",
    "            # Calculate improvements\n",
    "            accuracy_improvement = comparison_results['enhanced_matcher']['accuracy'] - best_baseline_metrics['accuracy']\n",
    "            precision_improvement = comparison_results['enhanced_matcher']['precision'] - best_baseline_metrics['precision']\n",
    "            recall_improvement = comparison_results['enhanced_matcher']['recall'] - best_baseline_metrics['recall']\n",
    "            f1_improvement = comparison_results['enhanced_matcher']['f1_score'] - best_baseline_metrics['f1_score']\n",
    "            \n",
    "            comparison_results['improvements'] = {\n",
    "                'best_baseline': best_baseline_name,\n",
    "                'accuracy_improvement': accuracy_improvement,\n",
    "                'precision_improvement': precision_improvement,\n",
    "                'recall_improvement': recall_improvement,\n",
    "                'f1_improvement': f1_improvement,\n",
    "                'relative_f1_improvement': f1_improvement / best_baseline_metrics['f1_score'] if best_baseline_metrics['f1_score'] > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # Save comparison results\n",
    "        comparison_path = os.path.join(self.output_dir, \"baseline_comparison.json\")\n",
    "        with open(comparison_path, 'w') as f:\n",
    "            json.dump(comparison_results, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Baseline comparison saved to {comparison_path}\")\n",
    "        \n",
    "        # Generate comparison visualization\n",
    "        try:\n",
    "            self._visualize_baseline_comparison(comparison_results)\n",
    "        except:\n",
    "            self.logger.warning(\"Could not generate baseline comparison visualization\")\n",
    "        \n",
    "        return comparison_results\n",
    "    \n",
    "    def _visualize_baseline_comparison(self, comparison_results):\n",
    "        \"\"\"Generate visualization for baseline comparison\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            \n",
    "            # Extract metrics for plotting\n",
    "            algorithms = ['Enhanced Matcher'] + list(comparison_results['baseline_algorithms'].keys())\n",
    "            \n",
    "            accuracy_values = [comparison_results['enhanced_matcher']['accuracy']]\n",
    "            precision_values = [comparison_results['enhanced_matcher']['precision']]\n",
    "            recall_values = [comparison_results['enhanced_matcher']['recall']]\n",
    "            f1_values = [comparison_results['enhanced_matcher']['f1_score']]\n",
    "            \n",
    "            for baseline in comparison_results['baseline_algorithms'].values():\n",
    "                accuracy_values.append(baseline['accuracy'])\n",
    "                precision_values.append(baseline['precision'])\n",
    "                recall_values.append(baseline['recall'])\n",
    "                f1_values.append(baseline['f1_score'])\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Set width of bars\n",
    "            barWidth = 0.2\n",
    "            \n",
    "            # Set positions of bars on X axis\n",
    "            r1 = np.arange(len(algorithms))\n",
    "            r2 = [x + barWidth for x in r1]\n",
    "            r3 = [x + barWidth for x in r2]\n",
    "            r4 = [x + barWidth for x in r3]\n",
    "            \n",
    "            # Create bars\n",
    "            plt.bar(r1, accuracy_values, width=barWidth, label='Accuracy', color='#3498db')\n",
    "            plt.bar(r2, precision_values, width=barWidth, label='Precision', color='#2ecc71')\n",
    "            plt.bar(r3, recall_values, width=barWidth, label='Recall', color='#e74c3c')\n",
    "            plt.bar(r4, f1_values, width=barWidth, label='F1 Score', color='#f39c12')\n",
    "            \n",
    "            # Add labels and legend\n",
    "            plt.xlabel('Algorithm')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title('Performance Comparison with Baseline Algorithms')\n",
    "            plt.xticks([r + barWidth*1.5 for r in range(len(algorithms))], algorithms)\n",
    "            plt.legend()\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, v in enumerate(accuracy_values):\n",
    "                plt.text(r1[i], v + 0.02, f'{v:.2f}', ha='center')\n",
    "            for i, v in enumerate(precision_values):\n",
    "                plt.text(r2[i], v + 0.02, f'{v:.2f}', ha='center')\n",
    "            for i, v in enumerate(recall_values):\n",
    "                plt.text(r3[i], v + 0.02, f'{v:.2f}', ha='center')\n",
    "            for i, v in enumerate(f1_values):\n",
    "                plt.text(r4[i], v + 0.02, f'{v:.2f}', ha='center')\n",
    "            \n",
    "            # Set y-axis limit\n",
    "            plt.ylim(0, 1.2)\n",
    "            \n",
    "            # Add grid\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Save figure\n",
    "            comparison_viz_path = os.path.join(self.output_dir, \"baseline_comparison.png\")\n",
    "            plt.savefig(comparison_viz_path)\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not generate baseline comparison visualization: {e}\")\n",
    "    \n",
    "    def run_end_to_end_pipeline(self, test_data=None, num_test_cases=200, domain=None,\n",
    "                              optimize_thresholds=True, calibrate_confidence=True,\n",
    "                              compare_baseline=True, generate_report=True):\n",
    "        \"\"\"\n",
    "        Run a complete end-to-end testing pipeline\n",
    "        \n",
    "        Args:\n",
    "            test_data (DataFrame): Test data with merchant pairs\n",
    "            num_test_cases (int): Number of test cases to generate if no test data\n",
    "            domain (str): Domain for testing\n",
    "            optimize_thresholds (bool): Whether to optimize thresholds\n",
    "            calibrate_confidence (bool): Whether to calibrate confidence scores\n",
    "            compare_baseline (bool): Whether to compare with baseline algorithms\n",
    "            generate_report (bool): Whether to generate report\n",
    "            \n",
    "        Returns:\n",
    "            dict: Pipeline results\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting end-to-end testing pipeline\")\n",
    "        \n",
    "        # Step 1: Prepare test data\n",
    "        if test_data is None:\n",
    "            # Generate test suite\n",
    "            self.logger.info(f\"Generating test suite with {num_test_cases} cases\")\n",
    "            test_data = self.generate_test_suite(\n",
    "                num_cases=num_test_cases,\n",
    "                include_edge_cases=True,\n",
    "                domain=domain,\n",
    "                output_file=\"generated_test_suite.csv\"\n",
    "            )\n",
    "        else:\n",
    "            # Validate test data\n",
    "            test_data = self._validate_test_data(test_data)\n",
    "        \n",
    "        # Step 2: Run tests\n",
    "        self.logger.info(\"Running tests\")\n",
    "        test_results = self.run_tests(\n",
    "            test_data,\n",
    "            domain=domain,\n",
    "            save_results=True\n",
    "        )\n",
    "        \n",
    "        pipeline_results = {\n",
    "            'test_results': test_results\n",
    "        }\n",
    "        \n",
    "        # Step 3: Analyze performance\n",
    "        self.logger.info(\"Analyzing performance\")\n",
    "        performance_analysis = self.analyze_performance(\n",
    "            by_domain=(domain is not None),\n",
    "            by_case_type=True,\n",
    "            by_confidence=True\n",
    "        )\n",
    "        \n",
    "        pipeline_results['performance_analysis'] = performance_analysis\n",
    "        \n",
    "        # Step 4: Optimize thresholds if requested\n",
    "        if optimize_thresholds:\n",
    "            self.logger.info(\"Optimizing thresholds\")\n",
    "            optimal_thresholds = self.optimize_thresholds(test_data, domain)\n",
    "            pipeline_results['optimal_thresholds'] = optimal_thresholds\n",
    "        \n",
    "        # Step 5: Calibrate confidence if requested\n",
    "        if calibrate_confidence:\n",
    "            self.logger.info(\"Calibrating confidence scores\")\n",
    "            try:\n",
    "                calibration_results = self.calibrate_confidence()\n",
    "                pipeline_results['confidence_calibration'] = calibration_results\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Confidence calibration failed: {e}\")\n",
    "        \n",
    "        # Step 6: Compare with baseline if requested\n",
    "        if compare_baseline:\n",
    "            self.logger.info(\"Comparing with baseline algorithms\")\n",
    "            comparison_results = self.compare_with_baseline()\n",
    "            pipeline_results['baseline_comparison'] = comparison_results\n",
    "        \n",
    "        # Step 7: Generate report if requested\n",
    "        if generate_report:\n",
    "            self.logger.info(\"Generating report\")\n",
    "            report_path = self.generate_report(\n",
    "                output_format='html',\n",
    "                include_visualizations=True\n",
    "            )\n",
    "            pipeline_results['report_path'] = report_path\n",
    "        \n",
    "        # Step 8: Export results\n",
    "        self.logger.info(\"Exporting results\")\n",
    "        export_path = self.export_results(format='excel', include_explanations=True)\n",
    "        pipeline_results['export_path'] = export_path\n",
    "        \n",
    "        self.logger.info(\"End-to-end testing pipeline completed successfully\")\n",
    "        \n",
    "        return pipeline_results\n",
    "    \n",
    "    def show_performance_dashboard(self):\n",
    "        \"\"\"\n",
    "        Display an interactive performance dashboard if running in a Jupyter notebook\n",
    "        \n",
    "        Returns:\n",
    "            object: Interactive dashboard object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from IPython.display import display, HTML\n",
    "            import matplotlib.pyplot as plt\n",
    "            from ipywidgets import interact, widgets\n",
    "            \n",
    "            if self.test_results.empty:\n",
    "                return HTML(\"<p>No test results available for dashboard</p>\")\n",
    "            \n",
    "            # Create dashboard HTML\n",
    "            dashboard_html = \"\"\"\n",
    "            <div style=\"padding: 20px; background-color: #f9f9f9; border-radius: 10px; margin-bottom: 20px;\">\n",
    "                <h2 style=\"color: #2c3e50;\">Merchant Matcher Performance Dashboard</h2>\n",
    "                <div style=\"display: flex; flex-wrap: wrap; gap: 15px; margin-top: 20px;\">\n",
    "                    <div style=\"flex: 1; min-width: 150px; background-color: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n",
    "                        <div style=\"font-size: 14px; color: #7f8c8d;\">Accuracy</div>\n",
    "                        <div style=\"font-size: 24px; font-weight: bold; color: #2c3e50;\">{accuracy:.2%}</div>\n",
    "                    </div>\n",
    "                    <div style=\"flex: 1; min-width: 150px; background-color: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n",
    "                        <div style=\"font-size: 14px; color: #7f8c8d;\">Precision</div>\n",
    "                        <div style=\"font-size: 24px; font-weight: bold; color: #2c3e50;\">{precision:.2%}</div>\n",
    "                    </div>\n",
    "                    <div style=\"flex: 1; min-width: 150px; background-color: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n",
    "                        <div style=\"font-size: 14px; color: #7f8c8d;\">Recall</div>\n",
    "                        <div style=\"font-size: 24px; font-weight: bold; color: #2c3e50;\">{recall:.2%}</div>\n",
    "                    </div>\n",
    "                    <div style=\"flex: 1; min-width: 150px; background-color: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n",
    "                        <div style=\"font-size: 14px; color: #7f8c8d;\">F1 Score</div>\n",
    "                        <div style=\"font-size: 24px; font-weight: bold; color: #2c3e50;\">{f1:.2%}</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Fill in metrics\n",
    "            metrics = self.performance_metrics\n",
    "            accuracy = metrics.get('accuracy', 0)\n",
    "            precision = metrics.get('precision', 0)\n",
    "            recall = metrics.get('recall', 0)\n",
    "            f1 = metrics.get('f1_score', 0)\n",
    "            \n",
    "            dashboard = HTML(dashboard_html.format(\n",
    "                accuracy=accuracy,\n",
    "                precision=precision,\n",
    "                recall=recall,\n",
    "                f1=f1\n",
    "            ))\n",
    "            \n",
    "            # Display dashboard\n",
    "            display(dashboard)\n",
    "            \n",
    "            # Create interactive functions for exploring results\n",
    "            def explore_results(case_type='All', score_threshold=0.7, correct_only=False):\n",
    "                \"\"\"Display filtered test results\"\"\"\n",
    "                filtered = self.test_results.copy()\n",
    "                \n",
    "                # Filter by case type\n",
    "                if case_type != 'All' and 'case_type' in filtered.columns:\n",
    "                    filtered = filtered[filtered['case_type'] == case_type]\n",
    "                \n",
    "                # Filter by score threshold\n",
    "                filtered = filtered[filtered['score'] >= score_threshold]\n",
    "                \n",
    "                # Filter by correctness\n",
    "                if correct_only and 'correct' in filtered.columns:\n",
    "                    filtered = filtered[filtered['correct']]\n",
    "                \n",
    "                # Select columns to display\n",
    "                display_cols = ['s1', 's2', 'score', 'confidence', 'level']\n",
    "                if 'expected_match' in filtered.columns:\n",
    "                    display_cols.append('expected_match')\n",
    "                if 'correct' in filtered.columns:\n",
    "                    display_cols.append('correct')\n",
    "                \n",
    "                # Show top 20 rows\n",
    "                return filtered[display_cols].head(20)\n",
    "            \n",
    "            # Create dropdown for case types\n",
    "            case_types = ['All']\n",
    "            if 'case_type' in self.test_results.columns:\n",
    "                case_types.extend(self.test_results['case_type'].unique())\n",
    "            \n",
    "            # Create interactive widgets\n",
    "            interact(\n",
    "                explore_results,\n",
    "                case_type=widgets.Dropdown(options=case_types, description='Case Type:'),\n",
    "                score_threshold=widgets.FloatSlider(min=0, max=1, step=0.05, value=0.7, description='Score :'),\n",
    "                correct_only=widgets.Checkbox(value=False, description='Correct Only')\n",
    "            )\n",
    "            \n",
    "            # Create visualization function\n",
    "            def plot_visualization(plot_type):\n",
    "                \"\"\"Display selected visualization\"\"\"\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                if plot_type == 'Score Distribution':\n",
    "                    if 'expected_match' in self.test_results.columns:\n",
    "                        plt.hist(\n",
    "                            [\n",
    "                                self.test_results[self.test_results['expected_match'] == 1]['score'],\n",
    "                                self.test_results[self.test_results['expected_match'] == 0]['score']\n",
    "                            ],\n",
    "                            bins=20,\n",
    "                            label=['Match', 'No Match'],\n",
    "                            alpha=0.7\n",
    "                        )\n",
    "                        plt.legend()\n",
    "                    else:\n",
    "                        plt.hist(self.test_results['score'], bins=20)\n",
    "                    plt.title('Score Distribution')\n",
    "                    plt.xlabel('Match Score')\n",
    "                    plt.ylabel('Count')\n",
    "                    \n",
    "                elif plot_type == 'Confidence Calibration':\n",
    "                    if 'confidence' in self.test_results.columns and 'correct' in self.test_results.columns:\n",
    "                        # Group by confidence bins\n",
    "                        self.test_results['confidence_bin'] = pd.cut(\n",
    "                            self.test_results['confidence'], \n",
    "                            bins=10\n",
    "                        )\n",
    "                        \n",
    "                        conf_acc = self.test_results.groupby('confidence_bin')['correct'].mean().reset_index()\n",
    "                        conf_acc['bin_center'] = conf_acc['confidence_bin'].apply(lambda x: x.mid)\n",
    "                        \n",
    "                        plt.plot(conf_acc['bin_center'], conf_acc['correct'], 'o-', linewidth=2)\n",
    "                        plt.plot([0, 1], [0, 1], 'k--')  # Ideal calibration line\n",
    "                        \n",
    "                        plt.title('Confidence Calibration Plot')\n",
    "                        plt.xlabel('Confidence')\n",
    "                        plt.ylabel('Accuracy')\n",
    "                        plt.grid(alpha=0.3)\n",
    "                    else:\n",
    "                        plt.text(0.5, 0.5, 'Confidence data not available', ha='center', va='center')\n",
    "                        \n",
    "                elif plot_type == 'Case Type Performance':\n",
    "                    if 'case_type' in self.test_results.columns and 'correct' in self.test_results.columns:\n",
    "                        case_perf = self.test_results.groupby('case_type')['correct'].mean().sort_values()\n",
    "                        \n",
    "                        colors = ['g' if x > 0.8 else 'y' if x > 0.6 else 'r' for x in case_perf]\n",
    "                        \n",
    "                        case_perf.plot(kind='barh', color=colors)\n",
    "                        plt.title('Accuracy by Case Type')\n",
    "                        plt.xlabel('Accuracy')\n",
    "                        plt.ylabel('Case Type')\n",
    "                        plt.xlim(0, 1)\n",
    "                    else:\n",
    "                        plt.text(0.5, 0.5, 'Case type data not available', ha='center', va='center')\n",
    "                        \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Create visualization widget\n",
    "            viz_options = [\n",
    "                'Score Distribution',\n",
    "                'Confidence Calibration',\n",
    "                'Case Type Performance'\n",
    "            ]\n",
    "            \n",
    "            interact(\n",
    "                plot_visualization,\n",
    "                plot_type=widgets.Dropdown(options=viz_options, description='Plot:')\n",
    "            )\n",
    "            \n",
    "            return dashboard\n",
    "            \n",
    "        except ImportError:\n",
    "            self.logger.warning(\"IPython dependencies not available. Cannot display dashboard.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b844ae8-55ea-4915-9704-c8b76c71917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Merchant Matcher Tester...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Initialize the tester\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing Merchant Matcher Tester...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m tester \u001b[38;5;241m=\u001b[39m MerchantMatcherTester(output_dir\u001b[38;5;241m=\u001b[39moutput_dir)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 2. Generate test suite with diverse cases\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating comprehensive test suite...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the tester\n",
    "print(\"Initializing Merchant Matcher Tester...\")\n",
    "tester = MerchantMatcherTester(output_dir=output_dir)\n",
    "\n",
    "# 2. Generate test suite with diverse cases\n",
    "print(\"Generating comprehensive test suite...\")\n",
    "test_suite = tester.generate_test_suite(\n",
    "    num_cases=300,\n",
    "    include_edge_cases=True,\n",
    "    output_file=\"merchant_test_suite.csv\"\n",
    ")\n",
    "\n",
    "# 3. Run end-to-end pipeline\n",
    "print(\"Running end-to-end evaluation pipeline...\")\n",
    "pipeline_results = tester.run_end_to_end_pipeline(\n",
    "    test_data=test_suite,\n",
    "    optimize_thresholds=True,\n",
    "    calibrate_confidence=True,\n",
    "    compare_baseline=True,\n",
    "    generate_report=True\n",
    ")\n",
    "\n",
    "# 4. Print summary results\n",
    "print(\"\\n==== Merchant Matcher Evaluation Results ====\")\n",
    "if 'test_results' in pipeline_results and 'performance' in pipeline_results['test_results']:\n",
    "    perf = pipeline_results['test_results']['performance']\n",
    "    print(f\"Accuracy:  {perf.get('accuracy', 0):.2%}\")\n",
    "    print(f\"Precision: {perf.get('precision', 0):.2%}\")\n",
    "    print(f\"Recall:    {perf.get('recall', 0):.2%}\")\n",
    "    print(f\"F1 Score:  {perf.get('f1_score', 0):.2%}\")\n",
    "\n",
    "# 5. Print comparison with baseline if available\n",
    "if 'baseline_comparison' in pipeline_results and 'improvements' in pipeline_results['baseline_comparison']:\n",
    "    imp = pipeline_results['baseline_comparison']['improvements']\n",
    "    best_baseline = imp.get('best_baseline', 'Unknown')\n",
    "    f1_imp = imp.get('f1_improvement', 0)\n",
    "    rel_imp = imp.get('relative_f1_improvement', 0)\n",
    "    \n",
    "    print(f\"\\nImprovement over best baseline ({best_baseline}):\")\n",
    "    print(f\"F1 Score Improvement: {f1_imp:.2%} ({rel_imp:.2%} relative improvement)\")\n",
    "\n",
    "# 6. Print paths to output files\n",
    "print(\"\\nOutput Files:\")\n",
    "if 'report_path' in pipeline_results:\n",
    "    print(f\"- Detailed Report: {pipeline_results['report_path']}\")\n",
    "if 'export_path' in pipeline_results:\n",
    "    print(f\"- Results Export: {pipeline_results['export_path']}\")\n",
    "\n",
    "print(\"\\nEvaluation complete! See above paths for detailed results.\")\n",
    "return tester, pipeline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cca1be-dd2d-4519-86a5-c1f8731e6b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
