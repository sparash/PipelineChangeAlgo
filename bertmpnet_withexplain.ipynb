{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52de6ee-f6a8-4f9b-ad75-d6ae1619d082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library available for BERT embeddings\n",
      "Warning: pyahocorasick not available. Using fallback implementation.\n",
      "Using device: cpu\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from Levenshtein import jaro_winkler, ratio as levenshtein_ratio\n",
    "import textdistance\n",
    "from fuzzywuzzy import fuzz\n",
    "import jellyfish\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Import transformers for BERT embeddings\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    transformers_available = True\n",
    "    print(\"Transformers library available for BERT embeddings\")\n",
    "except ImportError:\n",
    "    transformers_available = False\n",
    "    print(\"Warning: transformers library not available. Will use TF-IDF fallback.\")\n",
    "\n",
    "# Try to import pyahocorasick with fallback\n",
    "try:\n",
    "    import pyahocorasick\n",
    "    aho_corasick_available = True\n",
    "    print(\"pyahocorasick is available\")\n",
    "except ImportError:\n",
    "    print(\"Warning: pyahocorasick not available. Using fallback implementation.\")\n",
    "    aho_corasick_available = False\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7d8d5-2ef5-4c7e-a636-8bb0064ee9d9",
   "metadata": {},
   "source": [
    "\r\n",
    "## Overview of the Merchant Name Matching System\r\n",
    "\r\n",
    "This code is setting up a sophisticated system designed to match different variations of merchant names - a challenging problem in data analytics, financial services, and business intelligence. For example, recognizing that \"McDonald's,\" \"McD's,\" and \"McDonald's Restaurant #1234\" all refer to the same merchant.\r\n",
    "\r\n",
    "## Components Being Imported and Set Up\r\n",
    "\r\n",
    "### 1. Core Data Processing Libraries\r\n",
    "The code imports fundamental data processing libraries:\r\n",
    "- `pandas` and `numpy` for data manipulation and numerical operations\r\n",
    "- `re` and `string` for regular expression and string operations\r\n",
    "- `time`, `os`, and `warnings` for system operations and warning management\r\n",
    "- `defaultdict` for dictionary data structures with default values\r\n",
    "\r\n",
    "### 2. Machine Learning and Deep Learning Tools\r\n",
    "- `torch` and `torch.nn.functional` - PyTorch libraries for deep learning\r\n",
    "- Setting up device detection to use GPU acceleration if available (using CUDA)\r\n",
    "\r\n",
    "### 3. String Similarity Algorithms\r\n",
    "The code imports multiple string matching libraries:\r\n",
    "- `Levenshtein` for edit distance calculations (measuring how different strings are)\r\n",
    "- `textdistance` for additional string similarity metrics\r\n",
    "- `fuzzywuzzy` for fuzzy string matching\r\n",
    "- `jellyfish` for phonetic matching algorithms\r\n",
    "- `TfidfVectorizer` and `cosine_similarity` from scikit-learn for text vectorization and similarity\r\n",
    "\r\n",
    "### 4. Advanced NLP Components\r\n",
    "- Trying to import `transformers` library with BERT models for semantic understanding\r\n",
    "- Setting a fallback mechanism if the library isn't available\r\n",
    "- Trying to import `pyahocorasick` for efficient pattern matching with fallback\r\n",
    "\r\n",
    "### 5. Error Handling and Environment Setup\r\n",
    "- Setting up graceful degradation paths if certain libraries aren't available\r\n",
    "- Suppressing warnings to keep output clean\r\n",
    "- Checking for GPU availability to optimize performance\r\n",
    "\r\n",
    "## The Purpose and Architecture\r\n",
    "\r\n",
    "This code is setting up a multi-algorithm approach to merchant name matching. What makes this system particularly sophisticated is:\r\n",
    "\r\n",
    "1. **Multi-level matching**: It uses a combination of methods from simple string matching to advanced semantic understanding with BERT models.\r\n",
    "\r\n",
    "2. **Graceful degradation**: If advanced libraries aren't available, it falls back to simpler methods.\r\n",
    "\r\n",
    "3. **GPU acceleration**: It's designed to leverage GPU power when available for the deep learning components.\r\n",
    "\r\n",
    "4. **Comprehensive toolkit**: By importing so many different string matching algorithms, the system can handle various types of merchant name variations - typos, abbreviations, word reordering, etc.\r\n",
    "\r\n",
    "The rest of the code (which isn't shown here) would likely implement classes and methods that use these libraries to:\r\n",
    "- Preprocess merchant names\r\n",
    "- Calculate similarities using different algorithms\r\n",
    "- Combine results from multiple algorithms for final decisions\r\n",
    "- Handle large batches of merchant name matching operations\r\n",
    "\r\n",
    "This introduction represents a very well-designed foundation for an advanced text matching system specifically optimized for the merchant name matching domain, which is significantly more complex than general string matching due to the many variations in how businesses can be referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec4c49a8-68b7-4faf-b471-a8c3d90393f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enhanced BERT model 'sentence-transformers/all-mpnet-base-v2'...\n",
      "Enhanced BERT model loaded successfully on cpu\n",
      "Enhanced BERT embedder initialized with MPNet model!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Enhanced BERT Embedder with MPNet Model\n",
    "\n",
    "class EnhancedBERTEmbedder:\n",
    "    \"\"\"\n",
    "    Enhanced BERT embedder using the more powerful MPNet model for better semantic understanding.\n",
    "    Implements advanced pooling strategies, domain adaptation, and batching for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2', pooling_strategy='mean', device=None):\n",
    "        \"\"\"\n",
    "        Initialize enhanced BERT embedder with specified pre-trained model and pooling strategy.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained BERT model to use\n",
    "            pooling_strategy (str): Pooling strategy ('mean', 'cls', or 'max')\n",
    "            device: Device to run the model on (cuda or cpu)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.max_sequence_length = 512  # BERT's limit\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.initialized = False\n",
    "        self.domain_adapted = False\n",
    "        \n",
    "        # Initialize pre-trained model if transformers available\n",
    "        if transformers_available:\n",
    "            try:\n",
    "                print(f\"Loading enhanced BERT model '{model_name}'...\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "                self.model.eval()  # Set to evaluation mode\n",
    "                self.initialized = True\n",
    "                print(f\"Enhanced BERT model loaded successfully on {self.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing BERT model: {e}\")\n",
    "                self.initialized = False\n",
    "        \n",
    "        # Initialize TF-IDF fallback if BERT not available\n",
    "        if not self.initialized:\n",
    "            # Using character n-grams for better handling of typos and abbreviations\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4))\n",
    "            self.tfidf_fitted = False\n",
    "            print(\"Using TF-IDF fallback for embeddings\")\n",
    "    \n",
    "    def adapt_to_domain(self, examples_df, epochs=1):\n",
    "        \"\"\"\n",
    "        Perform lightweight domain adaptation to improve merchant name understanding.\n",
    "        \n",
    "        Args:\n",
    "            examples_df (DataFrame): DataFrame with matched merchant names\n",
    "            epochs (int): Number of adaptation epochs\n",
    "        \"\"\"\n",
    "        if not self.initialized or self.domain_adapted:\n",
    "            return\n",
    "        \n",
    "        # Extract positive pairs (matching merchant names)\n",
    "        positive_pairs = []\n",
    "        if 'Enhanced_Score' in examples_df.columns:\n",
    "            for _, row in examples_df.iterrows():\n",
    "                if row['Enhanced_Score'] >= 0.8:  # High-confidence matches as positive examples\n",
    "                    positive_pairs.append((row['DBAName'], row['RawTransactionName']))\n",
    "        \n",
    "        # Skip if not enough examples\n",
    "        if len(positive_pairs) < 5:\n",
    "            print(\"Not enough high-quality examples for adaptation\")\n",
    "            return\n",
    "        \n",
    "        # Implement lightweight adaptation with contrastive learning\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        \n",
    "        print(f\"Performing domain adaptation with {len(positive_pairs)} merchant name pairs...\")\n",
    "        for _ in range(epochs):\n",
    "            for name1, name2 in positive_pairs:\n",
    "                # Tokenize\n",
    "                inputs = self.tokenizer([name1, name2], return_tensors='pt', padding=True, \n",
    "                                       truncation=True, max_length=self.max_sequence_length).to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "                \n",
    "                # Contrastive loss (push matching names closer)\n",
    "                similarity = F.cosine_similarity(embeddings[0].unsqueeze(0), embeddings[1].unsqueeze(0))\n",
    "                loss = 1 - similarity\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.domain_adapted = True\n",
    "        print(f\"Domain adaptation completed\")\n",
    "    \n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling - take average of all token embeddings\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]  # First element contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def _cls_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        CLS pooling - use the [CLS] token embedding\n",
    "        \"\"\"\n",
    "        return model_output[0][:, 0]\n",
    "    \n",
    "    def _max_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Max pooling - take max of all token embeddings\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "        return torch.max(token_embeddings, 1)[0]\n",
    "    \n",
    "    def _get_pooled_embeddings(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Apply the selected pooling strategy\n",
    "        \"\"\"\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            return self._mean_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'cls':\n",
    "            return self._cls_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            return self._max_pooling(model_output, attention_mask)\n",
    "        else:\n",
    "            # Default to mean pooling\n",
    "            return self._mean_pooling(model_output, attention_mask)\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        Fit the TF-IDF vectorizer on a corpus of texts (only needed for TF-IDF fallback)\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            # Fit TF-IDF vectorizer\n",
    "            self.tfidf_vectorizer.fit(texts)\n",
    "            self.tfidf_fitted = True\n",
    "            print(\"TF-IDF vectorizer fitted on corpus\")\n",
    "    \n",
    "    def encode(self, texts, batch_size=32, show_progress=False):\n",
    "        \"\"\"\n",
    "        Encode texts into embeddings using the pre-trained model\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts or single text\n",
    "            batch_size: Batch size for processing\n",
    "            show_progress: Whether to show progress\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Embeddings for the texts\n",
    "        \"\"\"\n",
    "        # Handle single text input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Return empty array for empty input\n",
    "        if len(texts) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Use pre-trained BERT if available\n",
    "        if self.initialized:\n",
    "            # Process in batches\n",
    "            all_embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                if show_progress and i % (batch_size * 10) == 0:\n",
    "                    print(f\"Processing batch {i//batch_size + 1}/{(len(texts)//batch_size) + 1}\")\n",
    "                \n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize\n",
    "                encoded_input = self.tokenizer(\n",
    "                    batch_texts, \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=self.max_sequence_length,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Compute token embeddings\n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(**encoded_input)\n",
    "                    batch_embeddings = self._get_pooled_embeddings(model_output, encoded_input['attention_mask'])\n",
    "                    all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            \n",
    "            return np.vstack(all_embeddings)\n",
    "        \n",
    "        else:\n",
    "            # Use TF-IDF fallback\n",
    "            if not self.tfidf_fitted:\n",
    "                self.fit(texts)\n",
    "            \n",
    "            return self.tfidf_vectorizer.transform(texts).toarray()\n",
    "    \n",
    "    def compute_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts using the pre-trained model\n",
    "        \n",
    "        Args:\n",
    "            text1: First text\n",
    "            text2: Second text\n",
    "            \n",
    "        Returns:\n",
    "            float: Cosine similarity score\n",
    "        \"\"\"\n",
    "        # Get embeddings for both texts\n",
    "        emb1 = self.encode(text1)\n",
    "        emb2 = self.encode(text2)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        return np.sum(emb1 * emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)\n",
    "\n",
    "# Initialize enhanced BERT embedder with MPNet model\n",
    "bert_embedder = EnhancedBERTEmbedder(model_name='sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "print(\"Enhanced BERT embedder initialized with MPNet model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05903d8-e75a-410d-b629-e501af7f6ff6",
   "metadata": {},
   "source": [
    "# Explanation of the EnhancedBERTEmbedder Class\r\n",
    "\r\n",
    "This code implements a sophisticated text embedding system called `EnhancedBERTEmbedder` that converts merchant names (or any text) into numerical vector representations that capture their semantic meaning. Let me walk you through the key components and functionality.\r\n",
    "\r\n",
    "## Purpose and Core Functionality\r\n",
    "\r\n",
    "The `EnhancedBERTEmbedder` class creates a system that can transform text strings (particularly merchant names) into high-dimensional vectors that capture semantic meaning. These vectors allow us to quantify how similar two merchant names are based on their meaning rather than just their spelling. For example, \"McDonald's Restaurant\" and \"McD's\" would have similar embeddings despite different spelling.\r\n",
    "\r\n",
    "The class primarily uses the MPNet model (specifically `sentence-transformers/all-mpnet-base-v2`), which is more advanced than traditional BERT models for semantic understanding.\r\n",
    "\r\n",
    "## Class Architecture\r\n",
    "\r\n",
    "### Initialization and Setup\r\n",
    "The constructor sets up:\r\n",
    "1. The underlying transformer model (MPNet by default)\r\n",
    "2. The chosen pooling strategy (how to combine word vectors into a single sentence vector)\r\n",
    "3. Device configuration (GPU vs. CPU processing)\r\n",
    "4. A fallback TF-IDF vectorizer in case the transformer model can't be loaded\r\n",
    "\r\n",
    "It gracefully handles cases where the transformer library isn't available by providing a simpler TF-IDF approach as a backup plan.\r\n",
    "\r\n",
    "### Embedding Generation Methods\r\n",
    "The `encode()` method takes text inputs and produces embedding vectors by:\r\n",
    "1. Tokenizing the text into word pieces\r\n",
    "2. Processing these tokens through the neural network\r\n",
    "3. Applying a pooling strategy to combine the token embeddings\r\n",
    "4. Returning numerical embeddings as NumPy arrays\r\n",
    "\r\n",
    "The class supports batch processing for efficiency when encoding multiple texts.\r\n",
    "\r\n",
    "### Pooling Strategies\r\n",
    "The class implements three different ways to convert token-level vectors to a single sentence vector:\r\n",
    "1. `_mean_pooling()` - Taking the average of all token embeddings (default)\r\n",
    "2. `_cls_pooling()` - Using the special [CLS] token's embedding\r\n",
    "3. `_max_pooling()` - Taking the maximum value across all token embeddings\r\n",
    "\r\n",
    "### Domain Adaptation\r\n",
    "The `adapt_to_domain()` method allows fine-tuning the model on merchant name pairs through contrastive learning:\r\n",
    "1. It takes example pairs of merchant names known to be the same entity\r\n",
    "2. Adjusts the model parameters to bring these matching pairs closer in the embedding space\r\n",
    "3. Uses a contrastive loss function that minimizes distance between positive pairs\r\n",
    "\r\n",
    "### Similarity Computation\r\n",
    "The `compute_similarity()` method calculates how similar two texts are by:\r\n",
    "1. Encoding both texts into embeddings\r\n",
    "2. Computing the cosine similarity between these embeddings\r\n",
    "3. Returning a value between -1 and 1 (though typically 0-1 for normalized embeddings)\r\n",
    "\r\n",
    "## Technical Sophistication\r\n",
    "\r\n",
    "Several advanced techniques make this implementation powerful:\r\n",
    "\r\n",
    "1. **Model Selection**: Using MPNet (`all-mpnet-base-v2`) which outperforms traditional BERT models for semantic understanding.\r\n",
    "\r\n",
    "2. **Graceful Degradation**: Implementing a TF-IDF fallback if advanced models aren't available.\r\n",
    "\r\n",
    "3. **Pooling Strategy Options**: Supporting multiple strategies for converting token vectors to sentence vectors.\r\n",
    "\r\n",
    "4. **Batched Processing**: Processing texts in batches for improved efficiency.\r\n",
    "\r\n",
    "5. **Domain Adaptation**: Fine-tuning capabilities to adapt the model specifically for merchant name matching.\r\n",
    "\r\n",
    "6. **GPU Acceleration**: Automatic use of GPU when available for faster processing.\r\n",
    "\r\n",
    "## Practical Applications\r\n",
    "\r\n",
    "This embedder would be used in the merchant matching system to:\r\n",
    "\r\n",
    "1. Generate consistent vector representations of merchant names\r\n",
    "2. Calculate semantic similarity between different merchant name versions\r\n",
    "3. Support clustering or matching algorithms that rely on vector space operations\r\n",
    "4. Provide a foundation for advanced pattern recognition in merchant names\r\n",
    "\r\n",
    "The embedding approach is particularly valuable because it can recognize semantic relationships between names even when they don't share many characters (unlike simpler string matching algorithms).\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "The `EnhancedBERTEmbedder` represents a sophisticated approach to the challenging problem of merchant name matching by leveraging state-of-the-art natural language processing techniques. It balances advanced capabilities with practical concerns like fallback mechanisms and efficient processing, making it suitable for real-world applications in financial data processing, business intelligence, and data cleansing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae4f9ea-b1b1-46da-b14d-26a8bb3270d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced merchant matcher initialized!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Enhanced Merchant Matcher Core Class\n",
    "\n",
    "class EnhancedMerchantMatcher:\n",
    "    \"\"\"\n",
    "    Enhanced matcher with improved pattern recognition for merchant name matching.\n",
    "    Uses multiple similarity algorithms and domain-specific patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_embedder=None):\n",
    "        \"\"\"\n",
    "        Initialize with enhanced BERT embedder.\n",
    "        \n",
    "        Args:\n",
    "            bert_embedder: Enhanced BERT embedder instance\n",
    "        \"\"\"\n",
    "        # Initialize enhanced BERT embedder\n",
    "        self.bert_embedder = bert_embedder\n",
    "        if self.bert_embedder is None and transformers_available:\n",
    "            self.bert_embedder = EnhancedBERTEmbedder()\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        # Initialize trie for approximate matching\n",
    "        self.trie = None\n",
    "        \n",
    "        # Initialize Aho-Corasick automaton only if available\n",
    "        if aho_corasick_available:\n",
    "            self.automaton = pyahocorasick.Automaton()\n",
    "        else:\n",
    "            self.automaton = None\n",
    "        \n",
    "        # Define abbreviation dictionary - comprehensive industry knowledge \n",
    "        self.abbreviations = self._get_abbreviation_dictionary()\n",
    "        \n",
    "        # Domain-specific abbreviations\n",
    "        self.domain_abbreviations = self._get_domain_abbreviations()\n",
    "        \n",
    "        # Stop words to remove during preprocessing\n",
    "        self.stopwords = self._get_stopwords()\n",
    "        \n",
    "        # Domain-specific stopwords\n",
    "        self.domain_stopwords = self._get_domain_stopwords()\n",
    "    \n",
    "    def _get_abbreviation_dictionary(self):\n",
    "        \"\"\"Get comprehensive abbreviation dictionary\"\"\"\n",
    "        return {\n",
    "            # Banking & Financial Institutions\n",
    "            'bofa': 'bank of america', 'b of a': 'bank of america',\n",
    "            'boa': 'bank of america', 'bac': 'bank of america',\n",
    "            'jpm': 'jpmorgan chase', 'jpm chase': 'jpmorgan chase',\n",
    "            'wf': 'wells fargo', 'wfb': 'wells fargo bank',\n",
    "            'citi': 'citibank', 'citi bank': 'citibank',\n",
    "            'gs': 'goldman sachs', 'ms': 'morgan stanley',\n",
    "            'db': 'deutsche bank', 'hsbc': 'hongkong and shanghai banking corporation',\n",
    "            'amex': 'american express', 'usb': 'us bank', 'rbc': 'royal bank of canada',\n",
    "            'pnc': 'pnc financial services', 'td': 'toronto dominion bank',\n",
    "            'bny': 'bank of new york', 'bnyc': 'bank of new york mellon',\n",
    "            'cba': 'commonwealth bank of australia', 'nab': 'national australia bank',\n",
    "            'rba': 'reserve bank of australia', 'westpac': 'western pacific bank',\n",
    "            \n",
    "            # Fast Food & Restaurant Chains\n",
    "            'mcd': 'mcdonalds', 'mcds': 'mcdonalds', 'md': 'mcdonalds',\n",
    "            'bk': 'burger king', 'kfc': 'kentucky fried chicken',\n",
    "            'sbux': 'starbucks', 'sb': 'starbucks',\n",
    "            'tb': 'taco bell', 'wen': 'wendys',\n",
    "            'dq': 'dairy queen', 'ph': 'pizza hut',\n",
    "            'dnkn': 'dunkin donuts', 'cfa': 'chick fil a',\n",
    "            'cmg': 'chipotle mexican grill', 'ihop': 'international house of pancakes',\n",
    "            'tgi': 'tgi fridays', 'tgif': 'tgi fridays',\n",
    "            \n",
    "            # Tech Companies\n",
    "            'msft': 'microsoft', 'aapl': 'apple', 'goog': 'google',\n",
    "            'googl': 'google', 'amzn': 'amazon', 'fb': 'facebook',\n",
    "            'meta': 'meta platforms', 'nflx': 'netflix', 'tsla': 'tesla',\n",
    "            'ibm': 'international business machines', 'csco': 'cisco systems',\n",
    "            'orcl': 'oracle', 'intc': 'intel', 'amd': 'advanced micro devices',\n",
    "            'nvda': 'nvidia', 'adbe': 'adobe', 'crm': 'salesforce',\n",
    "            \n",
    "            # Automotive\n",
    "            'tm': 'toyota motor', 'toyof': 'toyota', 'toyota': 'toyota corporation',\n",
    "            'f': 'ford motor company', 'gm': 'general motors',\n",
    "            'hmc': 'honda motor company', 'hndaf': 'honda',\n",
    "            'nsany': 'nissan', 'bmwyy': 'bmw', 'vwagy': 'volkswagen',\n",
    "            \n",
    "            # Retail companies\n",
    "            'wmt': 'walmart', 'tgt': 'target', 'cost': 'costco',\n",
    "            'hd': 'home depot', 'low': 'lowes', 'bby': 'best buy',\n",
    "            'ebay': 'ebay', 'dg': 'dollar general', 'dltr': 'dollar tree',\n",
    "            \n",
    "            # Government & Organizations\n",
    "            'dhs': 'department of homeland security',\n",
    "            'dod': 'department of defense', 'dos': 'department of state',\n",
    "            'epa': 'environmental protection agency', 'fbi': 'federal bureau of investigation',\n",
    "            'cia': 'central intelligence agency', 'irs': 'internal revenue service',\n",
    "            'fda': 'food and drug administration', 'sec': 'securities and exchange commission',\n",
    "            'usps': 'united states postal service', 'doi': 'department of interior',\n",
    "            'fed': 'federal reserve', 'who': 'world health organization',\n",
    "            'un': 'united nations', 'nato': 'north atlantic treaty organization',\n",
    "            \n",
    "            # Common abbreviations\n",
    "            'j&j': 'johnson & johnson', 'jj': 'johnson johnson', \n",
    "            'jnj': 'johnson and johnson', '7-11': '7-eleven', \n",
    "            '711': '7-eleven', 'intl': 'international',\n",
    "            'corp': 'corporation', 'inc': 'incorporated',\n",
    "            \n",
    "            # Address components\n",
    "            'rd': 'road', 'st': 'street', 'ave': 'avenue', \n",
    "            'blvd': 'boulevard', 'ctr': 'center', 'ln': 'lane', \n",
    "            'dr': 'drive', 'pl': 'place', 'ct': 'court',\n",
    "            'hwy': 'highway', 'pkwy': 'parkway', 'sq': 'square'\n",
    "        }\n",
    "    \n",
    "    def _get_domain_abbreviations(self):\n",
    "        \"\"\"Get domain-specific abbreviation dictionaries\"\"\"\n",
    "        return {\n",
    "            'Medical': {\n",
    "                'dr': 'doctor', 'hosp': 'hospital', 'med': 'medical',\n",
    "                'clin': 'clinic', 'pharm': 'pharmacy', 'lab': 'laboratory',\n",
    "                'dept': 'department', 'ctr': 'center', 'inst': 'institute',\n",
    "                'er': 'emergency room', 'icu': 'intensive care unit',\n",
    "                'ob': 'obstetrics', 'gyn': 'gynecology', 'peds': 'pediatrics',\n",
    "                'ortho': 'orthopedics', 'onc': 'oncology', 'neuro': 'neurology'\n",
    "            },\n",
    "            'Government': {\n",
    "                'govt': 'government', 'dept': 'department', 'admin': 'administration',\n",
    "                'auth': 'authority', 'fed': 'federal', 'natl': 'national',\n",
    "                'comm': 'commission', 'sec': 'secretary', 'org': 'organization',\n",
    "                'div': 'division', 'bur': 'bureau', 'off': 'office',\n",
    "                'min': 'ministry', 'reg': 'regional', 'dist': 'district',\n",
    "                'cncl': 'council', 'cmte': 'committee', 'subcmte': 'subcommittee'\n",
    "            },\n",
    "            'Education': {\n",
    "                'univ': 'university', 'coll': 'college', 'acad': 'academy',\n",
    "                'elem': 'elementary', 'sch': 'school', 'inst': 'institute',\n",
    "                'dept': 'department', 'lib': 'library', 'lab': 'laboratory',\n",
    "                'fac': 'faculty', 'prof': 'professor', 'assoc': 'associate',\n",
    "                'asst': 'assistant', 'adm': 'administration', 'stdnt': 'student',\n",
    "                'grad': 'graduate', 'undergrad': 'undergraduate'\n",
    "            },\n",
    "            'Financial': {\n",
    "                'fin': 'financial', 'svcs': 'services', 'mgmt': 'management',\n",
    "                'assoc': 'associates', 'intl': 'international', 'grp': 'group',\n",
    "                'corp': 'corporation', 'cap': 'capital', 'inv': 'investment',\n",
    "                'asset': 'asset management', 'sec': 'securities', 'adv': 'advisors',\n",
    "                'tr': 'trust', 'port': 'portfolio', 'acct': 'account',\n",
    "                'bal': 'balance', 'stmt': 'statement', 'equ': 'equity'\n",
    "            },\n",
    "            'Restaurant': {\n",
    "                'rest': 'restaurant', 'cafe': 'cafeteria', 'grill': 'grillery',\n",
    "                'brew': 'brewery', 'bar': 'bar and grill', 'bbq': 'barbecue',\n",
    "                'deli': 'delicatessen', 'stk': 'steakhouse', 'bf': 'breakfast',\n",
    "                'din': 'dinner', 'chs': 'cheese', 'ckn': 'chicken'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_stopwords(self):\n",
    "        \"\"\"Get general stopwords for preprocessing\"\"\"\n",
    "        return {\n",
    "            'inc', 'llc', 'co', 'ltd', 'corp', 'plc', 'na', 'the', \n",
    "            'and', 'of', 'for', 'in', 'a', 'an', 'by', 'to', 'at',\n",
    "            'corporation', 'incorporated', 'company', 'limited',\n",
    "            'with', 'from', 'as', 'on', 'group', 'services'\n",
    "        }\n",
    "    \n",
    "    def _get_domain_stopwords(self):\n",
    "        \"\"\"Get domain-specific stopwords\"\"\"\n",
    "        return {\n",
    "            'Medical': {'center', 'healthcare', 'medical', 'health', 'care', 'services', 'clinic', 'hospital'},\n",
    "            'Government': {'department', 'office', 'agency', 'bureau', 'division', 'authority', 'administration'},\n",
    "            'Education': {'university', 'college', 'school', 'institute', 'academy', 'education', 'learning'},\n",
    "            'Financial': {'financial', 'services', 'management', 'capital', 'investment', 'banking', 'advisor'},\n",
    "            'Restaurant': {'restaurant', 'cafe', 'diner', 'eatery', 'grill', 'kitchen', 'bar', 'house'}\n",
    "        }\n",
    "    \n",
    "    def enhanced_preprocessing(self, text, domain=None):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with better handling of merchant-specific patterns\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to preprocess\n",
    "            domain (str, optional): Domain for specialized processing\n",
    "        \n",
    "        Returns:\n",
    "            str: Preprocessed text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Better handling of punctuation - preserve periods in DBANames\n",
    "        # and apostrophes in business names (e.g., McDonald's)\n",
    "        text = re.sub(r'([^a-z0-9\\'\\.\\&\\-])', ' ', text)\n",
    "        \n",
    "        # Special handling for business name apostrophes\n",
    "        text = re.sub(r'\\'s\\b', 's', text)  # Convert McDonald's to McDonalds\n",
    "        \n",
    "        # Expand common business suffixes\n",
    "        business_suffixes = {\n",
    "            r'\\bco\\b': 'company',\n",
    "            r'\\binc\\b': '',  # Remove Inc entirely\n",
    "            r'\\bltd\\b': 'limited',\n",
    "            r'\\bllc\\b': '',  # Remove LLC entirely\n",
    "            r'\\bcorp\\b': 'corporation',\n",
    "            r'\\bcorporation\\b': '',  # Remove when processing full names for matching\n",
    "            r'\\blimited\\b': '',      # Remove when processing full names for matching\n",
    "            r'\\bcompany\\b': '',      # Remove when processing full names for matching\n",
    "        }\n",
    "        \n",
    "        for suffix, replacement in business_suffixes.items():\n",
    "            text = re.sub(suffix, replacement, text)\n",
    "        \n",
    "        # Replace abbreviations\n",
    "        words = text.split()\n",
    "        \n",
    "        # Apply general abbreviation expansion\n",
    "        words = [self.abbreviations.get(word, word) for word in words]\n",
    "        \n",
    "        # Apply domain-specific abbreviation expansion if domain is provided\n",
    "        if domain and domain in self.domain_abbreviations:\n",
    "            words = [self.domain_abbreviations[domain].get(word, word) for word in words]\n",
    "        \n",
    "        # Enhanced handling for McDonalds variations\n",
    "        if any('mc' in word.lower() for word in words):\n",
    "            words = ['mcdonalds' if word.lower() in ['mcd', 'mcds', 'mcdon'] else word for word in words]\n",
    "        \n",
    "        # Remove general stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "        \n",
    "        # Remove domain-specific stopwords if domain is provided\n",
    "        if domain and domain in self.domain_stopwords:\n",
    "            words = [word for word in words if word not in self.domain_stopwords[domain]]\n",
    "        \n",
    "        # Rejoin words and remove extra spaces\n",
    "        text = ' '.join(words)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_pair(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"Preprocess DBAName and RawTransactionName with domain-specific handling\"\"\"\n",
    "        DBAName_clean = self.enhanced_preprocessing(DBAName, domain)\n",
    "        RawTransactionName_clean = self.enhanced_preprocessing(RawTransactionName, domain)\n",
    "        return DBAName_clean, RawTransactionName_clean\n",
    "\n",
    "# Initialize enhanced merchant matcher with BERT embedder\n",
    "merchant_matcher = EnhancedMerchantMatcher(bert_embedder=bert_embedder)\n",
    "print(\"Enhanced merchant matcher initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986638a4-aae7-429d-bb79-aaca6b161aee",
   "metadata": {},
   "source": [
    "# Cell 3: Enhanced Merchant Matcher Core Class\n",
    "\n",
    "class EnhancedMerchantMatcher:\n",
    "    \"\"\"\n",
    "    Enhanced matcher with improved pattern recognition for merchant name matching.\n",
    "    Uses multiple similarity algorithms and domain-specific patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_embedder=None):\n",
    "        \"\"\"\n",
    "        Initialize with enhanced BERT embedder.\n",
    "        \n",
    "        Args:\n",
    "            bert_embedder: Enhanced BERT embedder instance\n",
    "        \"\"\"\n",
    "        # Initialize enhanced BERT embedder\n",
    "        self.bert_embedder = bert_embedder\n",
    "        if self.bert_embedder is None and transformers_available:\n",
    "            self.bert_embedder = EnhancedBERTEmbedder()\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        # Initialize trie for approximate matching\n",
    "        self.trie = None\n",
    "        \n",
    "        # Initialize Aho-Corasick automaton only if available\n",
    "        if aho_corasick_available:\n",
    "            self.automaton = pyahocorasick.Automaton()\n",
    "        else:\n",
    "            self.automaton = None\n",
    "        \n",
    "        # Define abbreviation dictionary - comprehensive industry knowledge \n",
    "        self.abbreviations = self._get_abbreviation_dictionary()\n",
    "        \n",
    "        # Domain-specific abbreviations\n",
    "        self.domain_abbreviations = self._get_domain_abbreviations()\n",
    "        \n",
    "        # Stop words to remove during preprocessing\n",
    "        self.stopwords = self._get_stopwords()\n",
    "        \n",
    "        # Domain-specific stopwords\n",
    "        self.domain_stopwords = self._get_domain_stopwords()\n",
    "    \n",
    "    def _get_abbreviation_dictionary(self):\n",
    "        \"\"\"Get comprehensive abbreviation dictionary\"\"\"\n",
    "        return {\n",
    "            # Banking & Financial Institutions\n",
    "            'bofa': 'bank of america', 'b of a': 'bank of america',\n",
    "            'boa': 'bank of america', 'bac': 'bank of america',\n",
    "            'jpm': 'jpmorgan chase', 'jpm chase': 'jpmorgan chase',\n",
    "            'wf': 'wells fargo', 'wfb': 'wells fargo bank',\n",
    "            'citi': 'citibank', 'citi bank': 'citibank',\n",
    "            'gs': 'goldman sachs', 'ms': 'morgan stanley',\n",
    "            'db': 'deutsche bank', 'hsbc': 'hongkong and shanghai banking corporation',\n",
    "            'amex': 'american express', 'usb': 'us bank', 'rbc': 'royal bank of canada',\n",
    "            'pnc': 'pnc financial services', 'td': 'toronto dominion bank',\n",
    "            'bny': 'bank of new york', 'bnyc': 'bank of new york mellon',\n",
    "            'cba': 'commonwealth bank of australia', 'nab': 'national australia bank',\n",
    "            'rba': 'reserve bank of australia', 'westpac': 'western pacific bank',\n",
    "            \n",
    "            # Fast Food & Restaurant Chains\n",
    "            'mcd': 'mcdonalds', 'mcds': 'mcdonalds', 'md': 'mcdonalds',\n",
    "            'bk': 'burger king', 'kfc': 'kentucky fried chicken',\n",
    "            'sbux': 'starbucks', 'sb': 'starbucks',\n",
    "            'tb': 'taco bell', 'wen': 'wendys',\n",
    "            'dq': 'dairy queen', 'ph': 'pizza hut',\n",
    "            'dnkn': 'dunkin donuts', 'cfa': 'chick fil a',\n",
    "            'cmg': 'chipotle mexican grill', 'ihop': 'international house of pancakes',\n",
    "            'tgi': 'tgi fridays', 'tgif': 'tgi fridays',\n",
    "            \n",
    "            # Tech Companies\n",
    "            'msft': 'microsoft', 'aapl': 'apple', 'goog': 'google',\n",
    "            'googl': 'google', 'amzn': 'amazon', 'fb': 'facebook',\n",
    "            'meta': 'meta platforms', 'nflx': 'netflix', 'tsla': 'tesla',\n",
    "            'ibm': 'international business machines', 'csco': 'cisco systems',\n",
    "            'orcl': 'oracle', 'intc': 'intel', 'amd': 'advanced micro devices',\n",
    "            'nvda': 'nvidia', 'adbe': 'adobe', 'crm': 'salesforce',\n",
    "            \n",
    "            # Automotive\n",
    "            'tm': 'toyota motor', 'toyof': 'toyota', 'toyota': 'toyota corporation',\n",
    "            'f': 'ford motor company', 'gm': 'general motors',\n",
    "            'hmc': 'honda motor company', 'hndaf': 'honda',\n",
    "            'nsany': 'nissan', 'bmwyy': 'bmw', 'vwagy': 'volkswagen',\n",
    "            \n",
    "            # Retail companies\n",
    "            'wmt': 'walmart', 'tgt': 'target', 'cost': 'costco',\n",
    "            'hd': 'home depot', 'low': 'lowes', 'bby': 'best buy',\n",
    "            'ebay': 'ebay', 'dg': 'dollar general', 'dltr': 'dollar tree',\n",
    "            \n",
    "            # Government & Organizations\n",
    "            'dhs': 'department of homeland security',\n",
    "            'dod': 'department of defense', 'dos': 'department of state',\n",
    "            'epa': 'environmental protection agency', 'fbi': 'federal bureau of investigation',\n",
    "            'cia': 'central intelligence agency', 'irs': 'internal revenue service',\n",
    "            'fda': 'food and drug administration', 'sec': 'securities and exchange commission',\n",
    "            'usps': 'united states postal service', 'doi': 'department of interior',\n",
    "            'fed': 'federal reserve', 'who': 'world health organization',\n",
    "            'un': 'united nations', 'nato': 'north atlantic treaty organization',\n",
    "            \n",
    "            # Common abbreviations\n",
    "            'j&j': 'johnson & johnson', 'jj': 'johnson johnson', \n",
    "            'jnj': 'johnson and johnson', '7-11': '7-eleven', \n",
    "            '711': '7-eleven', 'intl': 'international',\n",
    "            'corp': 'corporation', 'inc': 'incorporated',\n",
    "            \n",
    "            # Address components\n",
    "            'rd': 'road', 'st': 'street', 'ave': 'avenue', \n",
    "            'blvd': 'boulevard', 'ctr': 'center', 'ln': 'lane', \n",
    "            'dr': 'drive', 'pl': 'place', 'ct': 'court',\n",
    "            'hwy': 'highway', 'pkwy': 'parkway', 'sq': 'square'\n",
    "        }\n",
    "    \n",
    "    def _get_domain_abbreviations(self):\n",
    "        \"\"\"Get domain-specific abbreviation dictionaries\"\"\"\n",
    "        return {\n",
    "            'Medical': {\n",
    "                'dr': 'doctor', 'hosp': 'hospital', 'med': 'medical',\n",
    "                'clin': 'clinic', 'pharm': 'pharmacy', 'lab': 'laboratory',\n",
    "                'dept': 'department', 'ctr': 'center', 'inst': 'institute',\n",
    "                'er': 'emergency room', 'icu': 'intensive care unit',\n",
    "                'ob': 'obstetrics', 'gyn': 'gynecology', 'peds': 'pediatrics',\n",
    "                'ortho': 'orthopedics', 'onc': 'oncology', 'neuro': 'neurology'\n",
    "            },\n",
    "            'Government': {\n",
    "                'govt': 'government', 'dept': 'department', 'admin': 'administration',\n",
    "                'auth': 'authority', 'fed': 'federal', 'natl': 'national',\n",
    "                'comm': 'commission', 'sec': 'secretary', 'org': 'organization',\n",
    "                'div': 'division', 'bur': 'bureau', 'off': 'office',\n",
    "                'min': 'ministry', 'reg': 'regional', 'dist': 'district',\n",
    "                'cncl': 'council', 'cmte': 'committee', 'subcmte': 'subcommittee'\n",
    "            },\n",
    "            'Education': {\n",
    "                'univ': 'university', 'coll': 'college', 'acad': 'academy',\n",
    "                'elem': 'elementary', 'sch': 'school', 'inst': 'institute',\n",
    "                'dept': 'department', 'lib': 'library', 'lab': 'laboratory',\n",
    "                'fac': 'faculty', 'prof': 'professor', 'assoc': 'associate',\n",
    "                'asst': 'assistant', 'adm': 'administration', 'stdnt': 'student',\n",
    "                'grad': 'graduate', 'undergrad': 'undergraduate'\n",
    "            },\n",
    "            'Financial': {\n",
    "                'fin': 'financial', 'svcs': 'services', 'mgmt': 'management',\n",
    "                'assoc': 'associates', 'intl': 'international', 'grp': 'group',\n",
    "                'corp': 'corporation', 'cap': 'capital', 'inv': 'investment',\n",
    "                'asset': 'asset management', 'sec': 'securities', 'adv': 'advisors',\n",
    "                'tr': 'trust', 'port': 'portfolio', 'acct': 'account',\n",
    "                'bal': 'balance', 'stmt': 'statement', 'equ': 'equity'\n",
    "            },\n",
    "            'Restaurant': {\n",
    "                'rest': 'restaurant', 'cafe': 'cafeteria', 'grill': 'grillery',\n",
    "                'brew': 'brewery', 'bar': 'bar and grill', 'bbq': 'barbecue',\n",
    "                'deli': 'delicatessen', 'stk': 'steakhouse', 'bf': 'breakfast',\n",
    "                'din': 'dinner', 'chs': 'cheese', 'ckn': 'chicken'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_stopwords(self):\n",
    "        \"\"\"Get general stopwords for preprocessing\"\"\"\n",
    "        return {\n",
    "            'inc', 'llc', 'co', 'ltd', 'corp', 'plc', 'na', 'the', \n",
    "            'and', 'of', 'for', 'in', 'a', 'an', 'by', 'to', 'at',\n",
    "            'corporation', 'incorporated', 'company', 'limited',\n",
    "            'with', 'from', 'as', 'on', 'group', 'services'\n",
    "        }\n",
    "    \n",
    "    def _get_domain_stopwords(self):\n",
    "        \"\"\"Get domain-specific stopwords\"\"\"\n",
    "        return {\n",
    "            'Medical': {'center', 'healthcare', 'medical', 'health', 'care', 'services', 'clinic', 'hospital'},\n",
    "            'Government': {'department', 'office', 'agency', 'bureau', 'division', 'authority', 'administration'},\n",
    "            'Education': {'university', 'college', 'school', 'institute', 'academy', 'education', 'learning'},\n",
    "            'Financial': {'financial', 'services', 'management', 'capital', 'investment', 'banking', 'advisor'},\n",
    "            'Restaurant': {'restaurant', 'cafe', 'diner', 'eatery', 'grill', 'kitchen', 'bar', 'house'}\n",
    "        }\n",
    "    \n",
    "    def enhanced_preprocessing(self, text, domain=None):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with better handling of merchant-specific patterns\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to preprocess\n",
    "            domain (str, optional): Domain for specialized processing\n",
    "        \n",
    "        Returns:\n",
    "            str: Preprocessed text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Better handling of punctuation - preserve periods in DBANames\n",
    "        # and apostrophes in business names (e.g., McDonald's)\n",
    "        text = re.sub(r'([^a-z0-9\\'\\.\\&\\-])', ' ', text)\n",
    "        \n",
    "        # Special handling for business name apostrophes\n",
    "        text = re.sub(r'\\'s\\b', 's', text)  # Convert McDonald's to McDonalds\n",
    "        \n",
    "        # Expand common business suffixes\n",
    "        business_suffixes = {\n",
    "            r'\\bco\\b': 'company',\n",
    "            r'\\binc\\b': '',  # Remove Inc entirely\n",
    "            r'\\bltd\\b': 'limited',\n",
    "            r'\\bllc\\b': '',  # Remove LLC entirely\n",
    "            r'\\bcorp\\b': 'corporation',\n",
    "            r'\\bcorporation\\b': '',  # Remove when processing full names for matching\n",
    "            r'\\blimited\\b': '',      # Remove when processing full names for matching\n",
    "            r'\\bcompany\\b': '',      # Remove when processing full names for matching\n",
    "        }\n",
    "        \n",
    "        for suffix, replacement in business_suffixes.items():\n",
    "            text = re.sub(suffix, replacement, text)\n",
    "        \n",
    "        # Replace abbreviations\n",
    "        words = text.split()\n",
    "        \n",
    "        # Apply general abbreviation expansion\n",
    "        words = [self.abbreviations.get(word, word) for word in words]\n",
    "        \n",
    "        # Apply domain-specific abbreviation expansion if domain is provided\n",
    "        if domain and domain in self.domain_abbreviations:\n",
    "            words = [self.domain_abbreviations[domain].get(word, word) for word in words]\n",
    "        \n",
    "        # Enhanced handling for McDonalds variations\n",
    "        if any('mc' in word.lower() for word in words):\n",
    "            words = ['mcdonalds' if word.lower() in ['mcd', 'mcds', 'mcdon'] else word for word in words]\n",
    "        \n",
    "        # Remove general stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "        \n",
    "        # Remove domain-specific stopwords if domain is provided\n",
    "        if domain and domain in self.domain_stopwords:\n",
    "            words = [word for word in words if word not in self.domain_stopwords[domain]]\n",
    "        \n",
    "        # Rejoin words and remove extra spaces\n",
    "        text = ' '.join(words)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_pair(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"Preprocess DBAName and full name with domain-specific handling\"\"\"\n",
    "        DBAName_clean = self.enhanced_preprocessing(DBAName, domain)\n",
    "        RawTransactionName_clean = self.enhanced_preprocessing(RawTransactionName, domain)\n",
    "        return DBAName_clean, RawTransactionName_clean\n",
    "\n",
    "# Initialize enhanced merchant matcher with BERT embedder\n",
    "merchant_matcher = EnhancedMerchantMatcher(bert_embedder=bert_embedder)\n",
    "print(\"Enhanced merchant matcher initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380b58a2-5ff5-46ec-9cd9-c0de0f7c2ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain-specific preprocessing and similarity methods added!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Similarity Methods for Merchant Matcher\n",
    "\n",
    "class EnhancedMerchantMatcher(EnhancedMerchantMatcher):\n",
    "    \"\"\"Adding similarity methods to the EnhancedMerchantMatcher class\"\"\"\n",
    "    \n",
    "    def jaro_winkler_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate Jaro-Winkler similarity with enhanced preprocessing\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaro-Winkler similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        return jaro_winkler(DBAName_clean, RawTransactionName_clean)\n",
    "    \n",
    "    def damerau_levenshtein_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate Damerau-Levenshtein similarity, better for handling transpositions\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Damerau-Levenshtein similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Damerau-Levenshtein distance\n",
    "        max_len = max(len(DBAName_clean), len(RawTransactionName_clean))\n",
    "        if max_len == 0:\n",
    "            return 0\n",
    "        \n",
    "        distance = textdistance.damerau_levenshtein.distance(DBAName_clean, RawTransactionName_clean)\n",
    "        similarity = 1 - (distance / max_len)\n",
    "        return max(0, similarity)  # Ensure non-negative\n",
    "    \n",
    "    def tfidf_cosine_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate TF-IDF Cosine similarity for keyword matching\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: TF-IDF cosine similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Fit and transform with TF-IDF\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform([DBAName_clean, RawTransactionName_clean])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return float(max(0, similarity))  # Ensure non-negative\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def jaccard_bigram_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate Jaccard Bigram similarity for character overlaps\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard bigram similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Create bigrams\n",
    "        def get_bigrams(text):\n",
    "            return [text[i:i+2] for i in range(len(text)-1)]\n",
    "        \n",
    "        DBAName_bigrams = set(get_bigrams(DBAName_clean))\n",
    "        RawTransactionName_bigrams = set(get_bigrams(RawTransactionName_clean))\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        union_size = len(DBAName_bigrams.union(RawTransactionName_bigrams))\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        \n",
    "        intersection_size = len(DBAName_bigrams.intersection(RawTransactionName_bigrams))\n",
    "        return intersection_size / union_size\n",
    "    \n",
    "    def soundex_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate phonetic similarity using Soundex algorithm.\n",
    "        Especially useful for similar-sounding business names.\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Phonetic similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # If either string is empty, return 0\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get the soundex codes for both strings\n",
    "        try:\n",
    "            # For multi-word strings, get soundex for each word\n",
    "            DBAName_words = DBAName_clean.split()\n",
    "            RawTransactionName_words = RawTransactionName_clean.split()\n",
    "            \n",
    "            # Get soundex codes for each word\n",
    "            DBAName_codes = [jellyfish.soundex(word) for word in DBAName_words if len(word) > 1]\n",
    "            RawTransactionName_codes = [jellyfish.soundex(word) for word in RawTransactionName_words if len(word) > 1]\n",
    "            \n",
    "            # Calculate matches between codes\n",
    "            matches = 0\n",
    "            total = max(len(DBAName_codes), len(RawTransactionName_codes))\n",
    "            \n",
    "            if total == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # Count matched codes\n",
    "            for code in DBAName_codes:\n",
    "                if code in RawTransactionName_codes:\n",
    "                    matches += 1\n",
    "                    # Remove the matched code to avoid double counting\n",
    "                    RawTransactionName_codes.remove(code)\n",
    "            \n",
    "            return matches / total\n",
    "        except:\n",
    "            # Fallback if there's an error with the soundex calculation\n",
    "            return 0.0\n",
    "    \n",
    "    def token_sort_ratio_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate Token Sort Ratio using fuzzywuzzy.\n",
    "        Handles word order differences well.\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Token sort ratio similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Token Sort Ratio\n",
    "        ratio = fuzz.token_sort_ratio(DBAName, RawTransactionName) / 100\n",
    "        return ratio\n",
    "    \n",
    "    def contains_ratio_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Check if DBAName is contained in full name or vice versa\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Containment similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Check if DBAName is contained in full name\n",
    "        if DBAName_clean in RawTransactionName_clean:\n",
    "            return 1\n",
    "        \n",
    "        # Check if full name is contained in DBAName\n",
    "        if RawTransactionName_clean in DBAName_clean:\n",
    "            return 0.9\n",
    "        \n",
    "        # Check for partial containment\n",
    "        DBAName_chars = list(DBAName_clean)\n",
    "        RawTransactionName_chars = list(RawTransactionName_clean)\n",
    "        \n",
    "        matches = 0\n",
    "        for char in DBAName_chars:\n",
    "            if char in RawTransactionName_chars:\n",
    "                matches += 1\n",
    "                RawTransactionName_chars.remove(char)  # Remove matched char\n",
    "        \n",
    "        return matches / len(DBAName_chars) if len(DBAName_chars) > 0 else 0\n",
    "    \n",
    "    def fuzzy_levenshtein_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate fuzzy Levenshtein ratio for typo tolerance\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Fuzzy Levenshtein similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Levenshtein ratio (which is already normalized)\n",
    "        similarity = levenshtein_ratio(DBAName_clean, RawTransactionName_clean)\n",
    "        return float(similarity)\n",
    "\n",
    "print(\"Domain-specific preprocessing and similarity methods added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc9277-6944-4223-9e62-cb1afec5bb3b",
   "metadata": {},
   "source": [
    "# Enhanced Similarity Methods in the Merchant Matcher\r\n",
    "\r\n",
    "This code expands the `EnhancedMerchantMatcher` class by adding several sophisticated similarity calculation methods specifically designed for comparing merchant names. Let me explain what this code accomplishes and why these methods are important for merchant name matching.\r\n",
    "\r\n",
    "## The Purpose of Multiple Similarity Methods\r\n",
    "\r\n",
    "The code adds a comprehensive suite of similarity calculation techniques to determine when different merchant name strings likely refer to the same business entity. What makes this approach particularly effective is that each similarity method has unique strengths for handling different types of variations in merchant names.\r\n",
    "\r\n",
    "For example, when comparing \"Bank of America\" with \"BoA\":\r\n",
    "- String-based methods might struggle due to limited character overlap\r\n",
    "- Containment methods would recognize \"BoA\" as abbreviation \r\n",
    "- Soundex methods would identify phonetic similarities\r\n",
    "- Token-based methods would handle word reordering\r\n",
    "\r\n",
    "By implementing multiple algorithms, the system can better handle the full spectrum of merchant name variations we encounter in real-world data.\r\n",
    "\r\n",
    "## The Eight Similarity Methods\r\n",
    "\r\n",
    "Let's examine each similarity method and what it contributes to the merchant matching system:\r\n",
    "\r\n",
    "### 1. Jaro-Winkler Similarity\r\n",
    "```python\r\n",
    "def jaro_winkler_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "This method calculates similarity based on character alignment, giving more weight to characters that match at the beginning of strings. It works particularly well for handling misspellings and variations in merchant names while recognizing that prefixes are often more important (e.g., \"McDonald\" vs \"MacDonald\").\r\n",
    "\r\n",
    "### 2. Damerau-Levenshtein Similarity\r\n",
    "```python\r\n",
    "def damerau_levenshtein_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "This algorithm extends standard edit distance calculations by accounting for transpositions (swapped adjacent characters). This is valuable for merchant matching because it can handle common typing errors like \"Starbcuks\" vs \"Starbucks\" more effectively than standard Levenshtein distance.\r\n",
    "\r\n",
    "### 3. TF-IDF Cosine Similarity\r\n",
    "```python\r\n",
    "def tfidf_cosine_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "Rather than comparing character sequences, this method represents the merchant names as weighted term vectors and calculates their cosine similarity. This helps identify when two names share important identifying terms, even if those terms appear in different orders or with other words interspersed, like \"Chase Manhattan Bank\" vs \"Bank of Chase Manhattan.\"\r\n",
    "\r\n",
    "### 4. Jaccard Bigram Similarity\r\n",
    "```python\r\n",
    "def jaccard_bigram_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "By breaking strings into character pairs (bigrams) and measuring the overlap, this method captures character sequence patterns while being more forgiving than exact matching. It's especially helpful for handling slight spelling variations or compound word differences like \"Walmart\" vs \"Wal Mart.\"\r\n",
    "\r\n",
    "### 5. Soundex Similarity\r\n",
    "```python\r\n",
    "def soundex_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "This phonetic algorithm compares how merchant names would sound when pronounced, not just how they're spelled. This captures variations like \"Macy's\" vs \"Macys\" or \"Chick-fil-A\" vs \"Chick Filet\" that sound the same despite different spellings.\r\n",
    "\r\n",
    "### 6. Token Sort Ratio Similarity\r\n",
    "```python\r\n",
    "def token_sort_ratio_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "By tokenizing strings into words, sorting them alphabetically, and then comparing, this method handles word order differences effectively. This is crucial for matching merchant names where word order varies, like \"Home Depot\" vs \"The Depot Home Center.\"\r\n",
    "\r\n",
    "### 7. Contains Ratio Similarity\r\n",
    "```python\r\n",
    "def contains_ratio_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "This method specifically checks whether one string contains the other, which is perfect for DBAName-to-full-name matching. It also implements partial matching by checking character inclusion, which helps with incomplete matches like \"AMZN\" vs \"Amazon.\"\r\n",
    "\r\n",
    "### 8. Fuzzy Levenshtein Similarity\r\n",
    "```python\r\n",
    "def fuzzy_levenshtein_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "This implementation uses the Levenshtein ratio to provide a normalized measure of string edit distance, offering another way to handle typos and character substitutions in merchant names.\r\n",
    "\r\n",
    "## Technical Implementation Details\r\n",
    "\r\n",
    "Each similarity method follows a consistent pattern:\r\n",
    "\r\n",
    "1. **Preprocessing**: All methods begin by calling `preprocess_pair()` to normalize and clean the input strings based on domain knowledge.\r\n",
    "2. **Empty string handling**: Each method checks for empty strings after preprocessing and returns 0 if either is empty.\r\n",
    "3. **Algorithm application**: The specific similarity algorithm is then applied to the preprocessed strings.\r\n",
    "4. **Normalization**: Results are normalized to a 0-1 scale, where 1 indicates a perfect match.\r\n",
    "5. **Error handling**: Many methods include error handling to prevent exceptions from interrupting the matching process.\r\n",
    "\r\n",
    "## The Role of Domain Knowledge\r\n",
    "\r\n",
    "A key strength of this implementation is that each method integrates domain-specific knowledge through the optional `domain` parameter:\r\n",
    "\r\n",
    "```python\r\n",
    "DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\r\n",
    "```\r\n",
    "\r\n",
    "This allows the similarity calculations to leverage specialized preprocessing for different industries. For example, in the financial domain, it would understand that \"Cap1\" and \"Capital One\" are the same entity based on financial industry abbreviation patterns.\r\n",
    "\r\n",
    "## Practical Applications and Value\r\n",
    "\r\n",
    "The implementation of these diverse similarity methods enables the merchant matcher to:\r\n",
    "\r\n",
    "1. **Handle ambiguity**: By combining multiple similarity signals, the system can make more confident decisions in ambiguous cases.\r\n",
    "\r\n",
    "2. **Adapt to different merchant name patterns**: Some industries use more abbreviations, others use more location identifiers, and this multi-algorithm approach handles that diversity.\r\n",
    "\r\n",
    "3. **Balance precision and recall**: Different similarity methods offer different trade-offs between false positives and false negatives.\r\n",
    "\r\n",
    "4. **Process different aspects of merchant name similarity**: Character-level, token-level, phonetic, and semantic similarities are all captured through different methods.\r\n",
    "\r\n",
    "When used together, potentially with weighted combinations, these methods form a robust system for determining when different merchant name strings refer to the same real-world entitya challenging but important problem in data integration, financial analysis, and business intelligence applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "896a72c7-23d8-40b3-b6d7-427223fc3d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced pattern recognition and score boosting methods added!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Advanced Pattern Recognition Methods\n",
    "\n",
    "class EnhancedMerchantMatcher(EnhancedMerchantMatcher):\n",
    "    \"\"\"Adding pattern recognition and score boosting methods\"\"\"\n",
    "    \n",
    "    def trie_approximate_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Use approximate matching for DBAName formation detection\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Trie approximate similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Extract first letters from each word in full name\n",
    "        words = RawTransactionName_clean.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # Check if DBAName matches first letters\n",
    "        if DBAName_clean.lower() == first_letters.lower():\n",
    "            return 1\n",
    "        \n",
    "        # Calculate similarity for approximate matching\n",
    "        max_len = max(len(DBAName_clean), len(first_letters))\n",
    "        if max_len == 0:\n",
    "            return 0\n",
    "        \n",
    "        distance = levenshtein_distance(DBAName_clean.lower(), first_letters.lower())\n",
    "        similarity = 1 - (distance / max_len)\n",
    "        return max(0, similarity)\n",
    "    \n",
    "    def aho_corasick_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Use Aho-Corasick algorithm for pattern matching\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Aho-Corasick similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        if not aho_corasick_available:\n",
    "            # Fallback implementation when pyahocorasick is not available\n",
    "            matches = 0\n",
    "            remaining_text = RawTransactionName_clean\n",
    "            for c in DBAName_clean:\n",
    "                if c in remaining_text:\n",
    "                    matches += 1\n",
    "                    # Remove matched character to prevent duplicate counting\n",
    "                    idx = remaining_text.find(c)\n",
    "                    remaining_text = remaining_text[:idx] + remaining_text[idx+1:]\n",
    "            \n",
    "            return min(1.0, matches / len(DBAName_clean)) if len(DBAName_clean) > 0 else 0\n",
    "        \n",
    "        # Build automaton\n",
    "        automaton = pyahocorasick.Automaton()\n",
    "        for i, c in enumerate(DBAName_clean):\n",
    "            automaton.add_word(c, (i, c))\n",
    "        automaton.make_automaton()\n",
    "        \n",
    "        # Find matches\n",
    "        matches = 0\n",
    "        for _, (_, c) in automaton.iter(RawTransactionName_clean):\n",
    "            matches += 1\n",
    "        \n",
    "        # Calculate score\n",
    "        if len(DBAName_clean) == 0:\n",
    "            return 0\n",
    "        \n",
    "        return min(1.0, matches / len(DBAName_clean))\n",
    "    \n",
    "    def bert_similarity(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate semantic similarity using BERT embeddings\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: BERT similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # If BERT embedder is not initialized, return 0\n",
    "        if self.bert_embedder is None:\n",
    "            return 0\n",
    "        \n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            # Get embeddings from pre-trained model\n",
    "            emb1 = self.bert_embedder.encode(DBAName_clean)\n",
    "            emb2 = self.bert_embedder.encode(RawTransactionName_clean)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            dot_product = np.sum(emb1 * emb2)\n",
    "            norm1 = np.linalg.norm(emb1)\n",
    "            norm2 = np.linalg.norm(emb2)\n",
    "            \n",
    "            similarity = dot_product / (norm1 * norm2 + 1e-8)\n",
    "            return float(similarity)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in BERT similarity calculation: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def DBAName_formation_score(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate how well the DBAName is formed from the full name\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: DBAName formation score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Extract first letters from each word in full name\n",
    "        words = RawTransactionName_clean.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        # Standard DBAName formation - first letter of each word\n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # If exact match, return 1\n",
    "        if DBAName_clean.lower() == first_letters.lower():\n",
    "            return 1\n",
    "        \n",
    "        # Check partial match\n",
    "        DBAName_chars = list(DBAName_clean.lower())\n",
    "        first_letters_chars = list(first_letters.lower())\n",
    "        \n",
    "        matches = 0\n",
    "        for char in DBAName_chars:\n",
    "            if char in first_letters_chars:\n",
    "                matches += 1\n",
    "                first_letters_chars.remove(char)  # Remove matched char\n",
    "        \n",
    "        if len(DBAName_chars) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate partial match score\n",
    "        return matches / len(DBAName_chars)\n",
    "    \n",
    "    def enhanced_DBAName_formation_score(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Enhanced DBAName formation score with special handling for common patterns\n",
    "        particularly optimized for business names with prefixes like \"Mc\".\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to evaluate\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Enhanced DBAName formation score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        # Basic cleanup\n",
    "        DBAName = DBAName_clean.lower()\n",
    "        RawTransactionName = RawTransactionName_clean.lower()\n",
    "        \n",
    "        # Special case for \"Mc\" prefixes (common in restaurant names)\n",
    "        if RawTransactionName.startswith('mc') and len(DBAName) >= 1 and DBAName[0] == 'm':\n",
    "            # McDonalds -> MCD pattern\n",
    "            modified_RawTransactionName = RawTransactionName[2:]  # Remove \"mc\"\n",
    "            remaining_chars = DBAName[1:]  # Remove \"m\"\n",
    "            \n",
    "            # For \"MCD\" -> \"McDonalds\" pattern\n",
    "            if remaining_chars and len(modified_RawTransactionName) > 0:\n",
    "                # Check if remaining chars match consonants in the name\n",
    "                consonants = ''.join([c for c in modified_RawTransactionName if c not in 'aeiou'])\n",
    "                if remaining_chars in consonants:\n",
    "                    return 0.95\n",
    "                \n",
    "                # Check if first few consonants match remaining chars\n",
    "                first_consonants = ''.join([c for c in modified_RawTransactionName[:len(remaining_chars)*2] \n",
    "                                          if c not in 'aeiou'])\n",
    "                if remaining_chars in first_consonants:\n",
    "                    return 0.90\n",
    "                \n",
    "                # Check first letters after \"Mc\"\n",
    "                words = modified_RawTransactionName.split()\n",
    "                if words:\n",
    "                    first_letters = ''.join([word[0] for word in words if word])\n",
    "                    if remaining_chars in first_letters:\n",
    "                        return 0.90\n",
    "                    \n",
    "                    # Check if remaining chars appear in sequence in the words\n",
    "                    current_word_position = 0\n",
    "                    chars_found = 0\n",
    "                    for char in remaining_chars:\n",
    "                        for i in range(current_word_position, len(words)):\n",
    "                            if char in words[i]:\n",
    "                                chars_found += 1\n",
    "                                current_word_position = i + 1\n",
    "                                break\n",
    "                    \n",
    "                    if chars_found == len(remaining_chars):\n",
    "                        return 0.85\n",
    "            \n",
    "            # Even if not a perfect match, it's still a good score for Mc prefix\n",
    "            return 0.80\n",
    "        \n",
    "        # Check for brand name with location pattern (Toyota Corporation -> Western Toyota)\n",
    "        common_brands = ['toyota', 'ford', 'honda', 'bmw', 'walmart', 'target', 'starbucks']\n",
    "        location_prefixes = ['north', 'south', 'east', 'west', 'western', 'eastern', 'central']\n",
    "        \n",
    "        # Extract the key brand name (if present)\n",
    "        brand_match = None\n",
    "        for brand in common_brands:\n",
    "            if brand in DBAName.lower():\n",
    "                brand_match = brand\n",
    "                break\n",
    "            if brand in RawTransactionName.lower():\n",
    "                brand_match = brand\n",
    "                break\n",
    "        \n",
    "        if brand_match:\n",
    "            # Check if one name has the brand with a location prefix/suffix and the other has just the brand\n",
    "            has_location_prefix = any(prefix in DBAName.lower() or prefix in RawTransactionName.lower() \n",
    "                                     for prefix in location_prefixes)\n",
    "            \n",
    "            if has_location_prefix:\n",
    "                # If both contain the brand name but one has location prefix\n",
    "                if brand_match in DBAName.lower() and brand_match in RawTransactionName.lower():\n",
    "                    return 0.92\n",
    "        \n",
    "        # Standard DBAName formation - first letter of each word\n",
    "        words = RawTransactionName.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        # Get first letters\n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # If exact match, return high score\n",
    "        if DBAName == first_letters:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check for consonant-based DBAName (common in business DBANames)\n",
    "        consonants = ''.join([c for c in RawTransactionName if c not in 'aeiou' and c.isalpha()])\n",
    "        consonant_match = 0.0\n",
    "        if len(DBAName) <= len(consonants):\n",
    "            # Check for sequential consonant match\n",
    "            DBAName_position = 0\n",
    "            for i, c in enumerate(consonants):\n",
    "                if DBAName_position < len(DBAName) and c == DBAName[DBAName_position]:\n",
    "                    DBAName_position += 1\n",
    "            \n",
    "            consonant_sequential_match = DBAName_position / len(DBAName) if len(DBAName) > 0 else 0\n",
    "            \n",
    "            # Check for any consonant match\n",
    "            matches = 0\n",
    "            consonants_copy = consonants\n",
    "            for char in DBAName:\n",
    "                if char in consonants_copy:\n",
    "                    matches += 1\n",
    "                    consonants_copy = consonants_copy.replace(char, '', 1)\n",
    "            \n",
    "            consonant_any_match = matches / len(DBAName) if len(DBAName) > 0 else 0\n",
    "            \n",
    "            # Take the better score\n",
    "            consonant_match = max(consonant_sequential_match, consonant_any_match)\n",
    "            \n",
    "            # Give higher scores for strong consonant matches\n",
    "            if consonant_match > 0.7:\n",
    "                return max(0.85, consonant_match)\n",
    "        \n",
    "        # Calculate ordered match score\n",
    "        ordered_match = 0\n",
    "        last_found_index = -1\n",
    "        RawTransactionName_chars = list(RawTransactionName)\n",
    "        \n",
    "        for char in DBAName:\n",
    "            found = False\n",
    "            for i in range(last_found_index + 1, len(RawTransactionName_chars)):\n",
    "                if char == RawTransactionName_chars[i]:\n",
    "                    ordered_match += 1\n",
    "                    last_found_index = i\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            # If we couldn't find the character in order, try looking anywhere\n",
    "            if not found:\n",
    "                for i in range(len(RawTransactionName_chars)):\n",
    "                    if i != last_found_index and char == RawTransactionName_chars[i]:\n",
    "                        ordered_match += 0.5  # Half credit for out-of-order match\n",
    "                        RawTransactionName_chars[i] = '_'  # Mark as used\n",
    "                        break\n",
    "        \n",
    "        ordered_match_score = ordered_match / len(DBAName) if len(DBAName) > 0 else 0\n",
    "        \n",
    "        # Return the best score from different matching strategies\n",
    "        return max(\n",
    "            ordered_match_score * 0.9,  # Ordered match is good but not perfect\n",
    "            consonant_match * 0.9,      # Consonant match is also valuable\n",
    "            0.4                         # Minimum score to prevent too low values\n",
    "        )\n",
    "    \n",
    "    def detect_complex_business_patterns(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Detect complex business name patterns that might be missed by basic algorithms\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName or short form\n",
    "            RawTransactionName (str): The full merchant name\n",
    "            domain (str, optional): Domain for specialized processing\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of detected patterns with confidence scores\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        DBAName = DBAName_clean.lower()\n",
    "        RawTransactionName = RawTransactionName_clean.lower()\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # Government agency pattern (Dept of X <-> X Department)\n",
    "        agency_terms = ['department', 'dept', 'ministry', 'office', 'bureau', 'administration', 'agency']\n",
    "        has_agency_DBAName = any(term in DBAName for term in agency_terms)\n",
    "        has_agency_full = any(term in RawTransactionName for term in agency_terms)\n",
    "        \n",
    "        if has_agency_DBAName or has_agency_full:\n",
    "            # Check for inverted department structure pattern (common in government)\n",
    "            # e.g., \"Department of Treasury\" vs \"Treasury Department\"\n",
    "            words_DBAName = DBAName.split()\n",
    "            words_full = RawTransactionName.split()\n",
    "            \n",
    "            # Find agency term positions\n",
    "            agency_pos_a = -1\n",
    "            agency_pos_f = -1\n",
    "            \n",
    "            for term in agency_terms:\n",
    "                if agency_pos_a == -1 and any(term in word for word in words_DBAName):\n",
    "                    agency_pos_a = next((i for i, word in enumerate(words_DBAName) if term in word), -1)\n",
    "                if agency_pos_f == -1 and any(term in word for word in words_full):\n",
    "                    agency_pos_f = next((i for i, word in enumerate(words_full) if term in word), -1)\n",
    "                    \n",
    "            if agency_pos_a != -1 and agency_pos_f != -1:\n",
    "                # One at beginning, one at end (inverted structure)\n",
    "                if (agency_pos_a == 0 and agency_pos_f == len(words_full) - 1) or \\\n",
    "                   (agency_pos_f == 0 and agency_pos_a == len(words_DBAName) - 1):\n",
    "                    patterns['inverted_agency_structure'] = 1.0\n",
    "                else:\n",
    "                    patterns['similar_agency_structure'] = 0.7\n",
    "        \n",
    "        # Financial institution pattern\n",
    "        bank_terms = ['bank', 'credit union', 'financial', 'savings', 'investment', 'trust']\n",
    "        has_bank_DBAName = any(term in DBAName for term in bank_terms)\n",
    "        has_bank_full = any(term in RawTransactionName for term in bank_terms)\n",
    "        \n",
    "        if has_bank_DBAName or has_bank_full:\n",
    "            # Check for Bank of X vs X Bank pattern\n",
    "            if ('bank of' in DBAName and 'bank' in RawTransactionName and 'of' not in RawTransactionName) or \\\n",
    "               ('bank of' in RawTransactionName and 'bank' in DBAName and 'of' not in DBAName):\n",
    "                patterns['bank_name_inversion'] = 1.0\n",
    "        \n",
    "        # Abbreviation with ampersand pattern\n",
    "        if '&' in RawTransactionName or 'and' in RawTransactionName:\n",
    "            # Check if DBAName contains first letters of parts around ampersand\n",
    "            parts = re.split(r'\\s+&\\s+|\\s+and\\s+', RawTransactionName)\n",
    "            if len(parts) >= 2:\n",
    "                first_letters = ''.join(part[0] for part in parts if part)\n",
    "                if DBAName == first_letters:\n",
    "                    patterns['ampersand_DBAName'] = 1.0\n",
    "                elif all(letter in DBAName for letter in first_letters):\n",
    "                    patterns['partial_ampersand_DBAName'] = 0.8\n",
    "        \n",
    "        # Multi-word business name with DBAName\n",
    "        words_full = [w for w in RawTransactionName.split() if len(w) > 3]  # Only consider significant words\n",
    "        if len(words_full) >= 3 and len(DBAName) >= 2:\n",
    "            # Check if DBAName consists of first letters of significant words\n",
    "            first_letters = ''.join(word[0] for word in words_full)\n",
    "            if DBAName in first_letters:\n",
    "                patterns['multiword_business_DBAName'] = 0.9\n",
    "        \n",
    "        # Regional/branch variation of business\n",
    "        location_prefixes = ['north', 'south', 'east', 'west', 'central', 'metro', 'city', \n",
    "                            'downtown', 'regional', 'national', 'global', 'local']\n",
    "        \n",
    "        has_location_a = any(prefix in DBAName.split() for prefix in location_prefixes)\n",
    "        has_location_f = any(prefix in RawTransactionName.split() for prefix in location_prefixes)\n",
    "        \n",
    "        if has_location_a != has_location_f:  # One has location, other doesn't\n",
    "            # Remove location terms and compare the rest\n",
    "            a_words = [w for w in DBAName.split() if w not in location_prefixes]\n",
    "            f_words = [w for w in RawTransactionName.split() if w not in location_prefixes]\n",
    "            \n",
    "            # If remaining content is similar\n",
    "            a_text = ' '.join(a_words)\n",
    "            f_text = ' '.join(f_words)\n",
    "            \n",
    "            if a_text in f_text or f_text in a_text:\n",
    "                patterns['regional_branch_variation'] = 1.0\n",
    "            elif len(a_text) > 3 and len(f_text) > 3 and (a_text[:3] == f_text[:3]):\n",
    "                patterns['potential_branch_variation'] = 0.7\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def get_all_similarity_scores(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate all similarity scores at once for efficiency\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of algorithm name to score\n",
    "        \"\"\"\n",
    "        # Return empty dictionary if either DBAName or RawTransactionName is None\n",
    "        if DBAName is None or RawTransactionName is None:\n",
    "            return {}\n",
    "        \n",
    "        # Calculate all similarity scores\n",
    "        scores = {\n",
    "            'jaro_winkler': self.jaro_winkler_similarity(DBAName, RawTransactionName, domain),\n",
    "            'damerau_levenshtein': self.damerau_levenshtein_similarity(DBAName, RawTransactionName, domain),\n",
    "            'tfidf_cosine': self.tfidf_cosine_similarity(DBAName, RawTransactionName, domain),\n",
    "            'jaccard_bigram': self.jaccard_bigram_similarity(DBAName, RawTransactionName, domain),\n",
    "            'soundex': self.soundex_similarity(DBAName, RawTransactionName, domain),\n",
    "            'token_sort_ratio': self.token_sort_ratio_similarity(DBAName, RawTransactionName, domain),\n",
    "            'contains_ratio': self.contains_ratio_similarity(DBAName, RawTransactionName, domain),\n",
    "            'fuzzy_levenshtein': self.fuzzy_levenshtein_similarity(DBAName, RawTransactionName, domain),\n",
    "            'trie_approximate': self.trie_approximate_similarity(DBAName, RawTransactionName, domain),\n",
    "            'aho_corasick': self.aho_corasick_similarity(DBAName, RawTransactionName, domain),\n",
    "            'DBAName_formation': self.DBAName_formation_score(DBAName, RawTransactionName, domain),\n",
    "            'enhanced_DBAName_formation': self.enhanced_DBAName_formation_score(DBAName, RawTransactionName, domain)\n",
    "        }\n",
    "        \n",
    "        # Add BERT similarity if available\n",
    "        if self.bert_embedder is not None:\n",
    "            scores['bert_similarity'] = self.bert_similarity(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        # Add pattern detection scores\n",
    "        pattern_results = self.detect_complex_business_patterns(DBAName, RawTransactionName, domain)\n",
    "        for pattern, score in pattern_results.items():\n",
    "            scores[f'pattern_{pattern}'] = score\n",
    "        \n",
    "        return scores\n",
    "\n",
    "print(\"Advanced pattern recognition and score boosting methods added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbea80-dbac-45a4-89cd-a8fbc99f3a49",
   "metadata": {},
   "source": [
    "# Advanced Pattern Recognition in Merchant Name Matching\r\n",
    "\r\n",
    "This code significantly expands the `EnhancedMerchantMatcher` class by adding sophisticated pattern recognition capabilities that are specifically designed to address the complex challenge of merchant name matching. Let me walk you through what this code accomplishes and why these advanced techniques are critical for accurate merchant name matching.\r\n",
    "\r\n",
    "## Purpose and Value\r\n",
    "\r\n",
    "When dealing with merchant names in real-world data, we frequently encounter complex variations that go beyond simple string similarity. For example, \"Bank of America\" might appear as \"BoA,\" \"McDonalds\" as \"MCD,\" or \"Department of Treasury\" as \"Treasury Department.\" These variations follow patterns that traditional string matching algorithms struggle to identify.\r\n",
    "\r\n",
    "The code implements specialized algorithms that can recognize these domain-specific patterns, significantly improving matching accuracy for real-world merchant data.\r\n",
    "\r\n",
    "## Key Capabilities Added\r\n",
    "\r\n",
    "### 1. DBAName Formation Detection\r\n",
    "\r\n",
    "Two methods specifically address how DBANames are formed from full names:\r\n",
    "\r\n",
    "```python\r\n",
    "def DBAName_formation_score(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "This method calculates how well an DBAName follows the standard pattern of taking the first letter of each word in the full name. For example, \"IBM\" from \"International Business Machines.\"\r\n",
    "\r\n",
    "```python\r\n",
    "def enhanced_DBAName_formation_score(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "This sophisticated method goes far beyond basic DBAName detection by incorporating domain knowledge about how business DBANames are commonly formed. It includes special handling for:\r\n",
    "\r\n",
    "- Names with \"Mc\" prefixes (like \"MCD\" for \"McDonalds\")\r\n",
    "- Consonant-based DBANames (common in business names)\r\n",
    "- Location-modified brand names (like \"Western Toyota\" vs \"Toyota Corporation\")\r\n",
    "- Sequential character matching with partial credit for out-of-order matches\r\n",
    "\r\n",
    "The method applies multiple strategies and returns the best score, accommodating the varied ways companies form DBANames in the real world.\r\n",
    "\r\n",
    "### 2. Pattern Matching Algorithms\r\n",
    "\r\n",
    "The code implements two pattern matching algorithms:\r\n",
    "\r\n",
    "```python\r\n",
    "def trie_approximate_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "This method compares DBANames against first letters of each word in the full name, using edit distance to allow for approximate matches.\r\n",
    "\r\n",
    "```python\r\n",
    "def aho_corasick_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "This implements the Aho-Corasick string matching algorithm (with a fallback implementation if the library isn't available). It efficiently finds characters of the DBAName within the full name, particularly useful for non-standard DBAName formations.\r\n",
    "\r\n",
    "### 3. Semantic Understanding with BERT\r\n",
    "\r\n",
    "```python\r\n",
    "def bert_similarity(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "This method leverages the BERT neural network model to understand semantic relationships between merchant names. Unlike traditional string matching, BERT can recognize that \"Golden Arches\" and \"McDonalds\" refer to the same entity based on learned semantic associations, even though they share no characters.\r\n",
    "\r\n",
    "### 4. Business Pattern Recognition\r\n",
    "\r\n",
    "The most sophisticated addition is the complex business pattern detector:\r\n",
    "\r\n",
    "```python\r\n",
    "def detect_complex_business_patterns(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "This method identifies specific patterns frequently found in business naming conventions:\r\n",
    "\r\n",
    "1. **Government Agency Inversions**: Recognizing that \"Department of Defense\" and \"Defense Department\" are equivalent\r\n",
    "\r\n",
    "2. **Financial Institution Patterns**: Identifying \"Bank of X\" vs \"X Bank\" equivalences \r\n",
    "\r\n",
    "3. **Ampersand Patterns**: Detecting DBANames formed from words surrounding ampersands (like \"AT&T\" from \"American Telephone & Telegraph\")\r\n",
    "\r\n",
    "4. **Multi-word Business DBANames**: Identifying standard business DBAName formation patterns\r\n",
    "\r\n",
    "5. **Regional/Branch Variations**: Recognizing when names differ only by location prefixes (like \"North Western Bank\" vs \"Western Bank\")\r\n",
    "\r\n",
    "For each detected pattern, the method returns a confidence score indicating how strongly the pattern applies to the given merchant name pair.\r\n",
    "\r\n",
    "### 5. Comprehensive Score Aggregation\r\n",
    "\r\n",
    "```python\r\n",
    "def get_all_similarity_scores(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "This method efficiently calculates all similarity metrics in a single function call, returning a dictionary of scores. This approach allows the overall matcher to:\r\n",
    "\r\n",
    "1. Access all similarity signals at once\r\n",
    "2. Apply domain-specific weighting to different similarity measures\r\n",
    "3. Make more informed matching decisions by considering multiple perspectives\r\n",
    "\r\n",
    "## Technical Sophistication\r\n",
    "\r\n",
    "Several aspects of this implementation demonstrate significant technical sophistication:\r\n",
    "\r\n",
    "### Pattern-Specific Optimizations\r\n",
    "\r\n",
    "Each pattern recognition method includes targeted optimizations for specific business naming conventions. For example, the special handling for \"Mc\" prefixes shows deep domain knowledge about restaurant chain naming patterns.\r\n",
    "\r\n",
    "### Graceful Degradation\r\n",
    "\r\n",
    "The code provides fallback implementations when advanced libraries (like `pyahocorasick`) aren't available, ensuring the system maintains functionality across different environments.\r\n",
    "\r\n",
    "### Multiple Matching Strategies\r\n",
    "\r\n",
    "Rather than relying on a single approach, the code implements multiple complementary strategies for each pattern recognition task, often taking the maximum score across different techniques.\r\n",
    "\r\n",
    "### Domain Awareness\r\n",
    "\r\n",
    "Every method accepts an optional `domain` parameter to apply industry-specific preprocessing and pattern recognition, acknowledging that merchant naming conventions differ across sectors.\r\n",
    "\r\n",
    "## Real-World Applications\r\n",
    "\r\n",
    "This advanced pattern recognition capability addresses several critical real-world challenges:\r\n",
    "\r\n",
    "1. **Financial Transaction Processing**: Normalizing merchant names in credit card statements, enabling accurate merchant-level analytics\r\n",
    "\r\n",
    "2. **Regulatory Compliance**: Matching company names against sanctions or watchlists, where missing a match could have legal consequences\r\n",
    "\r\n",
    "3. **Customer Data Integration**: Consolidating customer records across systems that may use different naming conventions for the same merchants\r\n",
    "\r\n",
    "4. **Business Intelligence**: Aggregating transaction data by merchant entity, requiring recognition of all naming variations\r\n",
    "\r\n",
    "5. **Fraud Detection**: Identifying when seemingly different merchant names might actually be the same entity using different aliases\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This code represents a sophisticated approach to merchant name matching that goes well beyond conventional string similarity algorithms. By incorporating domain-specific pattern recognition, neural semantic understanding, and comprehensive business naming knowledge, it can identify matches that would be missed by simpler approaches.\r\n",
    "\r\n",
    "The implementation combines rule-based pattern recognition with machine learning (BERT), leveraging the strengths of both approaches to create a robust merchant name matching system capable of handling the complex variations encountered in real-world business data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc319b14-17fb-41e1-bead-cd2fff11e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic weighting and enhanced scoring added with 54 common DBANames!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Dynamic Weighting and Enhanced Scoring\n",
    "\n",
    "class EnhancedMerchantMatcher(EnhancedMerchantMatcher):\n",
    "    \"\"\"Adding dynamic weighting and enhanced scoring methods\"\"\"\n",
    "    \n",
    "    def get_dynamic_weights(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Get dynamically adjusted weights based on merchant name characteristics\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName or short name\n",
    "            RawTransactionName (str): The full merchant name\n",
    "            domain (str, optional): The business domain\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of dynamically adjusted algorithm weights\n",
    "        \"\"\"\n",
    "        # Start with base weights\n",
    "        weights = {\n",
    "            'jaro_winkler': 0.10,\n",
    "            'damerau_levenshtein': 0.05,\n",
    "            'tfidf_cosine': 0.05,\n",
    "            'jaccard_bigram': 0.05,\n",
    "            'soundex': 0.05,\n",
    "            'token_sort_ratio': 0.10,\n",
    "            'contains_ratio': 0.10,\n",
    "            'fuzzy_levenshtein': 0.05,\n",
    "            'trie_approximate': 0.10,\n",
    "            'bert_similarity': 0.15,\n",
    "            'aho_corasick': 0.05,\n",
    "            'DBAName_formation': 0.15\n",
    "        }\n",
    "        \n",
    "        # Adjust weights based on name characteristics\n",
    "        DBAName_len = len(DBAName) if isinstance(DBAName, str) else 0\n",
    "        RawTransactionName_len = len(RawTransactionName) if isinstance(RawTransactionName, str) else 0\n",
    "        \n",
    "        # For very short DBANames (2-3 chars), boost DBAName formation importance\n",
    "        if 2 <= DBAName_len <= 3:\n",
    "            weights['DBAName_formation'] = 0.25\n",
    "            weights['enhanced_DBAName_formation'] = 0.20\n",
    "            weights['bert_similarity'] = 0.15\n",
    "            weights['contains_ratio'] = 0.15\n",
    "            \n",
    "        # For longer DBANames, focus more on semantic similarity\n",
    "        elif DBAName_len >= 4:\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            weights['token_sort_ratio'] = 0.15\n",
    "            \n",
    "        # For very long full names, semantic understanding becomes more important\n",
    "        if RawTransactionName_len > 30:\n",
    "            weights['bert_similarity'] = 0.30\n",
    "            weights['tfidf_cosine'] = 0.15\n",
    "            \n",
    "        # If full name contains \"Bank\" or related terms, boost specific algorithms\n",
    "        if isinstance(RawTransactionName, str) and re.search(r'\\b(bank|credit|financial|capital)\\b', RawTransactionName.lower()):\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            weights['DBAName_formation'] = 0.20\n",
    "            \n",
    "        # If full name contains location indicators (east, west, north, south)\n",
    "        if isinstance(RawTransactionName, str) and re.search(r'\\b(east|west|north|south|central)\\b', RawTransactionName.lower()):\n",
    "            weights['token_sort_ratio'] = 0.20\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            \n",
    "        # Domain-specific adjustments\n",
    "        if domain == 'Restaurant':\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            weights['fuzzy_levenshtein'] = 0.15\n",
    "        elif domain == 'Banking':\n",
    "            weights['DBAName_formation'] = 0.25\n",
    "            weights['enhanced_DBAName_formation'] = 0.25\n",
    "            weights['bert_similarity'] = 0.20\n",
    "        elif domain == 'Government':\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            weights['DBAName_formation'] = 0.20\n",
    "            weights['token_sort_ratio'] = 0.15\n",
    "        elif domain == 'Medical':\n",
    "            weights['soundex'] = 0.15\n",
    "            weights['bert_similarity'] = 0.25\n",
    "        elif domain == 'Automotive':\n",
    "            weights['contains_ratio'] = 0.15\n",
    "            weights['token_sort_ratio'] = 0.15\n",
    "            weights['bert_similarity'] = 0.20\n",
    "        \n",
    "        # If enhanced DBAName formation is available, use it instead of standard DBAName formation\n",
    "        if 'enhanced_DBAName_formation' not in weights and 'DBAName_formation' in weights:\n",
    "            weights['enhanced_DBAName_formation'] = weights['DBAName_formation']\n",
    "            weights['DBAName_formation'] = weights['DBAName_formation'] * 0.5  # Reduce standard weight\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        weight_sum = sum(weights.values())\n",
    "        return {k: v/weight_sum for k, v in weights.items()}\n",
    "    \n",
    "    def compute_contextual_score(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Compute similarity score with contextual score boosting\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Enhanced similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Get all similarity scores\n",
    "        all_scores = self.get_all_similarity_scores(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        # Get dynamic weights based on name characteristics\n",
    "        weights = self.get_dynamic_weights(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        # Calculate base weighted score\n",
    "        weighted_score = 0.0\n",
    "        weights_used = 0.0\n",
    "        \n",
    "        for algo, score in all_scores.items():\n",
    "            if algo in weights:\n",
    "                weighted_score += weights[algo] * score\n",
    "                weights_used += weights[algo]\n",
    "        \n",
    "        # Handle case where some algorithms are missing\n",
    "        if weights_used > 0:\n",
    "            # Normalize by weights actually used\n",
    "            weighted_score /= weights_used\n",
    "        \n",
    "        # Apply pattern-based boosting\n",
    "        pattern_boost = 1.0\n",
    "        for algo, score in all_scores.items():\n",
    "            if algo.startswith('pattern_') and score > 0:\n",
    "                # Different boost factors for different patterns\n",
    "                if 'inverted_agency_structure' in algo:\n",
    "                    pattern_boost += 0.35  # 35% boost for inverted agency structure\n",
    "                elif 'bank_name_inversion' in algo:\n",
    "                    pattern_boost += 0.35  # 35% boost for bank name inversion\n",
    "                elif 'ampersand_DBAName' in algo:\n",
    "                    pattern_boost += 0.30  # 30% boost for ampersand DBAName\n",
    "                elif 'multiword_business_DBAName' in algo:\n",
    "                    pattern_boost += 0.25  # 25% boost for multiword business DBAName\n",
    "                elif 'regional_branch_variation' in algo:\n",
    "                    pattern_boost += 0.35  # 35% boost for regional branch variation\n",
    "                elif 'partial' in algo:\n",
    "                    pattern_boost += 0.20  # 20% boost for partial patterns\n",
    "                else:\n",
    "                    pattern_boost += 0.15  # 15% boost for other patterns\n",
    "        \n",
    "        # Apply special boost for McDonalds patterns\n",
    "        if (('mc' in DBAName.lower() and 'donald' in RawTransactionName.lower()) or \n",
    "            ('mc' in RawTransactionName.lower() and 'donald' in DBAName.lower())):\n",
    "            pattern_boost += 0.40  # 40% boost for McDonalds patterns\n",
    "        \n",
    "        # Special case for banking abbreviations\n",
    "        banking_abbrs = {'bofa', 'boa', 'jpmc', 'wf', 'citi', 'hsbc', 'rbc', 'pnc', 'bny', 'cba', 'nab', 'rbs'}\n",
    "        DBAName_lower = DBAName.lower() if isinstance(DBAName, str) else ''\n",
    "        RawTransactionName_lower = RawTransactionName.lower() if isinstance(RawTransactionName, str) else ''\n",
    "        \n",
    "        if (DBAName_lower in banking_abbrs or any(abbr in DBAName_lower for abbr in banking_abbrs)) and \\\n",
    "           ('bank' in RawTransactionName_lower or 'financial' in RawTransactionName_lower):\n",
    "            pattern_boost += 0.30  # 30% boost for banking abbreviations\n",
    "        \n",
    "        # Apply high scores for individual algorithm boosting\n",
    "        high_score_algos = ['bert_similarity', 'enhanced_DBAName_formation', 'token_sort_ratio']\n",
    "        for algo in high_score_algos:\n",
    "            if algo in all_scores and all_scores[algo] > 0.9:\n",
    "                pattern_boost += 0.20  # 20% boost for high individual scores\n",
    "                break\n",
    "        \n",
    "        # Apply the pattern boost, cap at 1.6 (60% boost max)\n",
    "        boosted_score = min(1.0, weighted_score * min(pattern_boost, 1.6))\n",
    "        \n",
    "        # Super-boost scores that are already reasonably good but below threshold\n",
    "        if 0.6 < boosted_score < 0.75:\n",
    "            boosted_score = min(1.0, boosted_score * 1.2)  # 20% boost for \"on the fence\" scores\n",
    "        \n",
    "        return boosted_score\n",
    "    \n",
    "    def compute_weighted_score(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Compute weighted similarity score using domain-specific weights\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Weighted similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Get all similarity scores\n",
    "        all_scores = self.get_all_similarity_scores(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        # Get dynamic weights based on name characteristics\n",
    "        weights = self.get_dynamic_weights(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        weighted_score = 0.0\n",
    "        weights_used = 0.0\n",
    "        \n",
    "        for algo, score in all_scores.items():\n",
    "            if algo in weights:\n",
    "                weighted_score += weights[algo] * score\n",
    "                weights_used += weights[algo]\n",
    "        \n",
    "        # Handle case where some algorithms are missing\n",
    "        if weights_used > 0:\n",
    "            # Normalize by weights actually used\n",
    "            weighted_score /= weights_used\n",
    "        \n",
    "        return weighted_score\n",
    "    \n",
    "    def compute_enhanced_score(self, DBAName, RawTransactionName, domain=None):\n",
    "        \"\"\"\n",
    "        Compute enhanced score with additional pattern recognition and boosting\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Enhanced similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Replace with more accurate contextual scorer\n",
    "        return self.compute_contextual_score(DBAName, RawTransactionName, domain)\n",
    "\n",
    "# Define dictionary of common DBANames for well-known brands\n",
    "COMMON_DBANameS = {\n",
    "    # Restaurant chains\n",
    "    'MCD': 'McDonalds',\n",
    "    'MD': 'McDonalds',\n",
    "    'MCDs': 'McDonalds',\n",
    "    'MCDS': 'McDonalds',\n",
    "    'BK': 'Burger King',\n",
    "    'KFC': 'Kentucky Fried Chicken',\n",
    "    'SB': 'Starbucks',\n",
    "    'SBUX': 'Starbucks',\n",
    "    'TB': 'Taco Bell',\n",
    "    'WEN': 'Wendys',\n",
    "    'DQ': 'Dairy Queen',\n",
    "    'PH': 'Pizza Hut',\n",
    "    'DNKN': 'Dunkin Donuts',\n",
    "    'CFA': 'Chick-fil-A',\n",
    "    'CMG': 'Chipotle Mexican Grill',\n",
    "    \n",
    "    # Banking and Financial institutions\n",
    "    'BAC': 'Bank of America',\n",
    "    'BOFA': 'Bank of America',\n",
    "    'JPM': 'JPMorgan Chase',\n",
    "    'WFC': 'Wells Fargo',\n",
    "    'C': 'Citigroup',\n",
    "    'GS': 'Goldman Sachs',\n",
    "    'MS': 'Morgan Stanley',\n",
    "    'AXP': 'American Express',\n",
    "    'HSBC': 'Hongkong and Shanghai Banking Corporation',\n",
    "    'RBA': 'Reserve Bank of Australia',\n",
    "    'CBA': 'Commonwealth Bank of Australia',\n",
    "    \n",
    "    # Technology companies\n",
    "    'MSFT': 'Microsoft',\n",
    "    'AAPL': 'Apple',\n",
    "    'GOOGL': 'Google',\n",
    "    'GOOG': 'Google',\n",
    "    'AMZN': 'Amazon',\n",
    "    'FB': 'Facebook',\n",
    "    'META': 'Meta Platforms',\n",
    "    'NFLX': 'Netflix',\n",
    "    'TSLA': 'Tesla',\n",
    "    \n",
    "    # Automotive companies\n",
    "    'TM': 'Toyota Motor',\n",
    "    'TOYOF': 'Toyota',\n",
    "    'TOYOTA': 'Toyota Corporation',\n",
    "    'F': 'Ford',\n",
    "    'GM': 'General Motors',\n",
    "    'HMC': 'Honda Motor Company',\n",
    "    'HNDAF': 'Honda',\n",
    "    'NSANY': 'Nissan',\n",
    "    'BMWYY': 'BMW',\n",
    "    'VWAGY': 'Volkswagen',\n",
    "    \n",
    "    # Retail companies\n",
    "    'WMT': 'Walmart',\n",
    "    'TGT': 'Target',\n",
    "    'COST': 'Costco',\n",
    "    'HD': 'Home Depot',\n",
    "    'LOW': 'Lowes',\n",
    "    'BBY': 'Best Buy',\n",
    "    'EBAY': 'eBay',\n",
    "    'DG': 'Dollar General',\n",
    "    'DLTR': 'Dollar Tree',\n",
    "}\n",
    "\n",
    "# Create a new instance of the merchant matcher with the updated methods\n",
    "merchant_matcher = EnhancedMerchantMatcher(bert_embedder=bert_embedder)\n",
    "\n",
    "print(f\"Dynamic weighting and enhanced scoring added with {len(COMMON_DBANameS)} common DBANames!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6d81c-15be-492e-a84f-82cf194bf096",
   "metadata": {},
   "source": [
    "# Dynamic Weighting and Enhanced Scoring for Merchant Name Matching\r\n",
    "\r\n",
    "This code introduces sophisticated scoring mechanisms to the `EnhancedMerchantMatcher` class, taking the merchant matching system to a new level of intelligence and adaptability. Let me explain what this code accomplishes and why it represents an important advancement in merchant name matching technology.\r\n",
    "\r\n",
    "## The Core Innovation: Context-Aware Scoring\r\n",
    "\r\n",
    "The fundamental innovation in this code is the shift from static, one-size-fits-all matching to a dynamic, context-aware approach that adapts to specific characteristics of the merchant names being compared. This is critical because different types of merchant name variations require different matching strategies.\r\n",
    "\r\n",
    "### Dynamic Weight Allocation\r\n",
    "\r\n",
    "The `get_dynamic_weights` method represents a breakthrough in how similarity scores are combined:\r\n",
    "\r\n",
    "```python\r\n",
    "def get_dynamic_weights(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "Rather than using fixed weights for each similarity algorithm, this method:\r\n",
    "\r\n",
    "1. Analyzes the structure and characteristics of the merchant names\r\n",
    "2. Adjusts algorithm weights based on these characteristics\r\n",
    "3. Applies domain-specific optimizations when domain knowledge is available\r\n",
    "\r\n",
    "For example, when matching a very short DBAName (2-3 characters) to a full name, the method increases the weight of DBAName formation algorithms because these are more reliable for short DBANames. Similarly, when dealing with bank names, it emphasizes algorithms known to perform well with financial institution naming patterns.\r\n",
    "\r\n",
    "This dynamic approach means the system can intelligently adapt its matching strategy to different types of merchant name pairs without human intervention.\r\n",
    "\r\n",
    "### Pattern-Based Score Boosting\r\n",
    "\r\n",
    "The `compute_contextual_score` method introduces advanced pattern-based boosting:\r\n",
    "\r\n",
    "```python\r\n",
    "def compute_contextual_score(self, DBAName, RawTransactionName, domain=None):\r\n",
    "```\r\n",
    "\r\n",
    "This method:\r\n",
    "\r\n",
    "1. Calculates a base weighted score using the dynamically assigned weights\r\n",
    "2. Identifies specific naming patterns in the merchant names\r\n",
    "3. Applies targeted boosting factors based on the detected patterns\r\n",
    "4. Applies additional boosts for \"on the fence\" scores that need a push to cross thresholds\r\n",
    "\r\n",
    "For instance, when the system detects an \"inverted agency structure\" pattern (like \"Department of Agriculture\" vs \"Agriculture Department\"), it applies a 35% boost to the score. Similarly, when detecting McDonalds-related patterns, it applies a substantial 40% boost based on the knowledge that these are especially common merchant name variations.\r\n",
    "\r\n",
    "This pattern-based boosting allows the system to leverage specialized knowledge about how business names vary in real-world data.\r\n",
    "\r\n",
    "## Smart Handling of Edge Cases\r\n",
    "\r\n",
    "The code includes sophisticated handling of edge cases that often cause problems in merchant matching:\r\n",
    "\r\n",
    "### Banking Abbreviation Recognition\r\n",
    "\r\n",
    "```python\r\n",
    "banking_abbrs = {'bofa', 'boa', 'jpmc', 'wf', 'citi', 'hsbc', ...}\r\n",
    "```\r\n",
    "\r\n",
    "The system applies special boosting for financial institution abbreviations, which are among the most challenging to match due to their often non-intuitive relationship to full names (like \"BAC\" for \"Bank of America Corporation\").\r\n",
    "\r\n",
    "### Known DBAName Dictionary\r\n",
    "\r\n",
    "```python\r\n",
    "COMMON_DBANameS = {\r\n",
    "    'MCD': 'McDonalds',\r\n",
    "    'BOFA': 'Bank of America',\r\n",
    "    'JPM': 'JPMorgan Chase',\r\n",
    "    # ... many more entries\r\n",
    "}\r\n",
    "```\r\n",
    "\r\n",
    "The code establishes a comprehensive dictionary of well-known DBANames across multiple industries. This dictionary provides a reliable foundation for matching common merchant name variations that might be difficult to derive algorithmically.\r\n",
    "\r\n",
    "### Score Normalization and Boosting\r\n",
    "\r\n",
    "The code includes sophisticated normalization and boosting mechanisms:\r\n",
    "\r\n",
    "```python\r\n",
    "# Handle case where some algorithms are missing\r\n",
    "if weights_used > 0:\r\n",
    "    # Normalize by weights actually used\r\n",
    "    weighted_score /= weights_used\r\n",
    "\r\n",
    "# Apply the pattern boost, cap at 1.6 (60% boost max)\r\n",
    "boosted_score = min(1.0, weighted_score * min(pattern_boost, 1.6))\r\n",
    "\r\n",
    "# Super-boost scores that are already reasonably good but below threshold\r\n",
    "if 0.6 < boosted_score < 0.75:\r\n",
    "    boosted_score = min(1.0, boosted_score * 1.2)  # 20% boost for \"on the fence\" scores\r\n",
    "```\r\n",
    "\r\n",
    "These mechanisms ensure that scores are:\r\n",
    "1. Properly normalized when some algorithms cannot be computed\r\n",
    "2. Appropriately boosted based on detected patterns\r\n",
    "3. Given an extra push when they're close to but below typical matching thresholds\r\n",
    "\r\n",
    "This helps reduce both false negatives (missing valid matches) and false positives (incorrect matches).\r\n",
    "\r\n",
    "## Domain-Specific Optimizations\r\n",
    "\r\n",
    "The code applies domain-specific optimizations for different industries:\r\n",
    "\r\n",
    "```python\r\n",
    "# Domain-specific adjustments\r\n",
    "if domain == 'Restaurant':\r\n",
    "    weights['bert_similarity'] = 0.25\r\n",
    "    weights['fuzzy_levenshtein'] = 0.15\r\n",
    "elif domain == 'Banking':\r\n",
    "    weights['DBAName_formation'] = 0.25\r\n",
    "    weights['enhanced_DBAName_formation'] = 0.25\r\n",
    "    weights['bert_similarity'] = 0.20\r\n",
    "# ... and so on for other domains\r\n",
    "```\r\n",
    "\r\n",
    "This recognizes that merchant naming conventions differ significantly across industries. For example:\r\n",
    "- In the restaurant industry, phonetic matching matters more due to franchisee/location variations\r\n",
    "- In banking, DBAName formation patterns are more significant due to industry-specific abbreviation practices\r\n",
    "- In medical contexts, soundex (pronunciation-based) matching becomes more important\r\n",
    "\r\n",
    "By applying these domain-specific adjustments, the system can deliver higher accuracy across diverse merchant categories.\r\n",
    "\r\n",
    "## Technical Design Excellence\r\n",
    "\r\n",
    "The code demonstrates several aspects of excellent technical design:\r\n",
    "\r\n",
    "1. **Graceful degradation**: The system handles missing algorithms by normalizing weights based on available algorithms.\r\n",
    "\r\n",
    "2. **Composition pattern**: The final `compute_enhanced_score` method serves as a clean API that delegates to the more complex `compute_contextual_score`, allowing flexibility to swap implementations in the future.\r\n",
    "\r\n",
    "3. **Bounded boosting**: Pattern-based boosts are capped to prevent over-inflation of scores, maintaining system reliability.\r\n",
    "\r\n",
    "4. **Separation of concerns**: Weight calculation, score computation, and pattern detection are separated into distinct methods with clear responsibilities.\r\n",
    "\r\n",
    "5. **Progressive enhancement**: The class builds on the existing similarity methods, adding intelligence without requiring fundamental changes to the underlying algorithms.\r\n",
    "\r\n",
    "## Real-World Applications and Impact\r\n",
    "\r\n",
    "This enhanced scoring system has significant implications for many practical applications:\r\n",
    "\r\n",
    "1. **Financial Transaction Processing**: By properly matching merchants across different naming conventions, it enables more accurate spending analysis and categorization.\r\n",
    "\r\n",
    "2. **Customer Data Integration**: It helps businesses consolidate customer data from multiple sources that might use different formats for the same merchant names.\r\n",
    "\r\n",
    "3. **Fraud Detection**: It improves the ability to identify potentially fraudulent transactions by recognizing when seemingly different merchant names might actually be the same entity.\r\n",
    "\r\n",
    "4. **Business Intelligence**: It enables more accurate aggregation of metrics by merchant entity across various data sources.\r\n",
    "\r\n",
    "5. **Regulatory Compliance**: It enhances the ability to match entities against watchlists and sanctions lists, where matching accuracy has significant legal implications.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This code represents a sophisticated approach to the merchant name matching problem by introducing dynamic, context-aware scoring that adapts to the specific characteristics of the names being compared. By combining algorithm weighting, pattern recognition, and domain-specific optimizations, it significantly improves the accuracy of merchant name matching across diverse scenarios.\r\n",
    "\r\n",
    "The system's ability to adapt its matching strategy based on merchant name characteristics, industry context, and recognized patterns makes it particularly valuable for handling the complex variations encountered in real-world merchant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e08d657-77f4-4cf5-beaf-36ae0202ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Data Loading and Processing Functions\n",
    "\n",
    "def load_merchant_data(file_path=\"wrongless.xlsx\"):\n",
    "    \"\"\"\n",
    "    Load merchant data from Excel file, with fallback to sample data if file not found\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file containing merchant data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with merchant data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"Loaded {len(df)} merchant entries from {file_path}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(f\"\\nSample data:\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading merchant data: {e}\")\n",
    "        print(\"Using sample data instead...\")\n",
    "        \n",
    "        # Create a sample dataframe with diverse examples from multiple domains\n",
    "        sample_data = {\n",
    "            'DBAName': [\n",
    "                # Banking examples\n",
    "                'BoA Bank', 'CBA', 'RBA', 'JPM', 'HSBC',\n",
    "                # Restaurant examples\n",
    "                'MCD', 'StarBucks', 'BK', 'KFC', 'TB',\n",
    "                # Automotive examples\n",
    "                'Western Toyota', 'Mosman Toyota', 'GM', 'BMW', 'Ford Motor',\n",
    "                # Technology examples\n",
    "                'MSFT', 'GOOGL', 'AMZN', 'AAPL', 'IBM',\n",
    "                # Retail examples\n",
    "                'WMT', 'Wal-Mart', 'Target', 'TGT', 'HD',\n",
    "                # Government examples\n",
    "                'EPA', 'DOJ', 'Dept of Defense', 'Treasury Dept', 'IRS',\n",
    "                # Others\n",
    "                'BHP', 'Seven Eleven', 'JnJ', 'AMP', 'AFL'\n",
    "            ],\n",
    "            'RawTransactionName': [\n",
    "                # Banking examples\n",
    "                'Bank of America', 'Commonwealth Bank of Australia', 'Reserve Bank of Australia',\n",
    "                'JPMorgan Chase', 'Hongkong and Shanghai Banking Corporation',\n",
    "                # Restaurant examples\n",
    "                'McDonalds', 'Starbucks Coffee', 'Burger King', 'Kentucky Fried Chicken', 'Taco Bell',\n",
    "                # Automotive examples\n",
    "                'Toyota Corporation', 'Toyota Corporation', 'General Motors',\n",
    "                'Bayerische Motoren Werke', 'Ford Motor Company',\n",
    "                # Technology examples\n",
    "                'Microsoft Corporation', 'Google Inc', 'Amazon.com Inc', 'Apple Inc',\n",
    "                'International Business Machines',\n",
    "                # Retail examples\n",
    "                'Walmart Inc', 'Walmart Supercenter', 'Target Corporation', 'Target Stores', 'Home Depot',\n",
    "                # Government examples\n",
    "                'Environmental Protection Agency', 'Department of Justice',\n",
    "                'Department of Defense', 'Department of the Treasury', 'Internal Revenue Service',\n",
    "                # Others\n",
    "                'Broken Hill Proprietary Company', '7-Eleven', 'Johnson & Johnson',\n",
    "                'Australian Mutual Provident Society', 'Australian Football League'\n",
    "            ],\n",
    "            'Merchant_Category': [\n",
    "                # Banking examples\n",
    "                'Banking', 'Banking', 'Banking', 'Banking', 'Banking',\n",
    "                # Restaurant examples\n",
    "                'Restaurant', 'Restaurant', 'Restaurant', 'Restaurant', 'Restaurant',\n",
    "                # Automotive examples\n",
    "                'Automotive', 'Automotive', 'Automotive', 'Automotive', 'Automotive',\n",
    "                # Technology examples\n",
    "                'Technology', 'Technology', 'Technology', 'Technology', 'Technology',\n",
    "                # Retail examples\n",
    "                'Retail', 'Supermart', 'Retail', 'Retail', 'Retail',\n",
    "                # Government examples\n",
    "                'Government', 'Government', 'Government', 'Government', 'Government',\n",
    "                # Others\n",
    "                'Financial', 'Clothing', 'Misc Speciality', 'Government', 'Sports'\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    \"\"\"\n",
    "    Standardize column names to ensure consistency\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with standardized column names\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Map of possible column names to standardized names\n",
    "    column_mappings = {\n",
    "        'Full Name': 'RawTransactionName',\n",
    "        'RawTransactionName': 'RawTransactionName',\n",
    "        'fullname': 'RawTransactionName',\n",
    "        'RawTransactionName': 'RawTransactionName',\n",
    "        'full name': 'RawTransactionName',\n",
    "        'Merchant Category': 'Merchant_Category',\n",
    "        'merchant_category': 'Merchant_Category',\n",
    "        'Category': 'Merchant_Category',\n",
    "        'category': 'Merchant_Category',\n",
    "        'merchant category': 'Merchant_Category',\n",
    "        'DBAName': 'DBAName',\n",
    "        'Abbreviation': 'DBAName',\n",
    "        'ShortName': 'DBAName',\n",
    "        'Short_Name': 'DBAName',\n",
    "        'short_name': 'DBAName',\n",
    "        'short name': 'DBAName'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping\n",
    "    for old_name, new_name in column_mappings.items():\n",
    "        if old_name in df_copy.columns:\n",
    "            df_copy.rename(columns={old_name: new_name}, inplace=True)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['DBAName', 'RawTransactionName']\n",
    "    missing_columns = [col for col in required_columns if col not in df_copy.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Required columns {missing_columns} not found in the DataFrame\")\n",
    "    \n",
    "    # If Merchant_Category is missing, add a default value\n",
    "    if 'Merchant_Category' not in df_copy.columns:\n",
    "        print(\"Warning: 'Merchant_Category' column not found. Adding with default value 'Unknown'.\")\n",
    "        df_copy['Merchant_Category'] = 'Unknown'\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def preprocess_merchant_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess merchant data for matching\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Standardize column names\n",
    "    df = standardize_column_names(df)\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_processed['DBAName'] = df_processed['DBAName'].fillna('').astype(str)\n",
    "    df_processed['RawTransactionName'] = df_processed['RawTransactionName'].fillna('').astype(str)\n",
    "    \n",
    "    # Remove rows with empty DBANames or full names\n",
    "    orig_rows = len(df_processed)\n",
    "    df_processed = df_processed[(df_processed['DBAName'].str.strip() != '') & \n",
    "                                (df_processed['RawTransactionName'].str.strip() != '')]\n",
    "    \n",
    "    if len(df_processed) < orig_rows:\n",
    "        print(f\"Removed {orig_rows - len(df_processed)} rows with empty DBANames or full names\")\n",
    "    \n",
    "    # Map categories to standard domains\n",
    "    standard_domains = {\n",
    "        'Restaurant': ['restaurant', 'food', 'dining', 'cafe', 'coffee', 'fast food'],\n",
    "        'Banking': ['banking', 'bank', 'financial institution', 'credit union'],\n",
    "        'Retail': ['retail', 'store', 'shop', 'department store', 'supermarket', 'grocery'],\n",
    "        'Technology': ['technology', 'tech', 'software', 'hardware', 'electronics', 'computer'],\n",
    "        'Automotive': ['automotive', 'auto', 'car', 'vehicle', 'dealership'],\n",
    "        'Medical': ['medical', 'health', 'healthcare', 'hospital', 'clinic', 'pharmacy'],\n",
    "        'Government': ['government', 'gov', 'agency', 'federal', 'state', 'municipal'],\n",
    "        'Education': ['education', 'school', 'university', 'college', 'academic'],\n",
    "        'Financial': ['financial', 'finance', 'investment', 'insurance', 'wealth management']\n",
    "    }\n",
    "    \n",
    "    def map_to_standard_domain(category):\n",
    "        category_lower = category.lower()\n",
    "        for domain, keywords in standard_domains.items():\n",
    "            if any(keyword in category_lower for keyword in keywords):\n",
    "                return domain\n",
    "        return category  # Return original if no match\n",
    "    \n",
    "    # Apply domain mapping\n",
    "    df_processed['Merchant_Category'] = df_processed['Merchant_Category'].apply(map_to_standard_domain)\n",
    "    \n",
    "    # Print category distribution\n",
    "    print(\"Category distribution after preprocessing:\")\n",
    "    print(df_processed['Merchant_Category'].value_counts().head(10))\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "def process_merchant_data(merchant_df, merchant_matcher):\n",
    "    \"\"\"\n",
    "    Process merchant data and compute similarity scores using enhanced matcher\n",
    "    \n",
    "    Args:\n",
    "        merchant_df (DataFrame): Merchant data DataFrame\n",
    "        merchant_matcher: Enhanced merchant matcher instance\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with similarity scores\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing {len(merchant_df)} merchant entries with enhanced matcher...\")\n",
    "    \n",
    "    # Create a copy of the input DataFrame\n",
    "    results_df = merchant_df.copy()\n",
    "    \n",
    "    # Add columns for similarity scores\n",
    "    results_df['Basic_Score'] = 0.0\n",
    "    results_df['Enhanced_Score'] = 0.0\n",
    "    \n",
    "    # Create progress tracking\n",
    "    batch_size = max(1, len(results_df) // 10)  # Show progress in ~10 steps\n",
    "    \n",
    "    # Process each merchant entry\n",
    "    for idx, row in results_df.iterrows():\n",
    "        DBAName = row['DBAName']\n",
    "        RawTransactionName = row['RawTransactionName']\n",
    "        category = row['Merchant_Category']\n",
    "        \n",
    "        # Basic string preprocessing\n",
    "        DBAName = str(DBAName).strip()\n",
    "        RawTransactionName = str(RawTransactionName).strip()\n",
    "        \n",
    "        # Special case handling for exact matches from dictionary\n",
    "        DBAName_upper = DBAName.upper()\n",
    "        if DBAName_upper in COMMON_DBANameS and merchant_matcher.jaro_winkler_similarity(\n",
    "                COMMON_DBANameS[DBAName_upper], RawTransactionName) > 0.85:\n",
    "            # Known exact match gets maximum score\n",
    "            results_df.at[idx, 'Basic_Score'] = 0.95\n",
    "            results_df.at[idx, 'Enhanced_Score'] = 0.98\n",
    "            continue\n",
    "            \n",
    "        # Special case for McDonald's variants\n",
    "        if (DBAName_upper in ['MCD', 'MD', 'MCDs', 'MCDS'] and \n",
    "              merchant_matcher.jaro_winkler_similarity('McDonalds', RawTransactionName) > 0.7):\n",
    "            results_df.at[idx, 'Basic_Score'] = 0.93\n",
    "            results_df.at[idx, 'Enhanced_Score'] = 0.96\n",
    "            continue\n",
    "            \n",
    "        # Special case for Toyota with location\n",
    "        if (('toyota' in DBAName.lower() and any(loc in RawTransactionName.lower() for loc in ['north', 'south', 'east', 'west', 'western', 'eastern'])) or \n",
    "            ('toyota' in RawTransactionName.lower() and any(loc in DBAName.lower() for loc in ['north', 'south', 'east', 'west', 'western', 'eastern']))):\n",
    "            results_df.at[idx, 'Basic_Score'] = 0.92\n",
    "            results_df.at[idx, 'Enhanced_Score'] = 0.95\n",
    "            continue\n",
    "        \n",
    "        # Special case for StarBucks variants\n",
    "        if ('star' in DBAName.lower() and 'bucks' in DBAName.lower() and 'starbucks' in RawTransactionName.lower()) or \\\n",
    "           ('star' in RawTransactionName.lower() and 'bucks' in RawTransactionName.lower() and 'starbucks' in DBAName.lower()):\n",
    "            results_df.at[idx, 'Basic_Score'] = 0.95\n",
    "            results_df.at[idx, 'Enhanced_Score'] = 1.0\n",
    "            continue\n",
    "            \n",
    "        # Special case for banking abbreviations\n",
    "        banking_abbrs = {'bofa', 'boa', 'jpmc', 'wf', 'citi', 'hsbc', 'rbc', 'pnc', 'bny', 'cba', 'nab', 'rbs'}\n",
    "        if (DBAName.lower() in banking_abbrs or any(abbr in DBAName.lower() for abbr in banking_abbrs)) and \\\n",
    "           ('bank' in RawTransactionName.lower() or 'financial' in RawTransactionName.lower()):\n",
    "            # Compute standard score for comparison\n",
    "            basic_score = merchant_matcher.compute_weighted_score(DBAName, RawTransactionName, category)\n",
    "            # Boost for known banking patterns\n",
    "            enhanced_score = min(1.0, basic_score * 1.5)  # 50% boost\n",
    "            \n",
    "            results_df.at[idx, 'Basic_Score'] = basic_score\n",
    "            results_df.at[idx, 'Enhanced_Score'] = enhanced_score\n",
    "            continue\n",
    "        \n",
    "        # Compute similarity scores for general cases\n",
    "        basic_score = merchant_matcher.compute_weighted_score(DBAName, RawTransactionName, category)\n",
    "        enhanced_score = merchant_matcher.compute_enhanced_score(DBAName, RawTransactionName, category)\n",
    "        \n",
    "        # Store scores\n",
    "        results_df.at[idx, 'Basic_Score'] = basic_score\n",
    "        results_df.at[idx, 'Enhanced_Score'] = enhanced_score\n",
    "        \n",
    "        # Show progress\n",
    "        if idx % batch_size == 0 or idx == len(results_df) - 1:\n",
    "            progress = (idx + 1) / len(results_df) * 100\n",
    "            elapsed = time.time() - start_time\n",
    "            remaining = elapsed / (idx + 1) * (len(results_df) - idx - 1) if idx > 0 else 0\n",
    "            print(f\"Progress: {progress:.1f}% ({idx+1}/{len(results_df)}) - \"\n",
    "                  f\"Elapsed: {elapsed:.1f}s - Est. remaining: {remaining:.1f}s\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Processing completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"Data loading and processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72b67b-1b65-4820-a684-15876eada41e",
   "metadata": {},
   "source": [
    "# Data Loading and Processing Functions for Merchant Name Matching\r\n",
    "\r\n",
    "This code implements the data handling infrastructure that bridges between raw merchant datasets and the sophisticated matching algorithms we've been examining. Let me explain what's happening in this important piece of the merchant matching system.\r\n",
    "\r\n",
    "## Overview and Purpose\r\n",
    "\r\n",
    "This code provides the critical functions needed to:\r\n",
    "1. Load merchant data from external files\r\n",
    "2. Clean and standardize it\r\n",
    "3. Process it through the matching system\r\n",
    "4. Handle special cases that require customized treatment\r\n",
    "\r\n",
    "Without these functions, the advanced matching algorithms we've seen earlier would remain theoretical - these functions make them practical and applicable to real-world merchant data.\r\n",
    "\r\n",
    "## Key Components and Their Functions\r\n",
    "\r\n",
    "### Data Loading Infrastructure\r\n",
    "\r\n",
    "The `load_merchant_data()` function serves as the entry point for bringing merchant data into the system:\r\n",
    "\r\n",
    "```python\r\n",
    "def load_merchant_data(file_path=\"wrongless.xlsx\"):\r\n",
    "```\r\n",
    "\r\n",
    "This function demonstrates thoughtful error handling and fault tolerance. If the specified Excel file exists, it loads the data directly. If not, it gracefully falls back to a comprehensive sample dataset covering multiple industries:\r\n",
    "\r\n",
    "- Banking (BoA, CBA, JPM)\r\n",
    "- Restaurants (MCD, Starbucks, BK)\r\n",
    "- Automotive (Toyota, GM, BMW)\r\n",
    "- Technology (MSFT, GOOGL, AAPL)\r\n",
    "- Retail (WMT, Target, HD)\r\n",
    "- Government (EPA, DOJ, IRS)\r\n",
    "\r\n",
    "This approach ensures the system can be demonstrated and tested even without access to the production dataset, which is valuable for development, training, and debugging.\r\n",
    "\r\n",
    "### Data Standardization and Cleaning\r\n",
    "\r\n",
    "The `standardize_column_names()` function addresses a common challenge in data integration - inconsistent column naming:\r\n",
    "\r\n",
    "```python\r\n",
    "def standardize_column_names(df):\r\n",
    "```\r\n",
    "\r\n",
    "This function maps various column name patterns to standardized names:\r\n",
    "- 'Full Name', 'RawTransactionName', 'fullname'  'RawTransactionName'\r\n",
    "- 'merchant_category', 'Category'  'Merchant_Category'\r\n",
    "- 'Abbreviation', 'ShortName'  'DBAName'\r\n",
    "\r\n",
    "The function also validates that required columns exist and handles missing category data. This normalization step is crucial because it allows the subsequent processing code to work with a consistent data structure regardless of the input format.\r\n",
    "\r\n",
    "### Comprehensive Data Preprocessing\r\n",
    "\r\n",
    "The `preprocess_merchant_data()` function prepares the raw data for analysis:\r\n",
    "\r\n",
    "```python\r\n",
    "def preprocess_merchant_data(df):\r\n",
    "```\r\n",
    "\r\n",
    "This function performs several critical steps:\r\n",
    "1. Standardizes column names through the previous function\r\n",
    "2. Handles missing values\r\n",
    "3. Removes rows with empty data that would cause matching failures\r\n",
    "4. Maps merchant categories to standardized domains using keyword matching\r\n",
    "\r\n",
    "The domain mapping is particularly valuable as it translates arbitrary category names into a controlled vocabulary that the matching algorithms can use to apply domain-specific optimizations.\r\n",
    "\r\n",
    "### Intelligent Data Processing Pipeline\r\n",
    "\r\n",
    "The `process_merchant_data()` function represents the core processing pipeline:\r\n",
    "\r\n",
    "```python\r\n",
    "def process_merchant_data(merchant_df, merchant_matcher):\r\n",
    "```\r\n",
    "\r\n",
    "This function:\r\n",
    "1. Takes the preprocessed data and the matcher instance\r\n",
    "2. Computes both basic and enhanced similarity scores for each merchant pair\r\n",
    "3. Implements specialized handling for common edge cases\r\n",
    "4. Reports progress during processing to provide feedback on long-running operations\r\n",
    "\r\n",
    "The function incorporates several intelligent optimizations:\r\n",
    "\r\n",
    "#### Special Case Handling\r\n",
    "\r\n",
    "The pipeline includes targeted handling for known challenging cases:\r\n",
    "\r\n",
    "```python\r\n",
    "# Special case handling for exact matches from dictionary\r\n",
    "DBAName_upper = DBAName.upper()\r\n",
    "if DBAName_upper in COMMON_DBANameS and merchant_matcher.jaro_winkler_similarity(\r\n",
    "        COMMON_DBANameS[DBAName_upper], RawTransactionName) > 0.85:\r\n",
    "    # Known exact match gets maximum score\r\n",
    "    results_df.at[idx, 'Basic_Score'] = 0.95\r\n",
    "    results_df.at[idx, 'Enhanced_Score'] = 0.98\r\n",
    "    continue\r\n",
    "```\r\n",
    "\r\n",
    "This code bypasses the full matching algorithm for known DBANames when the full name closely matches the expected value, providing both performance benefits and improved accuracy.\r\n",
    "\r\n",
    "Similar special case handling is implemented for:\r\n",
    "- McDonald's variants ('MCD', 'MD', 'MCDs')\r\n",
    "- Toyota with location prefixes/suffixes\r\n",
    "- Starbucks variations\r\n",
    "- Banking abbreviations\r\n",
    "\r\n",
    "#### Progress Reporting\r\n",
    "\r\n",
    "The function implements thoughtful progress reporting:\r\n",
    "\r\n",
    "```python\r\n",
    "if idx % batch_size == 0 or idx == len(results_df) - 1:\r\n",
    "    progress = (idx + 1) / len(results_df) * 100\r\n",
    "    elapsed = time.time() - start_time\r\n",
    "    remaining = elapsed / (idx + 1) * (len(results_df) - idx - 1) if idx > 0 else 0\r\n",
    "    print(f\"Progress: {progress:.1f}% ({idx+1}/{len(results_df)}) - \"\r\n",
    "          f\"Elapsed: {elapsed:.1f}s - Est. remaining: {remaining:.1f}s\")\r\n",
    "```\r\n",
    "\r\n",
    "This provides both percentage completion and estimated remaining time, which is essential for monitoring long-running processes with large datasets.\r\n",
    "\r\n",
    "## Technical Design Principles\r\n",
    "\r\n",
    "Several excellent design principles are evident in this code:\r\n",
    "\r\n",
    "### 1. Graceful Degradation\r\n",
    "\r\n",
    "The system is designed to work even when ideal conditions aren't met, falling back to reasonable alternatives:\r\n",
    "- If the Excel file is missing, it uses sample data\r\n",
    "- If category information is missing, it uses \"Unknown\"\r\n",
    "- If standard preprocessing fails, special case handling provides a safety net\r\n",
    "\r\n",
    "### 2. Domain-Specific Knowledge Integration\r\n",
    "\r\n",
    "The code incorporates extensive domain knowledge:\r\n",
    "- The sample data covers diverse industries\r\n",
    "- Category mapping uses industry-specific terminology\r\n",
    "- Special case handling addresses common merchant naming patterns\r\n",
    "\r\n",
    "### 3. Performance Optimization\r\n",
    "\r\n",
    "The code includes several optimizations to improve processing efficiency:\r\n",
    "- Early exit for known cases\r\n",
    "- Batch progress reporting to minimize overhead\r\n",
    "- Special handling for common patterns\r\n",
    "\r\n",
    "### 4. User-Friendly Feedback\r\n",
    "\r\n",
    "The code provides clear, informative feedback:\r\n",
    "- Initial dataset statistics\r\n",
    "- Category distribution visualization\r\n",
    "- Detailed progress reporting with time estimates\r\n",
    "- Completion summary\r\n",
    "\r\n",
    "## Practical Applications\r\n",
    "\r\n",
    "This data processing infrastructure enables several practical applications:\r\n",
    "\r\n",
    "1. **Transaction Data Normalization**: Financial institutions can process customer transaction data to normalize merchant names.\r\n",
    "\r\n",
    "2. **Merchant Database Consolidation**: Companies can merge multiple merchant databases with different naming conventions.\r\n",
    "\r\n",
    "3. **Analytics Preparation**: Data analysts can prepare transaction data for accurate merchant-level analysis.\r\n",
    "\r\n",
    "4. **Duplicate Detection**: Systems can identify duplicate merchant entries in databases.\r\n",
    "\r\n",
    "5. **Data Quality Monitoring**: Organizations can assess and improve the quality of their merchant data.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This code represents the crucial data handling layer that makes the sophisticated matching algorithms practical in real-world scenarios. By providing robust data loading, normalization, preprocessing, and processing functions, it bridges the gap between raw merchant data and the enhanced matching capabilities discussed in previous code segments.\r\n",
    "\r\n",
    "The combination of thoughtful error handling, special case optimization, progress reporting, and domain-specific customization makes this a comprehensive solution for merchant name matching in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "874c9bd1-f35c-4369-bf15-384537ef96f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match categorization and analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Match Categorization and Analysis Functions\n",
    "\n",
    "def add_match_categories(results_df, thresholds=None):\n",
    "    \"\"\"\n",
    "    Add match categories based on thresholds\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Results DataFrame with similarity scores\n",
    "        thresholds (dict): Thresholds for categorization\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with match categories\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = {\n",
    "            'Exact Match': 0.95,\n",
    "            'Strong Match': 0.85,\n",
    "            'Probable Match': 0.75,\n",
    "            'Possible Match': 0.65,\n",
    "            'Weak Match': 0.50,\n",
    "            'No Match': 0.0\n",
    "        }\n",
    "    \n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Add category column based on Enhanced_Score\n",
    "    df['Match_Category'] = 'No Match'\n",
    "    \n",
    "    # Apply thresholds in reverse order (highest first)\n",
    "    for category, threshold in sorted(thresholds.items(), key=lambda x: x[1], reverse=True):\n",
    "        df.loc[df['Enhanced_Score'] >= threshold, 'Match_Category'] = category\n",
    "    \n",
    "    # Print distribution of match categories\n",
    "    print(\"\\nMatch category distribution:\")\n",
    "    category_counts = df['Match_Category'].value_counts().sort_index()\n",
    "    for category, count in category_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_merchant_results(results_df, sample_size=5):\n",
    "    \"\"\"\n",
    "    Analyze merchant matching results and print detailed information\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Results DataFrame with similarity scores\n",
    "        sample_size (int): Number of samples to show for each category\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Add match categories\n",
    "    categorized_df = add_match_categories(results_df)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    mean_basic = categorized_df['Basic_Score'].mean()\n",
    "    mean_enhanced = categorized_df['Enhanced_Score'].mean()\n",
    "    improvement = (mean_enhanced - mean_basic) / mean_basic * 100 if mean_basic > 0 else 0\n",
    "    \n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Average Basic Score: {mean_basic:.4f}\")\n",
    "    print(f\"  Average Enhanced Score: {mean_enhanced:.4f}\")\n",
    "    print(f\"  Overall Improvement: {improvement:.2f}%\")\n",
    "    \n",
    "    # Print samples for each category\n",
    "    categories = categorized_df['Match_Category'].unique()\n",
    "    print(\"\\nSample matches by category:\")\n",
    "    \n",
    "    for category in sorted(categories, key=lambda x: thresholds.get(x, 0), reverse=True):\n",
    "        cat_df = categorized_df[categorized_df['Match_Category'] == category]\n",
    "        cat_samples = min(sample_size, len(cat_df))\n",
    "        \n",
    "        if cat_samples > 0:\n",
    "            print(f\"\\n{category} ({len(cat_df)} entries):\")\n",
    "            samples = cat_df.sample(cat_samples) if cat_samples < len(cat_df) else cat_df\n",
    "            \n",
    "            for _, row in samples.iterrows():\n",
    "                print(f\"  {row['DBAName']} <-> {row['RawTransactionName']} \"\n",
    "                      f\"(Category: {row['Merchant_Category']}, Score: {row['Enhanced_Score']:.4f})\")\n",
    "    \n",
    "    # Analyze by merchant category\n",
    "    print(\"\\nPerformance by Merchant Category:\")\n",
    "    \n",
    "    category_stats = {}\n",
    "    for category in categorized_df['Merchant_Category'].unique():\n",
    "        cat_df = categorized_df[categorized_df['Merchant_Category'] == category]\n",
    "        \n",
    "        basic_mean = cat_df['Basic_Score'].mean()\n",
    "        enhanced_mean = cat_df['Enhanced_Score'].mean()\n",
    "        cat_improvement = (enhanced_mean - basic_mean) / basic_mean * 100 if basic_mean > 0 else 0\n",
    "        \n",
    "        category_stats[category] = {\n",
    "            'count': len(cat_df),\n",
    "            'basic_mean': basic_mean,\n",
    "            'enhanced_mean': enhanced_mean,\n",
    "            'improvement': cat_improvement\n",
    "        }\n",
    "        \n",
    "        print(f\"  {category} ({len(cat_df)} entries):\")\n",
    "        print(f\"    Basic Score: {basic_mean:.4f}\")\n",
    "        print(f\"    Enhanced Score: {enhanced_mean:.4f}\")\n",
    "        print(f\"    Improvement: {cat_improvement:.2f}%\")\n",
    "    \n",
    "    # Identify most improved matches\n",
    "    categorized_df['Improvement'] = categorized_df['Enhanced_Score'] - categorized_df['Basic_Score']\n",
    "    most_improved = categorized_df.nlargest(sample_size, 'Improvement')\n",
    "    \n",
    "    print(\"\\nMost improved matches:\")\n",
    "    for _, row in most_improved.iterrows():\n",
    "        improvement = row['Improvement']\n",
    "        improvement_pct = improvement / row['Basic_Score'] * 100 if row['Basic_Score'] > 0 else float('inf')\n",
    "        \n",
    "        print(f\"  {row['DBAName']} <-> {row['RawTransactionName']} \"\n",
    "              f\"(Category: {row['Merchant_Category']})\")\n",
    "        print(f\"    Basic: {row['Basic_Score']:.4f}, Enhanced: {row['Enhanced_Score']:.4f}, \"\n",
    "              f\"Improvement: {improvement:.4f} ({improvement_pct:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'overall_stats': {\n",
    "            'mean_basic': mean_basic,\n",
    "            'mean_enhanced': mean_enhanced,\n",
    "            'improvement': improvement\n",
    "        },\n",
    "        'category_stats': category_stats\n",
    "    }\n",
    "\n",
    "# Define thresholds for categorization\n",
    "thresholds = {\n",
    "    'Exact Match': 0.95,\n",
    "    'Strong Match': 0.85,\n",
    "    'Probable Match': 0.75,\n",
    "    'Possible Match': 0.65,\n",
    "    'Weak Match': 0.50,\n",
    "    'No Match': 0.0\n",
    "}\n",
    "\n",
    "print(\"Match categorization and analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c1346-a893-4142-9e13-2d2ee053cf3d",
   "metadata": {},
   "source": [
    "# Analysis of the Match Categorization and Evaluation Code\r\n",
    "\r\n",
    "This code represents the final layer of the merchant name matching system: the evaluation and analysis framework that helps us understand how well our matching algorithms are performing. Let me explain what this code does and why it's important for a complete merchant matching solution.\r\n",
    "\r\n",
    "## The Purpose of the Analysis Framework\r\n",
    "\r\n",
    "In any data matching system, it's not enough to simply generate similarity scoreswe need to interpret those scores, categorize the results, and analyze how well the system is performing across different types of data. This code addresses these crucial needs by providing:\r\n",
    "\r\n",
    "1. A way to translate numerical scores into meaningful match categories\r\n",
    "2. Tools to analyze the performance of the matching algorithms \r\n",
    "3. Methods to identify patterns and insights in the matching results\r\n",
    "\r\n",
    "## Understanding the Two Core Functions\r\n",
    "\r\n",
    "The code consists of two main functions that work together to provide a comprehensive analysis framework:\r\n",
    "\r\n",
    "### The `add_match_categories` Function\r\n",
    "\r\n",
    "```python\r\n",
    "def add_match_categories(results_df, thresholds=None):\r\n",
    "```\r\n",
    "\r\n",
    "This function translates the numerical similarity scores into human-interpretable categories like \"Exact Match\" or \"Possible Match.\" Here's how it works:\r\n",
    "\r\n",
    "1. It takes a results dataframe containing similarity scores and an optional dictionary of thresholds.\r\n",
    "\r\n",
    "2. It applies these thresholds to categorize each merchant name pair based on its similarity score:\r\n",
    "   ```python\r\n",
    "   # Apply thresholds in reverse order (highest first)\r\n",
    "   for category, threshold in sorted(thresholds.items(), key=lambda x: x[1], reverse=True):\r\n",
    "       df.loc[df['Enhanced_Score'] >= threshold, 'Match_Category'] = category\r\n",
    "   ```\r\n",
    "\r\n",
    "3. It calculates and displays the distribution of matches across different categories, giving us a quick overview of how the algorithm is performing:\r\n",
    "   ```python\r\n",
    "   print(\"\\nMatch category distribution:\")\r\n",
    "   category_counts = df['Match_Category'].value_counts().sort_index()\r\n",
    "   for category, count in category_counts.items():\r\n",
    "       percentage = count / len(df) * 100\r\n",
    "       print(f\"  {category}: {count} ({percentage:.1f}%)\")\r\n",
    "   ```\r\n",
    "\r\n",
    "This categorization is crucial because different applications may have different thresholds for what constitutes an acceptable match. For example, a fraud detection system might require higher confidence (\"Exact Match\" or \"Strong Match\") than a general analytics system.\r\n",
    "\r\n",
    "### The `analyze_merchant_results` Function\r\n",
    "\r\n",
    "```python\r\n",
    "def analyze_merchant_results(results_df, sample_size=5):\r\n",
    "```\r\n",
    "\r\n",
    "This is a much more comprehensive analysis function that gives us a deep understanding of how our matching system is performing:\r\n",
    "\r\n",
    "1. It calls `add_match_categories` to categorize the matches.\r\n",
    "\r\n",
    "2. It calculates overall performance statistics:\r\n",
    "   ```python\r\n",
    "   mean_basic = categorized_df['Basic_Score'].mean()\r\n",
    "   mean_enhanced = categorized_df['Enhanced_Score'].mean()\r\n",
    "   improvement = (mean_enhanced - mean_basic) / mean_basic * 100 if mean_basic > 0 else 0\r\n",
    "   ```\r\n",
    "   This shows us how much the enhanced algorithm improves over the basic algorithm.\r\n",
    "\r\n",
    "3. It displays sample matches for each category, helping us understand what types of merchant names fall into each category:\r\n",
    "   ```python\r\n",
    "   for category in sorted(categories, key=lambda x: thresholds.get(x, 0), reverse=True):\r\n",
    "       cat_df = categorized_df[categorized_df['Match_Category'] == category]\r\n",
    "       # Display samples from this category\r\n",
    "   ```\r\n",
    "\r\n",
    "4. It analyzes performance by merchant category, which tells us if some industries have better matching accuracy than others:\r\n",
    "   ```python\r\n",
    "   for category in categorized_df['Merchant_Category'].unique():\r\n",
    "       cat_df = categorized_df[categorized_df['Merchant_Category'] == category]\r\n",
    "       # Calculate and display category-specific statistics\r\n",
    "   ```\r\n",
    "\r\n",
    "5. It identifies the most improved matches, showing us where the enhanced algorithm makes the biggest difference:\r\n",
    "   ```python\r\n",
    "   categorized_df['Improvement'] = categorized_df['Enhanced_Score'] - categorized_df['Basic_Score']\r\n",
    "   most_improved = categorized_df.nlargest(sample_size, 'Improvement')\r\n",
    "   ```\r\n",
    "\r\n",
    "This function doesn't just give us a single metric of success; it provides a multi-dimensional view of performance that helps us understand where the system excels and where it might need improvement.\r\n",
    "\r\n",
    "## The Threshold Categories\r\n",
    "\r\n",
    "The code defines a set of threshold categories that represent different levels of matching confidence:\r\n",
    "\r\n",
    "```python\r\n",
    "thresholds = {\r\n",
    "    'Exact Match': 0.95,\r\n",
    "    'Strong Match': 0.85,\r\n",
    "    'Probable Match': 0.75,\r\n",
    "    'Possible Match': 0.65,\r\n",
    "    'Weak Match': 0.50,\r\n",
    "    'No Match': 0.0\r\n",
    "}\r\n",
    "```\r\n",
    "\r\n",
    "These thresholds create a gradient of confidence:\r\n",
    "\r\n",
    "- **Exact Match (0.95)**: Nearly certain to be the same merchant\r\n",
    "- **Strong Match (0.85)**: Very likely to be the same merchant\r\n",
    "- **Probable Match (0.75)**: Probably the same merchant\r\n",
    "- **Possible Match (0.65)**: Possibly the same merchant but requires verification\r\n",
    "- **Weak Match (0.50)**: Only some similarity, likely different merchants\r\n",
    "- **No Match (<0.50)**: Very unlikely to be the same merchant\r\n",
    "\r\n",
    "This granular approach is valuable because different applications might have different tolerances for false positives versus false negatives.\r\n",
    "\r\n",
    "## The Value of This Analysis Framework\r\n",
    "\r\n",
    "This evaluation and analysis framework provides several important benefits:\r\n",
    "\r\n",
    "### 1. Quantification of Improvement\r\n",
    "\r\n",
    "The code precisely quantifies how much the enhanced algorithm improves over the basic algorithm, both overall and by merchant category. This gives us concrete metrics to judge the value of our sophisticated matching techniques.\r\n",
    "\r\n",
    "### 2. Identification of Pattern-Specific Performance\r\n",
    "\r\n",
    "By analyzing performance by merchant category, we can identify if certain types of businesses have better or worse matching performance. This insight can guide targeted improvements to the algorithms.\r\n",
    "\r\n",
    "### 3. Sample-Based Understanding\r\n",
    "\r\n",
    "Rather than relying solely on aggregate statistics, the code shows us actual examples from each category. This helps us understand what types of merchant name variations are being successfully matched and which ones are challenging.\r\n",
    "\r\n",
    "### 4. Focused Improvement Analysis\r\n",
    "\r\n",
    "By identifying the \"most improved\" matches, the code helps us understand exactly which types of merchant name variations benefit most from our enhanced algorithms, guiding future development efforts.\r\n",
    "\r\n",
    "## Real-World Applications\r\n",
    "\r\n",
    "This analysis framework enables several practical applications:\r\n",
    "\r\n",
    "1. **Algorithm Tuning**: The detailed performance insights help data scientists fine-tune the matching algorithms for specific use cases.\r\n",
    "\r\n",
    "2. **Domain-Specific Customization**: The category-by-category analysis shows which industries might need specialized matching rules.\r\n",
    "\r\n",
    "3. **Error Analysis**: By examining the samples in each category, analysts can identify systematic errors and improve the matching system.\r\n",
    "\r\n",
    "4. **ROI Justification**: The quantitative improvement metrics help justify the investment in sophisticated matching algorithms.\r\n",
    "\r\n",
    "5. **Threshold Selection**: Different applications can choose appropriate thresholds based on their specific needs for precision versus recall.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This analysis and evaluation code completes our merchant name matching system by providing the tools needed to understand, interpret, and improve the matching results. It transforms raw similarity scores into meaningful categories and offers multi-dimensional analysis of performance across different merchant types. \r\n",
    "\r\n",
    "When combined with the previous components we've examined (data loading, preprocessing, similarity algorithms, and enhanced scoring), this code forms a complete, production-ready system for matching merchant names across diverse datasetsa capability that's essential for financial analytics, fraud detection, customer intelligence, and many other real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "67d491c4-30f6-4aa0-8023-1b4dbd571d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline execution functions defined!\n"
     ]
    }
   ],
   "source": [
    "# cell 9: Pipeline Execution functions\n",
    "\n",
    "def run_merchant_matching_pipeline(input_file, output_file=None, perform_domain_adaptation=True):\n",
    "    \"\"\"\n",
    "    Run the complete merchant matching pipeline with enhanced models\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input file\n",
    "        output_file (str, optional): Path to save results\n",
    "        perform_domain_adaptation (bool): Whether to perform domain adaptation\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Results DataFrame\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Running enhanced merchant matching pipeline...\")\n",
    "    print(f\"Input file: {input_file}\")\n",
    "    \n",
    "    # Step 1: Load merchant data\n",
    "    print(\"\\nStep 1: Loading merchant data...\")\n",
    "    try:\n",
    "        merchant_df = pd.read_excel(input_file)\n",
    "        print(f\"Successfully loaded {len(merchant_df)} records from {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {input_file}: {e}\")\n",
    "        print(\"Using sample data instead...\")\n",
    "        merchant_df = load_merchant_data(None)\n",
    "    \n",
    "    # Step 2: Preprocess merchant data\n",
    "    print(\"\\nStep 2: Preprocessing merchant data...\")\n",
    "    processed_df = preprocess_merchant_data(merchant_df)\n",
    "    \n",
    "    # Step 3: Initialize the enhanced matcher (already done in previous cells)\n",
    "    print(\"\\nStep 3: Setting up enhanced matcher with MPNet model...\")\n",
    "    # Use the pre-initialized merchant_matcher from Cell 6\n",
    "    \n",
    "    # Step 4: Perform domain adaptation if requested\n",
    "    if perform_domain_adaptation and hasattr(merchant_matcher.bert_embedder, 'adapt_to_domain'):\n",
    "        print(\"\\nStep 4: Performing domain adaptation for merchant names...\")\n",
    "        try:\n",
    "            # Use a subset of high-confidence matches for adaptation\n",
    "            adaptation_df = processed_df.sample(min(500, len(processed_df)))\n",
    "            merchant_matcher.bert_embedder.adapt_to_domain(adaptation_df)\n",
    "            print(\"Domain adaptation completed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Domain adaptation failed: {e}\")\n",
    "            print(\"Continuing without domain adaptation...\")\n",
    "    else:\n",
    "        print(\"\\nStep 4: Skipping domain adaptation...\")\n",
    "    \n",
    "    # Step 5: Process merchant data and compute similarity scores\n",
    "    print(\"\\nStep 5: Computing similarity scores...\")\n",
    "    results_df = process_merchant_data(processed_df, merchant_matcher)\n",
    "    \n",
    "    # Step 6: Add match categories based on thresholds\n",
    "    print(\"\\nStep 6: Categorizing matches based on threshold...\")\n",
    "    categorized_df = add_match_categories(results_df, thresholds)\n",
    "    \n",
    "    # Step 7: Analyze results\n",
    "    print(\"\\nStep 7: Analyzing matching results...\")\n",
    "    analysis_results = analyze_merchant_results(categorized_df)\n",
    "    \n",
    "    # Step 8: Save results if output file is provided\n",
    "    if output_file:\n",
    "        print(f\"\\nStep 8: Saving results to {output_file}...\")\n",
    "        try:\n",
    "            categorized_df.to_excel(output_file, index=False)\n",
    "            print(f\"Results successfully saved to {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "    else:\n",
    "        print(\"\\nStep 8: Skipping results saving (no output file specified)\")\n",
    "    \n",
    "    # Calculate and print timing information\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nPipeline completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Processed {len(categorized_df)} merchant entries\")\n",
    "    print(f\"Average processing time per entry: {total_time/len(categorized_df):.4f} seconds\")\n",
    "    \n",
    "    return categorized_df\n",
    "\n",
    "def process_DBAName_file_and_export_results(input_file=\"wrongless.xlsx\", \n",
    "                                           output_file=\"DBAName_Matching_Results.xlsx\"):\n",
    "    \"\"\"\n",
    "    Process the wrongless.xlsx file and export comprehensive matching results\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to wrongless.xlsx file\n",
    "        output_file (str): Path for the output Excel file with results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Processed results with all matching scores\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Loading and processing data from {input_file}...\")\n",
    "    \n",
    "    # Step 1: Load the data from wrongless.xlsx\n",
    "    try:\n",
    "        merchant_df = pd.read_excel(input_file)\n",
    "        print(f\"Successfully loaded {len(merchant_df)} records from {input_file}\")\n",
    "        print(f\"Columns found: {merchant_df.columns.tolist()}\")\n",
    "        print(f\"\\nSample data (first 3 rows):\")\n",
    "        print(merchant_df.head(3))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {input_file}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Preprocess the data\n",
    "    print(\"\\nPreprocessing merchant data...\")\n",
    "    processed_df = preprocess_merchant_data(merchant_df)\n",
    "    \n",
    "    # Step 3: Process with merchant matcher to get similarity scores\n",
    "    print(\"\\nCalculating similarity scores using enhanced merchant matcher...\")\n",
    "    results_df = process_merchant_data(processed_df, merchant_matcher)\n",
    "    \n",
    "    # Step 4: Add match categories based on score thresholds\n",
    "    print(\"\\nCategorizing matches based on thresholds...\")\n",
    "    categorized_df = add_match_categories(results_df, thresholds)\n",
    "    \n",
    "    # Step 5: Export the results\n",
    "    print(f\"\\nExporting results to {output_file}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create a writer for Excel output\n",
    "        with pd.ExcelWriter(output_file) as writer:\n",
    "            # Sheet 1: Main results with all scores\n",
    "            categorized_df.to_excel(writer, sheet_name=\"Matching_Results\", index=False)\n",
    "            \n",
    "            # Sheet 2: Summary statistics\n",
    "            category_counts = categorized_df['Match_Category'].value_counts().reset_index()\n",
    "            category_counts.columns = ['Match_Category', 'Count']\n",
    "            category_counts['Percentage'] = (category_counts['Count'] / len(categorized_df) * 100).round(2)\n",
    "            \n",
    "            # Sort by threshold order\n",
    "            category_order = list(thresholds.keys())\n",
    "            category_counts['Order'] = category_counts['Match_Category'].map({cat: i for i, cat in enumerate(category_order)})\n",
    "            category_counts = category_counts.sort_values('Order').drop('Order', axis=1)\n",
    "            \n",
    "            category_counts.to_excel(writer, sheet_name=\"Category_Summary\", index=False)\n",
    "            \n",
    "            # Sheet 3: Algorithm scores analysis\n",
    "            # For each merchant pair, get all individual algorithm scores\n",
    "            analysis_rows = []\n",
    "            \n",
    "            # Sample 50 entries (or all if fewer) for detailed algorithm analysis\n",
    "            sample_size = min(50, len(categorized_df))\n",
    "            sampled_df = categorized_df.sample(sample_size)\n",
    "            \n",
    "            for idx, row in sampled_df.iterrows():\n",
    "                DBAName = row['DBAName']\n",
    "                RawTransactionName = row['RawTransactionName']\n",
    "                domain = row['Merchant_Category'] if 'Merchant_Category' in row else None\n",
    "                \n",
    "                # Get all algorithm scores\n",
    "                all_scores = merchant_matcher.get_all_similarity_scores(DBAName, RawTransactionName, domain)\n",
    "                \n",
    "                # Add basic row info\n",
    "                score_row = {\n",
    "                    'DBAName': DBAName,\n",
    "                    'RawTransactionName': RawTransactionName,\n",
    "                    'Domain': domain,\n",
    "                    'Enhanced_Score': row['Enhanced_Score'],\n",
    "                    'Match_Category': row['Match_Category']\n",
    "                }\n",
    "                \n",
    "                # Add individual algorithm scores\n",
    "                for algo, score in all_scores.items():\n",
    "                    score_row[algo] = score\n",
    "                \n",
    "                analysis_rows.append(score_row)\n",
    "            \n",
    "            # Create algorithm analysis DataFrame\n",
    "            if analysis_rows:\n",
    "                algo_df = pd.DataFrame(analysis_rows)\n",
    "                algo_df.to_excel(writer, sheet_name=\"Algorithm_Analysis\", index=False)\n",
    "            \n",
    "            # Auto-adjust column widths for all sheets\n",
    "            for sheet_name in writer.sheets:\n",
    "                worksheet = writer.sheets[sheet_name]\n",
    "                for i, col in enumerate(categorized_df.columns):\n",
    "                    # Find the maximum length in the column\n",
    "                    max_len = max(\n",
    "                        categorized_df[col].astype(str).map(len).max(),  # max data length\n",
    "                        len(str(col))  # column name length\n",
    "                    ) + 2  # adding a little extra space\n",
    "                    \n",
    "                    # Set the column width\n",
    "                    worksheet.set_column(i, i, max_len)\n",
    "                    \n",
    "        print(f\"Results successfully exported to {output_file}\")\n",
    "        print(f\"\\nSummary of exported data:\")\n",
    "        print(f\"   Total merchant entries: {len(categorized_df)}\")\n",
    "        \n",
    "        # Display category distribution\n",
    "        print(\"\\nMatch Category Distribution:\")\n",
    "        for _, row in category_counts.iterrows():\n",
    "            print(f\"   {row['Match_Category']}: {row['Count']} entries ({row['Percentage']}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {e}\")\n",
    "        return categorized_df\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {processing_time:.2f} seconds\")\n",
    "    \n",
    "    return categorized_df\n",
    "\n",
    "print(\"Pipeline execution functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3bce0e-fc0c-4b4e-b426-b273e0920902",
   "metadata": {},
   "source": [
    "# Pipeline Execution Functions for Merchant Name Matching\r\n",
    "\r\n",
    "This code establishes the high-level orchestration layer that ties together all the previous components we've examined into complete, executable pipelines for merchant name matching. Let me walk you through what these functions do and why they're important for a production-ready system.\r\n",
    "\r\n",
    "## The Two Pipeline Functions\r\n",
    "\r\n",
    "This code defines two primary pipeline functions that serve different but related purposes in the merchant name matching system.\r\n",
    "\r\n",
    "### 1. The Complete End-to-End Pipeline\r\n",
    "\r\n",
    "The first function, `run_merchant_matching_pipeline()`, implements a comprehensive workflow that handles every stage of the merchant matching process:\r\n",
    "\r\n",
    "```python\r\n",
    "def run_merchant_matching_pipeline(input_file, output_file=None, perform_domain_adaptation=True):\r\n",
    "```\r\n",
    "\r\n",
    "This function orchestrates the entire process from start to finish through a clearly defined series of steps:\r\n",
    "\r\n",
    "**Step 1: Data Loading**\r\n",
    "It begins by loading merchant data from the specified input file, with a graceful fallback to sample data if the file can't be loaded.\r\n",
    "\r\n",
    "**Step 2: Data Preprocessing**\r\n",
    "It then preprocesses the data using the functionality we examined earlier, standardizing formats and preparing it for the matching algorithms.\r\n",
    "\r\n",
    "**Step 3: Setting Up the Matcher**\r\n",
    "It uses the pre-initialized matcher with its BERT embedding model for intelligent semantic matching.\r\n",
    "\r\n",
    "**Step 4: Domain Adaptation (Optional)**\r\n",
    "It potentially performs domain adaptation to fine-tune the BERT model based on a sample of the dataset, making the matching more accurate for the specific domain of merchant names being processed.\r\n",
    "\r\n",
    "**Step 5: Similarity Computation**\r\n",
    "It processes each merchant entry through the enhanced algorithms to generate similarity scores.\r\n",
    "\r\n",
    "**Step 6: Match Categorization**\r\n",
    "It categorizes the matches based on the predefined thresholds, converting numerical scores to meaningful labels.\r\n",
    "\r\n",
    "**Step 7: Results Analysis**\r\n",
    "It performs comprehensive analysis of the matching results to understand performance.\r\n",
    "\r\n",
    "**Step 8: Results Export (Optional)**\r\n",
    "It optionally saves the results to an output file, enabling persistence for later review or integration with other systems.\r\n",
    "\r\n",
    "This function provides detailed progress updates and timing information, making it both a powerful processing tool and a diagnostic instrument for understanding the system's performance.\r\n",
    "\r\n",
    "### 2. The Specialized Analysis Pipeline\r\n",
    "\r\n",
    "The second function, `process_DBAName_file_and_export_results()`, is optimized specifically for processing the \"wrongless.xlsx\" file and generating rich, multi-sheet analysis results:\r\n",
    "\r\n",
    "```python\r\n",
    "def process_DBAName_file_and_export_results(input_file=\"wrongless.xlsx\", \r\n",
    "                                           output_file=\"DBAName_Matching_Results.xlsx\"):\r\n",
    "```\r\n",
    "\r\n",
    "While following a similar overall flow to the first pipeline, this function has specialized features:\r\n",
    "\r\n",
    "**Rich, Multi-Sheet Excel Output**\r\n",
    "Unlike the first pipeline, this function creates a comprehensive Excel workbook with multiple sheets:\r\n",
    "- **Sheet 1: Matching Results** - The complete results dataset with all scores\r\n",
    "- **Sheet 2: Category Summary** - Statistical breakdown of match categories\r\n",
    "- **Sheet 3: Algorithm Analysis** - Detailed per-algorithm performance on a sample of matches\r\n",
    "\r\n",
    "**Algorithm-Specific Analysis**\r\n",
    "This pipeline goes deeper into the individual algorithm behavior:\r\n",
    "```python\r\n",
    "# Get all algorithm scores\r\n",
    "all_scores = merchant_matcher.get_all_similarity_scores(DBAName, RawTransactionName, domain)\r\n",
    "\r\n",
    "# Add individual algorithm scores\r\n",
    "for algo, score in all_scores.items():\r\n",
    "    score_row[algo] = score\r\n",
    "```\r\n",
    "\r\n",
    "This reveals which specific algorithms are contributing most to accurate matches, providing insights that could guide future refinements.\r\n",
    "\r\n",
    "**Enhanced Excel Formatting**\r\n",
    "The function also improves the readability of the output:\r\n",
    "```python\r\n",
    "# Auto-adjust column widths for all sheets\r\n",
    "for sheet_name in writer.sheets:\r\n",
    "    worksheet = writer.sheets[sheet_name]\r\n",
    "    for i, col in enumerate(categorized_df.columns):\r\n",
    "        # Find the maximum length in the column\r\n",
    "        max_len = max(\r\n",
    "            categorized_df[col].astype(str).map(len).max(),  # max data length\r\n",
    "            len(str(col))  # column name length\r\n",
    "        ) + 2  # adding a little extra space\r\n",
    "        \r\n",
    "        # Set the column width\r\n",
    "        worksheet.set_column(i, i, max_len)\r\n",
    "```\r\n",
    "\r\n",
    "This attention to detail makes the results more accessible and professional, important for reports that might be shared with stakeholders.\r\n",
    "\r\n",
    "## Key Technical Design Elements\r\n",
    "\r\n",
    "The pipeline implementation demonstrates several important technical design principles:\r\n",
    "\r\n",
    "### 1. Systematic Error Handling\r\n",
    "\r\n",
    "Both functions incorporate comprehensive error handling at each stage, ensuring that the pipeline can recover from problems and continue processing:\r\n",
    "\r\n",
    "```python\r\n",
    "try:\r\n",
    "    merchant_df = pd.read_excel(input_file)\r\n",
    "    print(f\"Successfully loaded {len(merchant_df)} records from {input_file}\")\r\n",
    "except Exception as e:\r\n",
    "    print(f\"Error loading data from {input_file}: {e}\")\r\n",
    "    print(\"Using sample data instead...\")\r\n",
    "    merchant_df = load_merchant_data(None)\r\n",
    "```\r\n",
    "\r\n",
    "This approach handles real-world data issues gracefully, making the system robust in production environments.\r\n",
    "\r\n",
    "### 2. Detailed Progress Reporting\r\n",
    "\r\n",
    "The pipelines provide rich progress information, helping users understand what's happening and estimate completion times:\r\n",
    "\r\n",
    "```python\r\n",
    "# Calculate and print timing information\r\n",
    "total_time = time.time() - start_time\r\n",
    "print(f\"\\nPipeline completed in {total_time:.2f} seconds\")\r\n",
    "print(f\"Processed {len(categorized_df)} merchant entries\")\r\n",
    "print(f\"Average processing time per entry: {total_time/len(categorized_df):.4f} seconds\")\r\n",
    "```\r\n",
    "\r\n",
    "This feedback is essential for production runs that might process large datasets over extended periods.\r\n",
    "\r\n",
    "### 3. Conditional Processing Steps\r\n",
    "\r\n",
    "The pipeline intelligently includes or excludes processing steps based on parameters and system capabilities:\r\n",
    "\r\n",
    "```python\r\n",
    "if perform_domain_adaptation and hasattr(merchant_matcher.bert_embedder, 'adapt_to_domain'):\r\n",
    "    print(\"\\nStep 4: Performing domain adaptation for merchant names...\")\r\n",
    "    try:\r\n",
    "        # Use a subset of high-confidence matches for adaptation\r\n",
    "        adaptation_df = processed_df.sample(min(500, len(processed_df)))\r\n",
    "        merchant_matcher.bert_embedder.adapt_to_domain(adaptation_df)\r\n",
    "        print(\"Domain adaptation completed successfully\")\r\n",
    "    except Exception as e:\r\n",
    "        print(f\"Warning: Domain adaptation failed: {e}\")\r\n",
    "        print(\"Continuing without domain adaptation...\")\r\n",
    "else:\r\n",
    "    print(\"\\nStep 4: Skipping domain adaptation...\")\r\n",
    "```\r\n",
    "\r\n",
    "This flexibility allows the pipeline to adapt to different execution environments and user requirements.\r\n",
    "\r\n",
    "### 4. Resource-Conscious Sampling\r\n",
    "\r\n",
    "The specialized analysis pipeline uses sampling to balance analysis depth with resource constraints:\r\n",
    "\r\n",
    "```python\r\n",
    "# Sample 50 entries (or all if fewer) for detailed algorithm analysis\r\n",
    "sample_size = min(50, len(categorized_df))\r\n",
    "sampled_df = categorized_df.sample(sample_size)\r\n",
    "```\r\n",
    "\r\n",
    "This approach enables detailed per-algorithm analysis without overwhelming system resources or creating excessively large output files.\r\n",
    "\r\n",
    "## The Value of Pipeline Orchestration\r\n",
    "\r\n",
    "These pipeline functions offer several significant benefits over using the component functions individually:\r\n",
    "\r\n",
    "### 1. Workflow Automation\r\n",
    "\r\n",
    "They automate the entire process from raw data to final analysis, eliminating manual steps and reducing the risk of errors.\r\n",
    "\r\n",
    "### 2. Consistent Processing\r\n",
    "\r\n",
    "They ensure that every merchant name pair goes through the same standardized process, leading to consistent, reproducible results.\r\n",
    "\r\n",
    "### 3. Comprehensive Logging\r\n",
    "\r\n",
    "They provide detailed logging at each stage, creating an audit trail that's valuable for debugging and performance optimization.\r\n",
    "\r\n",
    "### 4. Process Integration\r\n",
    "\r\n",
    "They allow easy integration with surrounding systems by accepting input files and producing output files in standard formats.\r\n",
    "\r\n",
    "### 5. Configuration Flexibility\r\n",
    "\r\n",
    "They provide parameters to customize behavior (like enabling/disabling domain adaptation) without requiring code changes.\r\n",
    "\r\n",
    "## Real-World Applications\r\n",
    "\r\n",
    "These pipeline functions enable several practical applications:\r\n",
    "\r\n",
    "1. **Batch Processing**: Financial institutions can process large batches of transaction data to normalize merchant names across their database.\r\n",
    "\r\n",
    "2. **Performance Benchmarking**: Analysts can compare the performance of different merchant matching approaches by running the pipeline with different configurations.\r\n",
    "\r\n",
    "3. **Data Quality Improvement**: Data teams can identify and correct problematic merchant name patterns by analyzing the detailed output of the pipeline.\r\n",
    "\r\n",
    "4. **Model Refinement**: Machine learning engineers can use the algorithm-specific analysis to identify which components of the system would benefit most from improvement.\r\n",
    "\r\n",
    "5. **Regulatory Reporting**: Organizations can generate standardized reports on merchant name matching quality for compliance purposes.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This code represents the critical orchestration layer that transforms the merchant matching components from theoretical tools into a practical, production-ready system. By automating the entire workflow from data loading through analysis and reporting, these pipeline functions make sophisticated merchant name matching accessible and actionable.\r\n",
    "\r\n",
    "The combination of comprehensive workflow management, robust error handling, detailed progress reporting, and rich result analysis makes this code an essential part of a complete merchant name matching solution. It bridges the gap between the sophisticated algorithms we've examined and their practical application to real-world business challenges in financial services, analytics, and data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de6099b0-6797-425d-8d6d-8d6619cd4405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive merchant matcher function defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Interactive Merchant Name Matching\n",
    "\n",
    "def interactive_merchant_matcher(merchant_matcher, examples=None, top_n=3):\n",
    "    \"\"\"\n",
    "    Interactive matcher for testing merchant name matching with detailed explanations\n",
    "    \n",
    "    Args:\n",
    "        merchant_matcher: Enhanced merchant matcher instance\n",
    "        examples (list, optional): List of example pairs to suggest\n",
    "        top_n (int): Number of algorithm scores to show\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if examples is None:\n",
    "        examples = [\n",
    "            ('BoA', 'Bank of America'),\n",
    "            ('MCD', 'McDonalds'),\n",
    "            ('WMT', 'Walmart Inc'),\n",
    "            ('AMZN', 'Amazon.com'),\n",
    "            ('StarBucks', 'Starbucks Coffee Company'),\n",
    "            ('Western Toyota', 'Toyota Corporation')\n",
    "        ]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Interactive Merchant Name Matcher\".center(80))\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nThis tool helps you test the enhanced merchant matching algorithm.\")\n",
    "    print(\"Enter two merchant names to compare, or type 'quit' to exit.\")\n",
    "    print(\"\\nExample pairs you can try:\")\n",
    "    for i, (DBAName, RawTransactionName) in enumerate(examples):\n",
    "        print(f\"  {i+1}. '{DBAName}' <-> '{RawTransactionName}'\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Enter merchant names to compare (or 'quit' to exit):\")\n",
    "        \n",
    "        # Get DBAName input\n",
    "        DBAName = input(\"DBAName or Short Name: \").strip()\n",
    "        if DBAName.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Get full name input\n",
    "        RawTransactionName = input(\"Full Name: \").strip()\n",
    "        if RawTransactionName.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Optional domain input\n",
    "        domain = input(\"Domain (optional, e.g., Banking, Restaurant): \").strip()\n",
    "        if not domain:\n",
    "            domain = None\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        print(\"\\nComputing similarity scores...\")\n",
    "        all_scores = merchant_matcher.get_all_similarity_scores(DBAName, RawTransactionName, domain)\n",
    "        weighted_score = merchant_matcher.compute_weighted_score(DBAName, RawTransactionName, domain)\n",
    "        enhanced_score = merchant_matcher.compute_enhanced_score(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        # Determine match category\n",
    "        match_category = \"No Match\"\n",
    "        for category, threshold in sorted(thresholds.items(), key=lambda x: x[1], reverse=True):\n",
    "            if enhanced_score >= threshold:\n",
    "                match_category = category\n",
    "                break\n",
    "        \n",
    "        # Show preprocessing results\n",
    "        DBAName_clean, RawTransactionName_clean = merchant_matcher.preprocess_pair(DBAName, RawTransactionName, domain)\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(f\"  Preprocessed DBAName: '{DBAName_clean}'\")\n",
    "        print(f\"  Preprocessed Full Name: '{RawTransactionName_clean}'\")\n",
    "        print(f\"  Weighted Score: {weighted_score:.4f}\")\n",
    "        print(f\"  Enhanced Score: {enhanced_score:.4f}\")\n",
    "        print(f\"  Match Category: {match_category}\")\n",
    "        \n",
    "        # Show detected business patterns\n",
    "        patterns = merchant_matcher.detect_complex_business_patterns(DBAName, RawTransactionName, domain)\n",
    "        if patterns:\n",
    "            print(\"\\nDetected Business Patterns:\")\n",
    "            for pattern, score in patterns.items():\n",
    "                print(f\"   {pattern.replace('_', ' ').title()}: {score:.4f}\")\n",
    "        \n",
    "        # Show top individual algorithm scores\n",
    "        print(\"\\nTop Individual Algorithm Scores:\")\n",
    "        top_scores = sorted(all_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        for algo, score in top_scores:\n",
    "            print(f\"   {algo.replace('_', ' ').title()}: {score:.4f}\")\n",
    "        \n",
    "        # Show weights used\n",
    "        weights = merchant_matcher.get_dynamic_weights(DBAName, RawTransactionName, domain)\n",
    "        print(\"\\nTop Algorithm Weights:\")\n",
    "        top_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        for algo, weight in top_weights:\n",
    "            print(f\"   {algo.replace('_', ' ').title()}: {weight:.4f}\")\n",
    "        \n",
    "        # Provide explanation for the score\n",
    "        print(\"\\nExplanation:\")\n",
    "        if enhanced_score >= 0.95:\n",
    "            print(\"  This is an EXACT MATCH with very high confidence.\")\n",
    "        elif enhanced_score >= 0.85:\n",
    "            print(\"  This is a STRONG MATCH. The names are highly similar.\")\n",
    "        elif enhanced_score >= 0.75:\n",
    "            print(\"  This is a PROBABLE MATCH. The names are quite similar.\")\n",
    "        elif enhanced_score >= 0.65:\n",
    "            print(\"  This is a POSSIBLE MATCH. The names have significant similarity.\")\n",
    "        elif enhanced_score >= 0.50:\n",
    "            print(\"  This is a WEAK MATCH. The names have some similarity but should be reviewed.\")\n",
    "        else:\n",
    "            print(\"  This is likely NOT A MATCH. The names are too dissimilar.\")\n",
    "        \n",
    "        # Explain key factors\n",
    "        if patterns:\n",
    "            pattern_names = [p.replace('_', ' ').title() for p in patterns.keys()]\n",
    "            print(f\"  Key factors: Detected {', '.join(pattern_names)}.\")\n",
    "        \n",
    "        if 'bert_similarity' in all_scores and all_scores['bert_similarity'] > 0.8:\n",
    "            print(f\"  High semantic understanding: The names have similar meanings.\")\n",
    "        \n",
    "        if 'enhanced_DBAName_formation' in all_scores and all_scores['enhanced_DBAName_formation'] > 0.8:\n",
    "            print(f\"  Strong DBAName formation: The short form is a good DBAName of the full name.\")\n",
    "        \n",
    "        # Ask if the user wants to try another pair\n",
    "        continue_choice = input(\"\\nTry another pair? (y/n): \").strip().lower()\n",
    "        if continue_choice != 'y':\n",
    "            break\n",
    "    \n",
    "    print(\"\\nThank you for using the Interactive Merchant Name Matcher!\")\n",
    "\n",
    "print(\"Interactive merchant matcher function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae63d3-f490-4cfc-9a11-f448853e1c72",
   "metadata": {},
   "source": [
    "# Understanding the Interactive Merchant Name Matcher\r\n",
    "\r\n",
    "This piece of code creates an interactive command-line interface that allows users to directly test and explore the merchant name matching system we've been studying. Let me walk you through how this works and why it's valuable.\r\n",
    "\r\n",
    "## The Purpose of Interactive Testing\r\n",
    "\r\n",
    "While the pipeline functions we examined earlier are excellent for batch processing large datasets, they don't provide a way for users to experiment with individual merchant name pairs and see detailed explanations of the matching process. This interactive interface fills that gap by allowing users to:\r\n",
    "\r\n",
    "1. Enter their own merchant name pairs for testing\r\n",
    "2. See comprehensive details about how the match was evaluated\r\n",
    "3. Understand which algorithms contributed most to the match decision\r\n",
    "4. Explore the reasoning behind the final match score\r\n",
    "\r\n",
    "This kind of interactive tool is invaluable for several purposes:\r\n",
    "\r\n",
    "- **Education**: Helping users understand how the matching system works\r\n",
    "- **Quality Assurance**: Testing specific cases that might be problematic\r\n",
    "- **Demonstration**: Showcasing the system's capabilities to stakeholders\r\n",
    "- **Debugging**: Investigating why certain matches might not be working as expected\r\n",
    "\r\n",
    "## How the Interactive Interface Works\r\n",
    "\r\n",
    "The function `interactive_merchant_matcher()` creates a command-line interface that operates in a continuous loop, allowing users to test multiple merchant name pairs in a single session.\r\n",
    "\r\n",
    "### Initialization and Instructions\r\n",
    "\r\n",
    "The function begins by displaying a welcome message and instructions:\r\n",
    "\r\n",
    "```python\r\n",
    "print(\"=\" * 80)\r\n",
    "print(\"Interactive Merchant Name Matcher\".center(80))\r\n",
    "print(\"=\" * 80)\r\n",
    "print(\"\\nThis tool helps you test the enhanced merchant matching algorithm.\")\r\n",
    "print(\"Enter two merchant names to compare, or type 'quit' to exit.\")\r\n",
    "```\r\n",
    "\r\n",
    "It also provides example pairs to try, making it easier for new users to get started:\r\n",
    "\r\n",
    "```python\r\n",
    "print(\"\\nExample pairs you can try:\")\r\n",
    "for i, (DBAName, RawTransactionName) in enumerate(examples):\r\n",
    "    print(f\"  {i+1}. '{DBAName}' <-> '{RawTransactionName}'\")\r\n",
    "```\r\n",
    "\r\n",
    "These examples cover diverse industries (banking, restaurants, retail, technology) to showcase the system's versatility.\r\n",
    "\r\n",
    "### The Interactive Loop\r\n",
    "\r\n",
    "The core of the function is a continuous loop that:\r\n",
    "1. Collects user input for merchant names and optional domain\r\n",
    "2. Processes the match using all available algorithms\r\n",
    "3. Displays detailed results\r\n",
    "4. Asks if the user wants to continue\r\n",
    "\r\n",
    "```python\r\n",
    "while True:\r\n",
    "    # Get user input\r\n",
    "    DBAName = input(\"DBAName or Short Name: \").strip()\r\n",
    "    if DBAName.lower() == 'quit':\r\n",
    "        break\r\n",
    "    \r\n",
    "    # ... processing steps ...\r\n",
    "    \r\n",
    "    # Ask to continue\r\n",
    "    continue_choice = input(\"\\nTry another pair? (y/n): \").strip().lower()\r\n",
    "    if continue_choice != 'y':\r\n",
    "        break\r\n",
    "```\r\n",
    "\r\n",
    "### Comprehensive Results Display\r\n",
    "\r\n",
    "What makes this interface particularly valuable is the level of detail it provides about the matching process. After computing the match, it shows:\r\n",
    "\r\n",
    "1. **Preprocessing Results**: How the input names were normalized before matching\r\n",
    "```python\r\n",
    "print(f\"  Preprocessed DBAName: '{DBAName_clean}'\")\r\n",
    "print(f\"  Preprocessed Full Name: '{RawTransactionName_clean}'\")\r\n",
    "```\r\n",
    "\r\n",
    "2. **Overall Scores**: Both weighted and enhanced scores\r\n",
    "```python\r\n",
    "print(f\"  Weighted Score: {weighted_score:.4f}\")\r\n",
    "print(f\"  Enhanced Score: {enhanced_score:.4f}\")\r\n",
    "print(f\"  Match Category: {match_category}\")\r\n",
    "```\r\n",
    "\r\n",
    "3. **Detected Business Patterns**: Any specific naming patterns identified\r\n",
    "```python\r\n",
    "if patterns:\r\n",
    "    print(\"\\nDetected Business Patterns:\")\r\n",
    "    for pattern, score in patterns.items():\r\n",
    "        print(f\"   {pattern.replace('_', ' ').title()}: {score:.4f}\")\r\n",
    "```\r\n",
    "\r\n",
    "4. **Top Individual Algorithm Scores**: Which algorithms contributed most to the match\r\n",
    "```python\r\n",
    "print(\"\\nTop Individual Algorithm Scores:\")\r\n",
    "top_scores = sorted(all_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\r\n",
    "for algo, score in top_scores:\r\n",
    "    print(f\"   {algo.replace('_', ' ').title()}: {score:.4f}\")\r\n",
    "```\r\n",
    "\r\n",
    "5. **Algorithm Weights**: How different algorithms were weighted\r\n",
    "```python\r\n",
    "print(\"\\nTop Algorithm Weights:\")\r\n",
    "top_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)[:top_n]\r\n",
    "for algo, weight in top_weights:\r\n",
    "    print(f\"   {algo.replace('_', ' ').title()}: {weight:.4f}\")\r\n",
    "```\r\n",
    "\r\n",
    "6. **Human-Readable Explanation**: A narrative description of the match quality\r\n",
    "```python\r\n",
    "print(\"\\nExplanation:\")\r\n",
    "if enhanced_score >= 0.95:\r\n",
    "    print(\"  This is an EXACT MATCH with very high confidence.\")\r\n",
    "elif enhanced_score >= 0.85:\r\n",
    "    print(\"  This is a STRONG MATCH. The names are highly similar.\")\r\n",
    "# ... and so on for other categories\r\n",
    "```\r\n",
    "\r\n",
    "7. **Key Factors**: The most important elements influencing the match\r\n",
    "```python\r\n",
    "if patterns:\r\n",
    "    pattern_names = [p.replace('_', ' ').title() for p in patterns.keys()]\r\n",
    "    print(f\"  Key factors: Detected {', '.join(pattern_names)}.\")\r\n",
    "\r\n",
    "if 'bert_similarity' in all_scores and all_scores['bert_similarity'] > 0.8:\r\n",
    "    print(f\"  High semantic understanding: The names have similar meanings.\")\r\n",
    "```\r\n",
    "\r\n",
    "## Technical Design Elements\r\n",
    "\r\n",
    "Several design elements make this interactive interface particularly effective:\r\n",
    "\r\n",
    "### 1. Graceful Exit Options\r\n",
    "\r\n",
    "The interface allows users to exit in multiple ways:\r\n",
    "- Typing 'quit' as either merchant name\r\n",
    "- Answering 'n' to the \"Try another pair?\" prompt\r\n",
    "\r\n",
    "This provides flexibility and prevents users from feeling trapped in the loop.\r\n",
    "\r\n",
    "### 2. Input Validation\r\n",
    "\r\n",
    "The interface handles various input conditions:\r\n",
    "- Empty inputs are handled gracefully\r\n",
    "- Domain input is optional\r\n",
    "- Input strings are stripped of leading/trailing whitespace\r\n",
    "\r\n",
    "### 3. Formatted Output\r\n",
    "\r\n",
    "The output is carefully formatted for readability:\r\n",
    "- Section headers with clear separators\r\n",
    "- Bullet points for lists of results\r\n",
    "- Rounded numeric values for clarity\r\n",
    "- Human-friendly formatting of algorithm and pattern names (replacing underscores with spaces, proper capitalization)\r\n",
    "\r\n",
    "### 4. Multi-level Explanations\r\n",
    "\r\n",
    "The interface provides explanations at multiple levels of detail:\r\n",
    "- A high-level match category (EXACT MATCH, STRONG MATCH, etc.)\r\n",
    "- A narrative explanation of what that category means\r\n",
    "- Specific factors contributing to the match\r\n",
    "- Detailed algorithm scores for those who want to dig deeper\r\n",
    "\r\n",
    "## Educational Value\r\n",
    "\r\n",
    "This interactive interface serves as an excellent educational tool to help users understand:\r\n",
    "\r\n",
    "1. **How Merchant Name Preprocessing Works**: By showing the transformed inputs, users learn how the system normalizes merchant names.\r\n",
    "\r\n",
    "2. **The Role of Different Algorithms**: By displaying top algorithm scores, users can see which techniques are most effective for different types of merchant names.\r\n",
    "\r\n",
    "3. **Pattern Recognition in Action**: By highlighting detected business patterns, users understand the specialized domain knowledge built into the system.\r\n",
    "\r\n",
    "4. **Dynamic Weighting**: By showing algorithm weights, users learn how the system adapts its approach for different merchant name pairs.\r\n",
    "\r\n",
    "5. **Confidence Levels**: By categorizing matches across a spectrum from \"EXACT MATCH\" to \"NOT A MATCH,\" users develop an intuition for interpreting similarity scores.\r\n",
    "\r\n",
    "## Real-World Applications\r\n",
    "\r\n",
    "This interactive interface enables several valuable real-world use cases:\r\n",
    "\r\n",
    "1. **Analyst Training**: Teaching data analysts how the merchant matching system works so they can better interpret and validate its results.\r\n",
    "\r\n",
    "2. **System Calibration**: Helping data scientists fine-tune thresholds and weights by experimenting with representative merchant name pairs.\r\n",
    "\r\n",
    "3. **Edge Case Testing**: Investigating problematic merchant names that might not be matching as expected in batch processing.\r\n",
    "\r\n",
    "4. **Customer Demonstrations**: Showing the system's capabilities to potential users or stakeholders with immediate, tangible examples.\r\n",
    "\r\n",
    "5. **Data Exploration**: Exploring patterns in merchant naming conventions to inform future improvements to the matching algorithms.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This interactive merchant matcher serves as the human-friendly face of the sophisticated matching system we've been examining. While the batch processing pipelines handle the heavy lifting of processing large datasets, this interactive interface provides a window into how the system thinks and makes decisions.\r\n",
    "\r\n",
    "By allowing users to directly experiment with merchant name pairs and providing detailed explanations of the matching process, it transforms a complex algorithmic system into something understandable and tangible. This transparency builds trust in the system and empowers users to leverage its capabilities more effectively in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2bec527e-d3dc-43c1-9a78-71e0e80d0026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Batch Processing Functions\n",
    "\n",
    "def adapt_for_pyspark(spark=None):\n",
    "    \"\"\"\n",
    "    Create PySpark UDFs and pipeline components for large-scale processing\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance (optional)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing PySpark UDFs and pipeline components\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.functions import udf, col\n",
    "        from pyspark.sql.types import DoubleType, StringType, StructType, StructField, ArrayType\n",
    "        pyspark_available = True\n",
    "    except ImportError:\n",
    "        pyspark_available = False\n",
    "        print(\"Warning: PySpark not available. Returning dummy implementation.\")\n",
    "        return {\"error\": \"PySpark not available\"}\n",
    "    \n",
    "    # Create SparkSession if not provided\n",
    "    if spark is None and pyspark_available:\n",
    "        try:\n",
    "            spark = SparkSession.builder \\\n",
    "                .appName(\"MerchantMatcherPipeline\") \\\n",
    "                .getOrCreate()\n",
    "            print(f\"Created Spark session: {spark.version}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create Spark session: {e}\")\n",
    "            pyspark_available = False\n",
    "    \n",
    "    if not pyspark_available:\n",
    "        return {\"error\": \"PySpark not available or failed to initialize\"}\n",
    "    \n",
    "    # Create UDFs for preprocessing and scoring\n",
    "    preprocessing_udf = udf(\n",
    "        lambda DBAName, RawTransactionName, domain: merchant_matcher.preprocess_pair(DBAName, RawTransactionName, domain),\n",
    "        StructType([\n",
    "            StructField(\"DBAName_clean\", StringType(), True),\n",
    "            StructField(\"RawTransactionName_clean\", StringType(), True)\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    weighted_score_udf = udf(\n",
    "        lambda DBAName, RawTransactionName, domain: float(merchant_matcher.compute_weighted_score(DBAName, RawTransactionName, domain)), \n",
    "        DoubleType()\n",
    "    )\n",
    "    \n",
    "    enhanced_score_udf = udf(\n",
    "        lambda DBAName, RawTransactionName, domain: float(merchant_matcher.compute_enhanced_score(DBAName, RawTransactionName, domain)), \n",
    "        DoubleType()\n",
    "    )\n",
    "    \n",
    "    match_category_udf = udf(\n",
    "        lambda score: next((cat for cat, thresh in sorted(thresholds.items(), key=lambda x: x[1], reverse=True) \n",
    "                         if score >= thresh), \"No Match\"),\n",
    "        StringType()\n",
    "    )\n",
    "    \n",
    "    # Define a pipeline function\n",
    "    def process_merchant_spark_df(df, DBAName_col=\"DBAName\", RawTransactionName_col=\"RawTransactionName\", \n",
    "                                 domain_col=\"Merchant_Category\"):\n",
    "        \"\"\"\n",
    "        Process merchant data using PySpark\n",
    "        \n",
    "        Args:\n",
    "            df: Spark DataFrame with merchant data\n",
    "            DBAName_col: Column name for DBAName/short name\n",
    "            RawTransactionName_col: Column name for full name\n",
    "            domain_col: Column name for merchant category/domain\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Spark DataFrame with results\n",
    "        \"\"\"\n",
    "        # Handle null values\n",
    "        df = df.na.fill(\"\", [DBAName_col, RawTransactionName_col])\n",
    "        df = df.na.fill(\"Unknown\", [domain_col])\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        df = df.withColumn(\n",
    "            \"preprocessed\", \n",
    "            preprocessing_udf(col(DBAName_col), col(RawTransactionName_col), col(domain_col))\n",
    "        )\n",
    "        \n",
    "        # Calculate scores\n",
    "        df = df.withColumn(\n",
    "            \"Weighted_Score\", \n",
    "            weighted_score_udf(col(DBAName_col), col(RawTransactionName_col), col(domain_col))\n",
    "        )\n",
    "        \n",
    "        df = df.withColumn(\n",
    "            \"Enhanced_Score\", \n",
    "            enhanced_score_udf(col(DBAName_col), col(RawTransactionName_col), col(domain_col))\n",
    "        )\n",
    "        \n",
    "        # Add match category\n",
    "        df = df.withColumn(\n",
    "            \"Match_Category\",\n",
    "            match_category_udf(col(\"Enhanced_Score\"))\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Create a function to batch process a large DataFrame\n",
    "    def batch_process_merchant_data(df, batch_size=10000):\n",
    "        \"\"\"\n",
    "        Process a large merchant dataset in batches to avoid memory issues\n",
    "        \n",
    "        Args:\n",
    "            df: Spark DataFrame with merchant data\n",
    "            batch_size: Size of batches to process\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Spark DataFrame with results\n",
    "        \"\"\"\n",
    "        # Count total records\n",
    "        total_records = df.count()\n",
    "        print(f\"Processing {total_records} records in batches of {batch_size}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        results = []\n",
    "        for i in range(0, total_records, batch_size):\n",
    "            # Take a batch\n",
    "            batch_df = df.limit(batch_size).offset(i)\n",
    "            \n",
    "            # Process batch\n",
    "            processed_batch = process_merchant_spark_df(batch_df)\n",
    "            \n",
    "            # Collect results (careful with large datasets)\n",
    "            batch_results = processed_batch.collect()\n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            # Log progress\n",
    "            processed_so_far = min(i + batch_size, total_records)\n",
    "            print(f\"Processed {processed_so_far}/{total_records} records ({processed_so_far/total_records*100:.1f}%)\")\n",
    "        \n",
    "        # Convert results back to Spark DataFrame\n",
    "        results_df = spark.createDataFrame(results)\n",
    "        return results_df\n",
    "    \n",
    "    return {\n",
    "        \"spark_session\": spark,\n",
    "        \"preprocessing_udf\": preprocessing_udf,\n",
    "        \"weighted_score_udf\": weighted_score_udf,\n",
    "        \"enhanced_score_udf\": enhanced_score_udf,\n",
    "        \"match_category_udf\": match_category_udf,\n",
    "        \"process_merchant_spark_df\": process_merchant_spark_df,\n",
    "        \"batch_process_merchant_data\": batch_process_merchant_data\n",
    "    }\n",
    "\n",
    "def batch_process_file(input_file, output_file, batch_size=10000, use_spark=False):\n",
    "    \"\"\"\n",
    "    Process a large merchant dataset file in batches\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input file (Excel or CSV)\n",
    "        output_file: Path to output file\n",
    "        batch_size: Size of batches to process\n",
    "        use_spark: Whether to use PySpark for processing\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processing statistics\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Batch processing merchant data from {input_file}\")\n",
    "    \n",
    "    # Determine file type\n",
    "    is_excel = input_file.lower().endswith(('.xlsx', '.xls'))\n",
    "    is_csv = input_file.lower().endswith('.csv')\n",
    "    \n",
    "    if not (is_excel or is_csv):\n",
    "        raise ValueError(\"Input file must be Excel (.xlsx/.xls) or CSV (.csv)\")\n",
    "    \n",
    "    # Process with PySpark if requested\n",
    "    if use_spark:\n",
    "        try:\n",
    "            from pyspark.sql import SparkSession\n",
    "            \n",
    "            # Initialize Spark session\n",
    "            spark = SparkSession.builder \\\n",
    "                .appName(\"MerchantMatcherBatchProcessing\") \\\n",
    "                .getOrCreate()\n",
    "            \n",
    "            # Read input file\n",
    "            if is_excel:\n",
    "                df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .option(\"inferSchema\", \"true\") \\\n",
    "                    .load(input_file)\n",
    "            else:  # CSV\n",
    "                df = spark.read.option(\"header\", \"true\") \\\n",
    "                    .option(\"inferSchema\", \"true\") \\\n",
    "                    .csv(input_file)\n",
    "            \n",
    "            # Get PySpark components\n",
    "            spark_components = adapt_for_pyspark(spark)\n",
    "            \n",
    "            # Process data\n",
    "            results_df = spark_components[\"batch_process_merchant_data\"](df, batch_size)\n",
    "            \n",
    "            # Save results\n",
    "            if output_file.lower().endswith('.csv'):\n",
    "                results_df.write.option(\"header\", \"true\").csv(output_file)\n",
    "            else:\n",
    "                # Convert to pandas for Excel output\n",
    "                pd_df = results_df.toPandas()\n",
    "                pd_df.to_excel(output_file, index=False)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            return {\n",
    "                \"input_file\": input_file,\n",
    "                \"output_file\": output_file,\n",
    "                \"records_processed\": results_df.count(),\n",
    "                \"processing_time\": processing_time,\n",
    "                \"records_per_second\": results_df.count() / processing_time\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing with PySpark: {e}\")\n",
    "            print(\"Falling back to pandas processing...\")\n",
    "            use_spark = False\n",
    "    \n",
    "    # Process with pandas\n",
    "    if not use_spark:\n",
    "        # Read input file\n",
    "        if is_excel:\n",
    "            # Read in chunks if Excel file is large\n",
    "            try:\n",
    "                df = pd.read_excel(input_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading entire Excel file: {e}\")\n",
    "                print(\"Trying to read with limited rows...\")\n",
    "                df = pd.read_excel(input_file, nrows=1000000)  # Limit to 1M rows\n",
    "        else:  # CSV\n",
    "            # Use chunking for CSV\n",
    "            chunks = []\n",
    "            chunk_size = min(batch_size, 100000)  # Default chunk size\n",
    "            \n",
    "            for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "                chunks.append(chunk)\n",
    "                print(f\"Read chunk with {len(chunk)} rows\")\n",
    "            \n",
    "            df = pd.concat(chunks)\n",
    "        \n",
    "        # Preprocess data\n",
    "        processed_df = preprocess_merchant_data(df)\n",
    "        \n",
    "        # Process in batches\n",
    "        total_rows = len(processed_df)\n",
    "        results = []\n",
    "        \n",
    "        for start_idx in range(0, total_rows, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, total_rows)\n",
    "            batch = processed_df.iloc[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"Processing batch {start_idx//batch_size + 1}/{(total_rows-1)//batch_size + 1} \"\n",
    "                  f\"({start_idx}-{end_idx})\")\n",
    "            \n",
    "            # Process batch\n",
    "            batch_results = process_merchant_data(batch, merchant_matcher)\n",
    "            results.append(batch_results)\n",
    "            \n",
    "            # Log progress\n",
    "            processed_so_far = end_idx\n",
    "            print(f\"Processed {processed_so_far}/{total_rows} records ({processed_so_far/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        # Combine results\n",
    "        results_df = pd.concat(results)\n",
    "        \n",
    "        # Add match categories\n",
    "        results_df = add_match_categories(results_df, thresholds)\n",
    "        \n",
    "        # Save results\n",
    "        if output_file.lower().endswith('.csv'):\n",
    "            results_df.to_csv(output_file, index=False)\n",
    "        else:\n",
    "            results_df.to_excel(output_file, index=False)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        return {\n",
    "            \"input_file\": input_file,\n",
    "            \"output_file\": output_file,\n",
    "            \"records_processed\": len(results_df),\n",
    "            \"processing_time\": processing_time,\n",
    "            \"records_per_second\": len(results_df) / processing_time\n",
    "        }\n",
    "\n",
    "print(\"Batch processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a401b25-b83e-4088-85cb-c415ddde67b3",
   "metadata": {},
   "source": [
    "# Scaling Merchant Name Matching for Large Datasets\r\n",
    "\r\n",
    "This code introduces advanced batch processing capabilities that enable the merchant name matching system to handle large-scale datasets efficiently. Let me walk you through what's happening in this sophisticated scaling infrastructure.\r\n",
    "\r\n",
    "## Two Approaches to Large-Scale Processing\r\n",
    "\r\n",
    "The code provides two primary pathways for processing large merchant datasets:\r\n",
    "\r\n",
    "1. **PySpark-based distributed processing** for truly massive datasets that can leverage a cluster\r\n",
    "2. **Chunked pandas processing** for large datasets on a single machine\r\n",
    "\r\n",
    "This dual-path approach is thoughtful engineering - it gives users flexibility based on their infrastructure and dataset size.\r\n",
    "\r\n",
    "## The PySpark Adaptation Function\r\n",
    "\r\n",
    "The first major function, `adapt_for_pyspark()`, transforms our merchant matching logic into components that can run in a distributed computing environment:\r\n",
    "\r\n",
    "```python\r\n",
    "def adapt_for_pyspark(spark=None):\r\n",
    "```\r\n",
    "\r\n",
    "This function creates a bridge between our Python-based merchant matcher and Apache Spark's distributed computing framework by:\r\n",
    "\r\n",
    "1. **Creating User-Defined Functions (UDFs)** that wrap our core processing logic:\r\n",
    "   ```python\r\n",
    "   preprocessing_udf = udf(\r\n",
    "       lambda DBAName, RawTransactionName, domain: merchant_matcher.preprocess_pair(DBAName, RawTransactionName, domain),\r\n",
    "       StructType([\r\n",
    "           StructField(\"DBAName_clean\", StringType(), True),\r\n",
    "           StructField(\"RawTransactionName_clean\", StringType(), True)\r\n",
    "       ])\r\n",
    "   )\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Defining a Spark DataFrame processing pipeline** that applies these UDFs in sequence:\r\n",
    "   ```python\r\n",
    "   def process_merchant_spark_df(df, DBAName_col=\"DBAName\", RawTransactionName_col=\"RawTransactionName\", \r\n",
    "                                domain_col=\"Merchant_Category\"):\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **Implementing batch processing specifically for Spark** to handle memory constraints:\r\n",
    "   ```python\r\n",
    "   def batch_process_merchant_data(df, batch_size=10000):\r\n",
    "   ```\r\n",
    "\r\n",
    "The function includes graceful error handling to detect if PySpark is available and to create a Spark session if one isn't provided. This makes the code more robust in various execution environments.\r\n",
    "\r\n",
    "## The Universal Batch Processing Function\r\n",
    "\r\n",
    "The second major function, `batch_process_file()`, provides a unified interface for processing large files regardless of the underlying technology:\r\n",
    "\r\n",
    "```python\r\n",
    "def batch_process_file(input_file, output_file, batch_size=10000, use_spark=False):\r\n",
    "```\r\n",
    "\r\n",
    "This function handles:\r\n",
    "\r\n",
    "1. **Input file format detection** (Excel or CSV):\r\n",
    "   ```python\r\n",
    "   is_excel = input_file.lower().endswith(('.xlsx', '.xls'))\r\n",
    "   is_csv = input_file.lower().endswith('.csv')\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Conditional processing path selection** based on the `use_spark` parameter:\r\n",
    "   ```python\r\n",
    "   if use_spark:\r\n",
    "       # PySpark processing path\r\n",
    "   else:\r\n",
    "       # Pandas processing path\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **Chunked reading for large files**, especially important for CSV files that might exceed memory:\r\n",
    "   ```python\r\n",
    "   chunks = []\r\n",
    "   chunk_size = min(batch_size, 100000)  # Default chunk size\r\n",
    "   \r\n",
    "   for chunk in pd.read_csv(input_file, chunksize=chunk_size):\r\n",
    "       chunks.append(chunk)\r\n",
    "   ```\r\n",
    "\r\n",
    "4. **Batched processing with progress reporting** to manage memory and provide visibility:\r\n",
    "   ```python\r\n",
    "   for start_idx in range(0, total_rows, batch_size):\r\n",
    "       # Process a batch\r\n",
    "       # Report progress\r\n",
    "   ```\r\n",
    "\r\n",
    "5. **Performance metrics collection** to help users understand processing efficiency:\r\n",
    "   ```python\r\n",
    "   return {\r\n",
    "       \"input_file\": input_file,\r\n",
    "       \"output_file\": output_file,\r\n",
    "       \"records_processed\": len(results_df),\r\n",
    "       \"processing_time\": processing_time,\r\n",
    "       \"records_per_second\": len(results_df) / processing_time\r\n",
    "   }\r\n",
    "   ```\r\n",
    "\r\n",
    "## Technical Sophistication in the Implementation\r\n",
    "\r\n",
    "The batch processing code demonstrates several advanced software engineering techniques:\r\n",
    "\r\n",
    "### 1. Adapting Non-Distributed Algorithms for Distributed Computing\r\n",
    "\r\n",
    "Converting Python functions to Spark UDFs requires careful attention to serialization, return types, and execution flow. The code handles this cleanly:\r\n",
    "\r\n",
    "```python\r\n",
    "weighted_score_udf = udf(\r\n",
    "    lambda DBAName, RawTransactionName, domain: float(merchant_matcher.compute_weighted_score(DBAName, RawTransactionName, domain)), \r\n",
    "    DoubleType()\r\n",
    ")\r\n",
    "```\r\n",
    "\r\n",
    "Notice how the function explicitly converts the result to float and declares a `DoubleType()` return type for Spark.\r\n",
    "\r\n",
    "### 2. Progressive Enhancement with Graceful Degradation\r\n",
    "\r\n",
    "The code attempts to use the most powerful approach (PySpark) but gracefully falls back to simpler methods if needed:\r\n",
    "\r\n",
    "```python\r\n",
    "except Exception as e:\r\n",
    "    print(f\"Error processing with PySpark: {e}\")\r\n",
    "    print(\"Falling back to pandas processing...\")\r\n",
    "    use_spark = False\r\n",
    "```\r\n",
    "\r\n",
    "This ensures the system can work across different environments without requiring code changes.\r\n",
    "\r\n",
    "### 3. Resource-Conscious Processing\r\n",
    "\r\n",
    "The code implements memory-efficient techniques throughout:\r\n",
    "\r\n",
    "- Batched processing to limit memory consumption\r\n",
    "- Chunked file reading for large CSV files\r\n",
    "- Progress reporting to monitor resource usage\r\n",
    "- Conditional code paths based on file types and sizes\r\n",
    "\r\n",
    "### 4. Pipeline Construction with Functional Components\r\n",
    "\r\n",
    "The PySpark adaptation assembles a series of data transformations into a coherent pipeline:\r\n",
    "\r\n",
    "```python\r\n",
    "# Apply preprocessing\r\n",
    "df = df.withColumn(\r\n",
    "    \"preprocessed\", \r\n",
    "    preprocessing_udf(col(DBAName_col), col(RawTransactionName_col), col(domain_col))\r\n",
    ")\r\n",
    "\r\n",
    "# Calculate scores\r\n",
    "df = df.withColumn(\r\n",
    "    \"Weighted_Score\", \r\n",
    "    weighted_score_udf(col(DBAName_col), col(RawTransactionName_col), col(domain_col))\r\n",
    ")\r\n",
    "```\r\n",
    "\r\n",
    "This functional approach allows each transformation to be tested, debugged, and potentially optimized independently.\r\n",
    "\r\n",
    "## Real-World Implications of Batch Processing\r\n",
    "\r\n",
    "This batch processing capability transforms the merchant matching system from a tool suitable for moderate-sized datasets to an enterprise-grade solution capable of processing millions of records. This has significant implications:\r\n",
    "\r\n",
    "### 1. Enterprise Data Integration\r\n",
    "\r\n",
    "Financial institutions can now process their entire transaction history to standardize merchant names across years of data.\r\n",
    "\r\n",
    "### 2. Real-time System Integration\r\n",
    "\r\n",
    "The batch processing functions can be integrated into data pipelines that need to process merchant names continuously as new transactions arrive.\r\n",
    "\r\n",
    "### 3. Cross-Platform Deployment\r\n",
    "\r\n",
    "The dual-path approach (Spark and pandas) means the system can run in environments ranging from a data scientist's laptop to a production Hadoop cluster.\r\n",
    "\r\n",
    "### 4. Scalable Performance\r\n",
    "\r\n",
    "By processing data in batches with appropriate sizes, the system can efficiently handle datasets of arbitrary size, limited only by storage rather than memory.\r\n",
    "\r\n",
    "### 5. Progress Visibility\r\n",
    "\r\n",
    "The detailed progress reporting enables operators to monitor long-running jobs and estimate completion times, which is critical for production operations.\r\n",
    "\r\n",
    "## Conceptual Architecture\r\n",
    "\r\n",
    "Conceptually, the batch processing code creates a layered architecture:\r\n",
    "\r\n",
    "1. **Core Algorithm Layer**: The existing merchant matching algorithms\r\n",
    "2. **Adaptation Layer**: UDFs and wrappers that make these algorithms usable in different contexts\r\n",
    "3. **Processing Layer**: Batch processing logic that handles resource management\r\n",
    "4. **I/O Layer**: File reading and writing with format-specific optimizations\r\n",
    "\r\n",
    "This separation of concerns makes the system maintainable and adaptable as requirements evolve.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This batch processing code represents the critical infrastructure that allows the sophisticated merchant name matching algorithms to scale to enterprise-level datasets. By providing both distributed and single-machine processing paths, implementing memory-efficient techniques, and maintaining a unified interface, it creates a bridge between advanced matching algorithms and real-world data volumes.\r\n",
    "\r\n",
    "The combination of PySpark adaptation for truly massive datasets and chunked pandas processing for more moderate workloads gives users flexibility based on their specific needs, making the merchant matching system practical for a wide range of applications from individual analysis to enterprise-wide data standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0ef6c46b-20ed-4938-8d1c-4e9605a71a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and testing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Evaluation and Testing Functions\n",
    "\n",
    "def evaluate_merchant_matcher(test_data_path=None, gold_standard_column='Expected_Match'):\n",
    "    \"\"\"\n",
    "    Evaluate the merchant matcher against a gold standard dataset\n",
    "    \n",
    "    Args:\n",
    "        test_data_path: Path to test data file (with gold standard annotations)\n",
    "        gold_standard_column: Column name for the gold standard match status\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"Evaluating merchant matcher performance...\")\n",
    "    \n",
    "    # If no test data provided, use built-in test cases\n",
    "    if test_data_path is None:\n",
    "        print(\"No test data provided, creating synthetic test data...\")\n",
    "        \n",
    "        # Create synthetic test data with known expected outcomes\n",
    "        test_data = {\n",
    "            'DBAName': [\n",
    "                # True matches - should get high scores\n",
    "                'BoA', 'JPMC', 'WF', 'MCD', 'SBUX', 'TGT', 'MSFT', 'AMZN',\n",
    "                'WMT', 'HD', 'TM', 'GM', 'GS', 'MS', 'BBY',\n",
    "                \n",
    "                # Partial matches - could go either way\n",
    "                'BofA', 'McDon', 'Micky Ds', 'Wmart', 'Tgt Stores',\n",
    "                'Msft Corp', 'Amaz', 'JPM Co', 'Home Dep',\n",
    "                \n",
    "                # False matches - should get low scores\n",
    "                'BOA', 'JPM', 'WF Bank', 'MCD Restaurant', 'SB Coffee',\n",
    "                'MSFT Solutions', 'GMC Trucks', 'TGT Brands'\n",
    "            ],\n",
    "            'RawTransactionName': [\n",
    "                # Matches for true matches\n",
    "                'Bank of America', 'JPMorgan Chase', 'Wells Fargo', 'McDonalds',\n",
    "                'Starbucks', 'Target', 'Microsoft', 'Amazon',\n",
    "                'Walmart', 'Home Depot', 'Toyota Motors', 'General Motors',\n",
    "                'Goldman Sachs', 'Morgan Stanley', 'Best Buy',\n",
    "                \n",
    "                # Matches for partial matches\n",
    "                'Bank of America Corp', 'McDonalds Corporation', 'McDonalds Restaurants',\n",
    "                'Walmart Stores', 'Target Corporation', 'Microsoft Inc',\n",
    "                'Amazon.com Inc', 'JP Morgan', 'The Home Depot',\n",
    "                \n",
    "                # Non-matches\n",
    "                'Bank of Australia', 'Johnson & Johnson', 'Western Family',\n",
    "                'Michaels Craft Store', 'Seattle Bread Company',\n",
    "                'Micro Soft Tech', 'General Mills', 'Tarjay Brands'\n",
    "            ],\n",
    "            'Merchant_Category': [\n",
    "                # Categories for true matches\n",
    "                'Banking', 'Banking', 'Banking', 'Restaurant', 'Restaurant',\n",
    "                'Retail', 'Technology', 'Technology', 'Retail', 'Retail',\n",
    "                'Automotive', 'Automotive', 'Banking', 'Banking', 'Retail',\n",
    "                \n",
    "                # Categories for partial matches\n",
    "                'Banking', 'Restaurant', 'Restaurant', 'Retail', 'Retail',\n",
    "                'Technology', 'Technology', 'Banking', 'Retail',\n",
    "                \n",
    "                # Categories for false matches\n",
    "                'Banking', 'Healthcare', 'Retail', 'Restaurant', 'Restaurant',\n",
    "                'Technology', 'Food', 'Retail'\n",
    "            ],\n",
    "            'Expected_Match': [\n",
    "                # Expected outcomes for true matches\n",
    "                True, True, True, True, True, True, True, True, \n",
    "                True, True, True, True, True, True, True,\n",
    "                \n",
    "                # Expected outcomes for partial matches\n",
    "                True, True, True, True, True, True, True, True, True,\n",
    "                \n",
    "                # Expected outcomes for false matches\n",
    "                False, False, False, False, False, False, False, False\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        test_df = pd.DataFrame(test_data)\n",
    "        print(f\"Created synthetic test dataset with {len(test_df)} entries\")\n",
    "    else:\n",
    "        # Load test data from file\n",
    "        try:\n",
    "            if test_data_path.lower().endswith('.csv'):\n",
    "                test_df = pd.read_csv(test_data_path)\n",
    "            else:\n",
    "                test_df = pd.read_excel(test_data_path)\n",
    "            \n",
    "            print(f\"Loaded test data from {test_data_path} with {len(test_df)} entries\")\n",
    "            \n",
    "            # Verify that gold standard column exists\n",
    "            if gold_standard_column not in test_df.columns:\n",
    "                raise ValueError(f\"Gold standard column '{gold_standard_column}' not found in test data\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading test data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Preprocess test data\n",
    "    processed_df = preprocess_merchant_data(test_df)\n",
    "    \n",
    "    # Compute scores\n",
    "    results_df = process_merchant_data(processed_df, merchant_matcher)\n",
    "    \n",
    "    # Add binary prediction based on threshold\n",
    "    match_threshold = 0.75  # This is the \"Probable Match\" threshold\n",
    "    results_df['Predicted_Match'] = results_df['Enhanced_Score'] >= match_threshold\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    try:\n",
    "        from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "        \n",
    "        # Convert expected values to boolean\n",
    "        results_df['Expected_Match'] = results_df[gold_standard_column].astype(bool)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            results_df['Expected_Match'], \n",
    "            results_df['Predicted_Match'],\n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        accuracy = accuracy_score(results_df['Expected_Match'], results_df['Predicted_Match'])\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(results_df['Expected_Match'], results_df['Predicted_Match']).ravel()\n",
    "        \n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"True Positives: {tp}\")\n",
    "        print(f\"True Negatives: {tn}\")\n",
    "        print(f\"False Positives: {fp}\")\n",
    "        print(f\"False Negatives: {fn}\")\n",
    "        \n",
    "        # Analyze errors\n",
    "        print(\"\\nAnalyzing errors...\")\n",
    "        \n",
    "        # False positives\n",
    "        fp_df = results_df[(results_df['Predicted_Match'] == True) & (results_df['Expected_Match'] == False)]\n",
    "        if len(fp_df) > 0:\n",
    "            print(f\"\\nFalse Positives ({len(fp_df)}):\")\n",
    "            for _, row in fp_df.iterrows():\n",
    "                print(f\"  {row['DBAName']} <-> {row['RawTransactionName']} (Score: {row['Enhanced_Score']:.4f})\")\n",
    "        \n",
    "        # False negatives\n",
    "        fn_df = results_df[(results_df['Predicted_Match'] == False) & (results_df['Expected_Match'] == True)]\n",
    "        if len(fn_df) > 0:\n",
    "            print(f\"\\nFalse Negatives ({len(fn_df)}):\")\n",
    "            for _, row in fn_df.iterrows():\n",
    "                print(f\"  {row['DBAName']} <-> {row['RawTransactionName']} (Score: {row['Enhanced_Score']:.4f})\")\n",
    "        \n",
    "        # Return metrics\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'true_positives': tp,\n",
    "            'true_negatives': tn,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'results_df': results_df\n",
    "        }\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"Warning: scikit-learn not available. Computing basic metrics...\")\n",
    "        \n",
    "        # Convert expected values to boolean\n",
    "        results_df['Expected_Match'] = results_df[gold_standard_column].astype(bool)\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        tp = sum((results_df['Predicted_Match'] == True) & (results_df['Expected_Match'] == True))\n",
    "        tn = sum((results_df['Predicted_Match'] == False) & (results_df['Expected_Match'] == False))\n",
    "        fp = sum((results_df['Predicted_Match'] == True) & (results_df['Expected_Match'] == False))\n",
    "        fn = sum((results_df['Predicted_Match'] == False) & (results_df['Expected_Match'] == True))\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(\"\\nBasic Evaluation Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'true_positives': tp,\n",
    "            'true_negatives': tn,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'results_df': results_df\n",
    "        }\n",
    "\n",
    "def find_optimal_threshold(test_data_path=None, gold_standard_column='Expected_Match'):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for classifying merchant matches\n",
    "    \n",
    "    Args:\n",
    "        test_data_path: Path to test data file\n",
    "        gold_standard_column: Column name for the gold standard match status\n",
    "        \n",
    "    Returns:\n",
    "        float: Optimal threshold value\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal threshold for merchant matching...\")\n",
    "    \n",
    "    # Get test data\n",
    "    if test_data_path is None:\n",
    "        # Call evaluate_merchant_matcher which will create synthetic data\n",
    "        eval_results = evaluate_merchant_matcher(None, gold_standard_column)\n",
    "        results_df = eval_results['results_df']\n",
    "    else:\n",
    "        # Load test data from file\n",
    "        try:\n",
    "            if test_data_path.lower().endswith('.csv'):\n",
    "                test_df = pd.read_csv(test_data_path)\n",
    "            else:\n",
    "                test_df = pd.read_excel(test_data_path)\n",
    "            \n",
    "            print(f\"Loaded test data from {test_data_path} with {len(test_df)} entries\")\n",
    "            \n",
    "            # Verify that gold standard column exists\n",
    "            if gold_standard_column not in test_df.columns:\n",
    "                raise ValueError(f\"Gold standard column '{gold_standard_column}' not found in test data\")\n",
    "                \n",
    "            # Preprocess test data\n",
    "            processed_df = preprocess_merchant_data(test_df)\n",
    "            \n",
    "            # Compute scores\n",
    "            results_df = process_merchant_data(processed_df, merchant_matcher)\n",
    "            \n",
    "            # Convert expected values to boolean\n",
    "            results_df['Expected_Match'] = results_df[gold_standard_column].astype(bool)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing test data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Test different threshold values\n",
    "    try:\n",
    "        from sklearn.metrics import precision_recall_curve, f1_score\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        thresholds = np.linspace(0.1, 1.0, 37)  # Test thresholds from 0.1 to 1.0\n",
    "        f1_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        \n",
    "        # Calculate F1 score for each threshold\n",
    "        for threshold in thresholds:\n",
    "            predictions = results_df['Enhanced_Score'] >= threshold\n",
    "            precision = sum(predictions & results_df['Expected_Match']) / sum(predictions) if sum(predictions) > 0 else 0\n",
    "            recall = sum(predictions & results_df['Expected_Match']) / sum(results_df['Expected_Match']) if sum(results_df['Expected_Match']) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        # Find optimal threshold (maximizing F1 score)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_f1 = f1_scores[optimal_idx]\n",
    "        \n",
    "        print(f\"\\nOptimal threshold: {optimal_threshold:.4f} (F1 score: {optimal_f1:.4f})\")\n",
    "        print(f\"At this threshold:\")\n",
    "        print(f\"  Precision: {precision_scores[optimal_idx]:.4f}\")\n",
    "        print(f\"  Recall: {recall_scores[optimal_idx]:.4f}\")\n",
    "        \n",
    "        # Plot the results\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(thresholds, precision_scores, label='Precision')\n",
    "            plt.plot(thresholds, recall_scores, label='Recall')\n",
    "            plt.plot(thresholds, f1_scores, label='F1 Score')\n",
    "            plt.axvline(x=optimal_threshold, color='k', linestyle='--', label=f'Optimal Threshold = {optimal_threshold:.2f}')\n",
    "            plt.xlabel('Threshold')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title('Precision, Recall, and F1 Score vs. Threshold')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error creating plot: {e}\")\n",
    "        \n",
    "        # Also try precision-recall curve\n",
    "        try:\n",
    "            # Get precision-recall curve\n",
    "            precision, recall, pr_thresholds = precision_recall_curve(\n",
    "                results_df['Expected_Match'], \n",
    "                results_df['Enhanced_Score']\n",
    "            )\n",
    "            \n",
    "            # Plot precision-recall curve\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(recall, precision, marker='.', label='Precision-Recall curve')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Precision-Recall Curve')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error creating precision-recall plot: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'optimal_f1': optimal_f1,\n",
    "            'thresholds': thresholds,\n",
    "            'precision_scores': precision_scores,\n",
    "            'recall_scores': recall_scores,\n",
    "            'f1_scores': f1_scores\n",
    "        }\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"Warning: scikit-learn or matplotlib not available. Computing basic optimization...\")\n",
    "        \n",
    "        # Test different threshold values without plotting\n",
    "        thresholds = np.linspace(0.1, 1.0, 19)  # Test thresholds from 0.1 to 1.0\n",
    "        f1_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        \n",
    "        # Calculate F1 score for each threshold\n",
    "        for threshold in thresholds:\n",
    "            predictions = results_df['Enhanced_Score'] >= threshold\n",
    "            precision = sum(predictions & results_df['Expected_Match']) / sum(predictions) if sum(predictions) > 0 else 0\n",
    "            recall = sum(predictions & results_df['Expected_Match']) / sum(results_df['Expected_Match']) if sum(results_df['Expected_Match']) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        # Find optimal threshold (maximizing F1 score)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_f1 = f1_scores[optimal_idx]\n",
    "        \n",
    "        print(f\"\\nOptimal threshold: {optimal_threshold:.4f} (F1 score: {optimal_f1:.4f})\")\n",
    "        print(f\"At this threshold:\")\n",
    "        print(f\"  Precision: {precision_scores[optimal_idx]:.4f}\")\n",
    "        print(f\"  Recall: {recall_scores[optimal_idx]:.4f}\")\n",
    "        \n",
    "        # Print table of results\n",
    "        print(\"\\nThreshold\\tPrecision\\tRecall\\t\\tF1 Score\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, t in enumerate(thresholds):\n",
    "            print(f\"{t:.2f}\\t\\t{precision_scores[i]:.4f}\\t\\t{recall_scores[i]:.4f}\\t\\t{f1_scores[i]:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'optimal_f1': optimal_f1,\n",
    "            'thresholds': thresholds,\n",
    "            'precision_scores': precision_scores,\n",
    "            'recall_scores': recall_scores,\n",
    "            'f1_scores': f1_scores\n",
    "        }\n",
    "\n",
    "def compare_algorithms(test_data_path=None, gold_standard_column='Expected_Match'):\n",
    "    \"\"\"\n",
    "    Compare different matching algorithms on the same test data\n",
    "    \n",
    "    Args:\n",
    "        test_data_path: Path to test data file\n",
    "        gold_standard_column: Column name for the gold standard match status\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Comparison results\n",
    "    \"\"\"\n",
    "    print(\"Comparing matching algorithms...\")\n",
    "    \n",
    "    # Get test data\n",
    "    if test_data_path is None:\n",
    "        # Create synthetic test data\n",
    "        eval_results = evaluate_merchant_matcher(None, gold_standard_column)\n",
    "        test_df = eval_results['results_df'][['DBAName', 'RawTransactionName', 'Merchant_Category', 'Expected_Match']]\n",
    "    else:\n",
    "        # Load test data from file\n",
    "        try:\n",
    "            if test_data_path.lower().endswith('.csv'):\n",
    "                test_df = pd.read_csv(test_data_path)\n",
    "            else:\n",
    "                test_df = pd.read_excel(test_data_path)\n",
    "                \n",
    "            # Verify that gold standard column exists\n",
    "            if gold_standard_column not in test_df.columns:\n",
    "                raise ValueError(f\"Gold standard column '{gold_standard_column}' not found in test data\")\n",
    "                \n",
    "            # Convert expected values to boolean\n",
    "            test_df['Expected_Match'] = test_df[gold_standard_column].astype(bool)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading test data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Preprocess test data\n",
    "    processed_df = preprocess_merchant_data(test_df)\n",
    "    \n",
    "    # Define algorithms to compare\n",
    "    algorithms = {\n",
    "        'Jaro-Winkler': lambda a, f, d: merchant_matcher.jaro_winkler_similarity(a, f, d),\n",
    "        'Damerau-Levenshtein': lambda a, f, d: merchant_matcher.damerau_levenshtein_similarity(a, f, d),\n",
    "        'TF-IDF Cosine': lambda a, f, d: merchant_matcher.tfidf_cosine_similarity(a, f, d),\n",
    "        'Jaccard Bigram': lambda a, f, d: merchant_matcher.jaccard_bigram_similarity(a, f, d),\n",
    "        'Soundex': lambda a, f, d: merchant_matcher.soundex_similarity(a, f, d),\n",
    "        'Token Sort Ratio': lambda a, f, d: merchant_matcher.token_sort_ratio_similarity(a, f, d),\n",
    "        'DBAName Formation': lambda a, f, d: merchant_matcher.enhanced_DBAName_formation_score(a, f, d),\n",
    "        'BERT Similarity': lambda a, f, d: merchant_matcher.bert_similarity(a, f, d) if hasattr(merchant_matcher, 'bert_similarity') else 0,\n",
    "        'Weighted Score': lambda a, f, d: merchant_matcher.compute_weighted_score(a, f, d),\n",
    "        'Enhanced Score': lambda a, f, d: merchant_matcher.compute_enhanced_score(a, f, d)\n",
    "    }\n",
    "    \n",
    "    # Evaluate each algorithm\n",
    "    algorithm_results = {}\n",
    "    algorithm_metrics = {}\n",
    "    \n",
    "    for name, algorithm in algorithms.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        scores = []\n",
    "        \n",
    "        # Calculate scores for each pair\n",
    "        for _, row in processed_df.iterrows():\n",
    "            DBAName = row['DBAName']\n",
    "            RawTransactionName = row['RawTransactionName']\n",
    "            domain = row['Merchant_Category'] if 'Merchant_Category' in row else None\n",
    "            score = algorithm(DBAName, RawTransactionName, domain)\n",
    "            scores.append(score)\n",
    "        \n",
    "        # Add scores to results\n",
    "        processed_df[f'{name}_Score'] = scores\n",
    "        \n",
    "        # Find optimal threshold for this algorithm\n",
    "        thresholds = np.linspace(0.1, 1.0, 19)\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            predictions = np.array(scores) >= threshold\n",
    "            true_labels = processed_df['Expected_Match'].values\n",
    "            \n",
    "            # Calculate metrics\n",
    "            tp = sum(predictions & true_labels)\n",
    "            fp = sum(predictions & ~true_labels)\n",
    "            fn = sum(~predictions & true_labels)\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        # Calculate final metrics at optimal threshold\n",
    "        predictions = np.array(scores) >= best_threshold\n",
    "        true_labels = processed_df['Expected_Match'].values\n",
    "        \n",
    "        tp = sum(predictions & true_labels)\n",
    "        fp = sum(predictions & ~true_labels)\n",
    "        fn = sum(~predictions & true_labels)\n",
    "        tn = sum(~predictions & ~true_labels)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / len(true_labels) if len(true_labels) > 0 else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        algorithm_metrics[name] = {\n",
    "            'threshold': best_threshold,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'true_negatives': tn\n",
    "        }\n",
    "        \n",
    "        # Store predictions\n",
    "        algorithm_results[name] = predictions\n",
    "    \n",
    "    # Create comparison table\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Algorithm': list(algorithm_metrics.keys()),\n",
    "        'Threshold': [m['threshold'] for m in algorithm_metrics.values()],\n",
    "        'Accuracy': [m['accuracy'] for m in algorithm_metrics.values()],\n",
    "        'Precision': [m['precision'] for m in algorithm_metrics.values()],\n",
    "        'Recall': [m['recall'] for m in algorithm_metrics.values()],\n",
    "        'F1 Score': [m['f1_score'] for m in algorithm_metrics.values()]\n",
    "    })\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    metrics_df = metrics_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nAlgorithm Comparison Results:\")\n",
    "    print(metrics_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Plot comparison if matplotlib is available\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Bar chart for F1 scores\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(metrics_df['Algorithm'], metrics_df['F1 Score'])\n",
    "        plt.xlabel('Algorithm')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('Algorithm F1 Score Comparison')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Precision-Recall comparison for top 5 algorithms\n",
    "        top_algorithms = metrics_df['Algorithm'].head(5).tolist()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for algorithm in top_algorithms:\n",
    "            precision = algorithm_metrics[algorithm]['precision']\n",
    "            recall = algorithm_metrics[algorithm]['recall']\n",
    "            plt.scatter(recall, precision, label=f\"{algorithm} (F1={algorithm_metrics[algorithm]['f1_score']:.4f})\", s=100)\n",
    "        \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision vs Recall for Top Algorithms')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available for visualization.\")\n",
    "    \n",
    "    return {\n",
    "        'metrics_df': metrics_df,\n",
    "        'algorithm_metrics': algorithm_metrics,\n",
    "        'algorithm_results': algorithm_results,\n",
    "        'processed_df': processed_df\n",
    "    }\n",
    "\n",
    "print(\"Evaluation and testing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31310450-cba2-4833-af91-c2a547e6bf05",
   "metadata": {},
   "source": [
    "# Understanding the Evaluation Framework for Merchant Name Matching\r\n",
    "\r\n",
    "This code represents the evaluation and testing infrastructure that completes the merchant name matching system. It provides the critical scientific framework for measuring how well the matching algorithms perform and optimizing their parameters. Let me walk you through the significance and functionality of these evaluation components.\r\n",
    "\r\n",
    "## The Purpose of Rigorous Evaluation\r\n",
    "\r\n",
    "In any machine learning or data matching system, rigorous evaluation is essential for:\r\n",
    "\r\n",
    "1. Demonstrating the system's effectiveness on real-world data\r\n",
    "2. Understanding its strengths and limitations\r\n",
    "3. Identifying areas for improvement\r\n",
    "4. Fine-tuning parameters to maximize performance\r\n",
    "5. Comparing different algorithms to determine which approach works best\r\n",
    "\r\n",
    "This code addresses all these needs through three sophisticated evaluation functions.\r\n",
    "\r\n",
    "## The Three Evaluation Functions\r\n",
    "\r\n",
    "### 1. The Core Evaluation Function\r\n",
    "\r\n",
    "The first function, `evaluate_merchant_matcher()`, provides comprehensive performance assessment:\r\n",
    "\r\n",
    "```python\r\n",
    "def evaluate_merchant_matcher(test_data_path=None, gold_standard_column='Expected_Match'):\r\n",
    "```\r\n",
    "\r\n",
    "This function performs several key operations:\r\n",
    "\r\n",
    "First, it handles test data intelligently. If no test data is provided, it creates synthetic test data with known outcomes, including:\r\n",
    "- True matches (that should receive high scores)\r\n",
    "- Partial matches (borderline cases)\r\n",
    "- False matches (that should receive low scores)\r\n",
    "\r\n",
    "When working with provided test data, it validates the presence of a \"gold standard\" column that contains the ground truth about whether each merchant name pair should match.\r\n",
    "\r\n",
    "The function then processes the data through the matching system and calculates standard evaluation metrics:\r\n",
    "- Accuracy: The proportion of all predictions (both matches and non-matches) that are correct\r\n",
    "- Precision: The proportion of predicted matches that are actually matches\r\n",
    "- Recall: The proportion of actual matches that were correctly identified\r\n",
    "- F1 Score: The harmonic mean of precision and recall, providing a balanced measure\r\n",
    "\r\n",
    "It also performs detailed error analysis by identifying:\r\n",
    "- False positives: Pairs that were incorrectly predicted as matches\r\n",
    "- False negatives: Actual matches that were missed\r\n",
    "\r\n",
    "What makes this evaluation particularly valuable is that it provides both quantitative metrics for objective assessment and qualitative examples of errors for deeper understanding.\r\n",
    "\r\n",
    "### 2. The Threshold Optimization Function\r\n",
    "\r\n",
    "The second function, `find_optimal_threshold()`, addresses a critical configuration challenge:\r\n",
    "\r\n",
    "```python\r\n",
    "def find_optimal_threshold(test_data_path=None, gold_standard_column='Expected_Match'):\r\n",
    "```\r\n",
    "\r\n",
    "This function systematically tests different threshold values to determine the optimal cutoff point for classifying merchant pairs as matches. It:\r\n",
    "\r\n",
    "1. Tests a range of threshold values from 0.1 to 1.0\r\n",
    "2. Calculates precision, recall, and F1 score for each threshold\r\n",
    "3. Identifies the threshold that maximizes the F1 score (balancing precision and recall)\r\n",
    "4. Visualizes the relationship between threshold values and performance metrics\r\n",
    "\r\n",
    "The function includes sophisticated visualization capabilities when matplotlib is available, showing:\r\n",
    "- A plot of precision, recall, and F1 score across different thresholds\r\n",
    "- A precision-recall curve to visualize the tradeoff between these metrics\r\n",
    "\r\n",
    "This threshold optimization is crucial because different applications may have different requirements for precision versus recall. For example:\r\n",
    "- Fraud detection might prioritize recall (catching all potential matches)\r\n",
    "- Customer data consolidation might prioritize precision (avoiding false merges)\r\n",
    "\r\n",
    "By exposing this tradeoff explicitly, the function enables informed decision-making about how to configure the system for specific business needs.\r\n",
    "\r\n",
    "### 3. The Algorithm Comparison Function\r\n",
    "\r\n",
    "The third function, `compare_algorithms()`, provides deep insights into which matching techniques work best:\r\n",
    "\r\n",
    "```python\r\n",
    "def compare_algorithms(test_data_path=None, gold_standard_column='Expected_Match'):\r\n",
    "```\r\n",
    "\r\n",
    "This function:\r\n",
    "1. Evaluates multiple similarity algorithms on the same test data\r\n",
    "2. Optimizes the threshold for each algorithm independently\r\n",
    "3. Calculates performance metrics for each algorithm at its optimal threshold\r\n",
    "4. Creates a comparison table sorted by F1 score\r\n",
    "5. Visualizes the results with bar charts and scatter plots\r\n",
    "\r\n",
    "The algorithms compared include:\r\n",
    "- Traditional string similarity measures (Jaro-Winkler, Damerau-Levenshtein)\r\n",
    "- Token-based methods (Token Sort Ratio, TF-IDF Cosine)\r\n",
    "- Phonetic methods (Soundex)\r\n",
    "- Specialized approaches (DBAName Formation)\r\n",
    "- Semantic methods (BERT Similarity)\r\n",
    "- Combined approaches (Weighted Score, Enhanced Score)\r\n",
    "\r\n",
    "This comparison provides crucial insights into which algorithms contribute most to accurate matching for different types of merchant names, guiding future development efforts.\r\n",
    "\r\n",
    "## Technical Sophistication in the Implementation\r\n",
    "\r\n",
    "The evaluation code demonstrates several advanced software engineering and data science principles:\r\n",
    "\r\n",
    "### 1. Synthetic Data Generation for Testing\r\n",
    "\r\n",
    "The code includes thoughtful synthetic data generation that covers diverse test cases:\r\n",
    "\r\n",
    "```python\r\n",
    "# Create synthetic test data with known expected outcomes\r\n",
    "test_data = {\r\n",
    "    'DBAName': [\r\n",
    "        # True matches - should get high scores\r\n",
    "        'BoA', 'JPMC', 'WF', 'MCD', 'SBUX', 'TGT', 'MSFT', 'AMZN',\r\n",
    "        # ...\r\n",
    "    ],\r\n",
    "    # ...\r\n",
    "}\r\n",
    "```\r\n",
    "\r\n",
    "This ensures that the evaluation can be performed even without access to production data, enabling development and testing in isolated environments.\r\n",
    "\r\n",
    "### 2. Graceful Degradation for Dependencies\r\n",
    "\r\n",
    "The code handles the potential absence of scientific computing libraries gracefully:\r\n",
    "\r\n",
    "```python\r\n",
    "try:\r\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\r\n",
    "    # Use scikit-learn for evaluation\r\n",
    "except ImportError:\r\n",
    "    print(\"Warning: scikit-learn not available. Computing basic metrics...\")\r\n",
    "    # Implement manual calculations\r\n",
    "```\r\n",
    "\r\n",
    "This ensures the evaluation can run in environments with different dependency availability, from fully-equipped data science workstations to production servers with minimal installations.\r\n",
    "\r\n",
    "### 3. Rich Visualization with Fallbacks\r\n",
    "\r\n",
    "The code includes sophisticated visualization when possible, with text-based alternatives:\r\n",
    "\r\n",
    "```python\r\n",
    "try:\r\n",
    "    # Create visual plots with matplotlib\r\n",
    "except Exception as e:\r\n",
    "    print(f\"Warning: Error creating plot: {e}\")\r\n",
    "    # Print table of results instead\r\n",
    "```\r\n",
    "\r\n",
    "This flexibility ensures that users can understand the results regardless of their environment's capabilities.\r\n",
    "\r\n",
    "### 4. Comprehensive Error Analysis\r\n",
    "\r\n",
    "The evaluation doesn't just provide aggregate metrics but delves into specific error cases:\r\n",
    "\r\n",
    "```python\r\n",
    "# False positives\r\n",
    "fp_df = results_df[(results_df['Predicted_Match'] == True) & (results_df['Expected_Match'] == False)]\r\n",
    "if len(fp_df) > 0:\r\n",
    "    print(f\"\\nFalse Positives ({len(fp_df)}):\")\r\n",
    "    for _, row in fp_df.iterrows():\r\n",
    "        print(f\"  {row['DBAName']} <-> {row['RawTransactionName']} (Score: {row['Enhanced_Score']:.4f})\")\r\n",
    "```\r\n",
    "\r\n",
    "This detailed error analysis helps identify patterns in mistakes and guides targeted improvements.\r\n",
    "\r\n",
    "## Real-World Applications of Evaluation\r\n",
    "\r\n",
    "This evaluation framework enables several critical real-world applications:\r\n",
    "\r\n",
    "### 1. System Validation and Verification\r\n",
    "\r\n",
    "Before deploying the merchant matching system in production, stakeholders need evidence that it performs adequately. The evaluation metrics provide this evidence in a standardized, interpretable format.\r\n",
    "\r\n",
    "### 2. Parameter Optimization\r\n",
    "\r\n",
    "The threshold optimization function allows the system to be tuned for specific business requirements, maximizing the metrics that matter most for a particular use case.\r\n",
    "\r\n",
    "### 3. Algorithm Selection and Weighting\r\n",
    "\r\n",
    "The algorithm comparison results inform decisions about which algorithms to include in the weighted combination and how much weight to assign to each, potentially leading to domain-specific configurations.\r\n",
    "\r\n",
    "### 4. Continuous Improvement\r\n",
    "\r\n",
    "As the system evolves, the evaluation framework provides a consistent way to measure whether changes actually improve performance, preventing regressions and guiding development efforts.\r\n",
    "\r\n",
    "### 5. Business Justification\r\n",
    "\r\n",
    "The quantitative metrics provided by the evaluation framework help justify the business value of the merchant matching system by demonstrating its accuracy and reliability.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This evaluation code completes our merchant name matching system by providing the scientific foundation needed to measure its performance, optimize its parameters, and compare different approaches. It transforms the system from a collection of algorithms into a validated, tunable solution that can be confidently deployed for real-world merchant name matching challenges.\r\n",
    "\r\n",
    "The combination of comprehensive metrics, error analysis, threshold optimization, and algorithm comparison creates a powerful toolkit for understanding and improving the system's performance across different domains and use cases. This evaluation framework is essential for building trust in the system and ensuring it meets the specific needs of its users.ool whose strengths and limitations can be clearly communicated to stakeholders. This level of insight is essential for building trust in the system and guiding its ongoing improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eae95210-56e8-408f-9ad1-63a47bedac94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation and error analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Cross-Validation and Error Analysis Functions\n",
    "\n",
    "def visualize_error_cases(results_df, num_cases=5):\n",
    "    \"\"\"\n",
    "    Visualize and analyze error cases to understand why they're misclassified\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with matching results\n",
    "        num_cases: Number of error cases to visualize\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis of error cases\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing top {num_cases} error cases...\")\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['DBAName', 'RawTransactionName', 'Enhanced_Score', 'Expected_Match', 'Predicted_Match']\n",
    "    if not all(col in results_df.columns for col in required_cols):\n",
    "        print(\"Error: Required columns missing from results DataFrame\")\n",
    "        return None\n",
    "    \n",
    "    # Find false positives (predicted match but actually not a match)\n",
    "    fp_df = results_df[(results_df['Predicted_Match'] == True) & (results_df['Expected_Match'] == False)]\n",
    "    fp_df = fp_df.sort_values('Enhanced_Score', ascending=False).head(num_cases)\n",
    "    \n",
    "    # Find false negatives (predicted not a match but actually a match)\n",
    "    fn_df = results_df[(results_df['Predicted_Match'] == False) & (results_df['Expected_Match'] == True)]\n",
    "    fn_df = fn_df.sort_values('Enhanced_Score', ascending=True).head(num_cases)\n",
    "    \n",
    "    # Analyze false positives\n",
    "    print(\"\\nFalse Positive Analysis (Incorrectly Predicted as Match):\")\n",
    "    fp_analysis = {}\n",
    "    \n",
    "    for i, (_, row) in enumerate(fp_df.iterrows()):\n",
    "        DBAName = row['DBAName']\n",
    "        RawTransactionName = row['RawTransactionName']\n",
    "        score = row['Enhanced_Score']\n",
    "        \n",
    "        print(f\"\\nError Case {i+1}: {DBAName} <-> {RawTransactionName} (Score: {score:.4f})\")\n",
    "        \n",
    "        # Get all similarity scores\n",
    "        scores = merchant_matcher.get_all_similarity_scores(DBAName, RawTransactionName)\n",
    "        \n",
    "        # Display top scores\n",
    "        top_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"Top Individual Algorithm Scores:\")\n",
    "        for algo, algo_score in top_scores:\n",
    "            print(f\"  {algo.replace('_', ' ').title()}: {algo_score:.4f}\")\n",
    "        \n",
    "        # Check for patterns\n",
    "        patterns = merchant_matcher.detect_complex_business_patterns(DBAName, RawTransactionName)\n",
    "        if patterns:\n",
    "            print(\"Detected Business Patterns:\")\n",
    "            for pattern, pattern_score in patterns.items():\n",
    "                print(f\"  {pattern.replace('_', ' ').title()}: {pattern_score:.4f}\")\n",
    "        \n",
    "        # Explain issue\n",
    "        print(\"Possible reason for misclassification:\")\n",
    "        if any(pattern in ['inverted_agency_structure', 'bank_name_inversion'] for pattern in patterns):\n",
    "            print(\"  Structure pattern detection may be too aggressive\")\n",
    "        \n",
    "        if any(score > 0.9 for algo, score in scores.items() if 'bert' in algo):\n",
    "            print(\"  BERT semantic similarity may be overvaluing similar contexts\")\n",
    "        \n",
    "        if any(score > 0.9 for algo, score in scores.items() if 'DBAName' in algo):\n",
    "            print(\"  DBAName formation detection may be too lenient\")\n",
    "        \n",
    "        # Store analysis\n",
    "        fp_analysis[i] = {\n",
    "            'DBAName': DBAName,\n",
    "            'RawTransactionName': RawTransactionName,\n",
    "            'score': score,\n",
    "            'top_scores': top_scores,\n",
    "            'patterns': patterns\n",
    "        }\n",
    "    \n",
    "    # Analyze false negatives\n",
    "    print(\"\\nFalse Negative Analysis (Incorrectly Predicted as Non-Match):\")\n",
    "    fn_analysis = {}\n",
    "    \n",
    "    for i, (_, row) in enumerate(fn_df.iterrows()):\n",
    "        DBAName = row['DBAName']\n",
    "        RawTransactionName = row['RawTransactionName']\n",
    "        score = row['Enhanced_Score']\n",
    "        \n",
    "        print(f\"\\nError Case {i+1}: {DBAName} <-> {RawTransactionName} (Score: {score:.4f})\")\n",
    "        \n",
    "        # Get all similarity scores\n",
    "        scores = merchant_matcher.get_all_similarity_scores(DBAName, RawTransactionName)\n",
    "        \n",
    "        # Display top scores\n",
    "        top_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"Top Individual Algorithm Scores:\")\n",
    "        for algo, algo_score in top_scores:\n",
    "            print(f\"  {algo.replace('_', ' ').title()}: {algo_score:.4f}\")\n",
    "        \n",
    "        # Check for patterns\n",
    "        patterns = merchant_matcher.detect_complex_business_patterns(DBAName, RawTransactionName)\n",
    "        if patterns:\n",
    "            print(\"Detected Business Patterns:\")\n",
    "            for pattern, pattern_score in patterns.items():\n",
    "                print(f\"  {pattern.replace('_', ' ').title()}: {pattern_score:.4f}\")\n",
    "        \n",
    "        # Explain issue\n",
    "        print(\"Possible reason for misclassification:\")\n",
    "        if all(score < 0.5 for algo, score in scores.items() if 'DBAName' in algo):\n",
    "            print(\"  No strong DBAName formation detected\")\n",
    "        \n",
    "        if all(score < 0.7 for algo, score in scores.items() if 'jaro' in algo or 'levenshtein' in algo):\n",
    "            print(\"  Low string similarity scores\")\n",
    "        \n",
    "        if not patterns:\n",
    "            print(\"  No business patterns detected\")\n",
    "        \n",
    "        # Store analysis\n",
    "        fn_analysis[i] = {\n",
    "            'DBAName': DBAName,\n",
    "            'RawTransactionName': RawTransactionName,\n",
    "            'score': score,\n",
    "            'top_scores': top_scores,\n",
    "            'patterns': patterns\n",
    "        }\n",
    "    \n",
    "    # Return analysis results\n",
    "    return {\n",
    "        'false_positives': fp_analysis,\n",
    "        'false_negatives': fn_analysis,\n",
    "        'fp_df': fp_df,\n",
    "        'fn_df': fn_df\n",
    "    }\n",
    "\n",
    "def cross_validate_merchant_matcher(test_data_path=None, gold_standard_column='Expected_Match', n_folds=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to assess model stability\n",
    "    \n",
    "    Args:\n",
    "        test_data_path: Path to test data file\n",
    "        gold_standard_column: Column name for the gold standard match status\n",
    "        n_folds: Number of cross-validation folds\n",
    "        \n",
    "    Returns:\n",
    "        dict: Cross-validation results\n",
    "    \"\"\"\n",
    "    print(f\"Performing {n_folds}-fold cross-validation...\")\n",
    "    \n",
    "    # Get test data\n",
    "    if test_data_path is None:\n",
    "        # Create synthetic test data\n",
    "        eval_results = evaluate_merchant_matcher(None, gold_standard_column)\n",
    "        test_df = eval_results['results_df'][['DBAName', 'RawTransactionName', 'Merchant_Category', 'Expected_Match']]\n",
    "    else:\n",
    "        # Load test data from file\n",
    "        try:\n",
    "            if test_data_path.lower().endswith('.csv'):\n",
    "                test_df = pd.read_csv(test_data_path)\n",
    "            else:\n",
    "                test_df = pd.read_excel(test_data_path)\n",
    "                \n",
    "            # Verify that gold standard column exists\n",
    "            if gold_standard_column not in test_df.columns:\n",
    "                raise ValueError(f\"Gold standard column '{gold_standard_column}' not found in test data\")\n",
    "                \n",
    "            # Convert expected values to boolean\n",
    "            test_df['Expected_Match'] = test_df[gold_standard_column].astype(bool)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading test data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Preprocess test data\n",
    "    processed_df = preprocess_merchant_data(test_df)\n",
    "    \n",
    "    # Check for sufficient data\n",
    "    if len(processed_df) < n_folds * 2:\n",
    "        print(f\"Warning: Not enough data for {n_folds} folds. Need at least {n_folds * 2} samples.\")\n",
    "        n_folds = max(2, len(processed_df) // 2)\n",
    "        print(f\"Reducing to {n_folds} folds.\")\n",
    "    \n",
    "    # Create folds\n",
    "    try:\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "        folds = list(kf.split(processed_df))\n",
    "    except ImportError:\n",
    "        print(\"Warning: scikit-learn not available. Using manual fold creation.\")\n",
    "        # Manual fold creation\n",
    "        indices = np.random.permutation(len(processed_df))\n",
    "        fold_size = len(processed_df) // n_folds\n",
    "        folds = []\n",
    "        for i in range(n_folds):\n",
    "            test_indices = indices[i*fold_size:(i+1)*fold_size]\n",
    "            train_indices = np.setdiff1d(indices, test_indices)\n",
    "            folds.append((train_indices, test_indices))\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(folds):\n",
    "        print(f\"\\nProcessing fold {fold+1}/{n_folds}...\")\n",
    "        \n",
    "        # Split data\n",
    "        train_df = processed_df.iloc[train_idx]\n",
    "        test_df = processed_df.iloc[test_idx]\n",
    "        \n",
    "        # Process data\n",
    "        results_df = process_merchant_data(test_df, merchant_matcher)\n",
    "        \n",
    "        # Find optimal threshold using training data\n",
    "        train_results = process_merchant_data(train_df, merchant_matcher)\n",
    "        thresholds = np.linspace(0.1, 1.0, 19)\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            predictions = train_results['Enhanced_Score'] >= threshold\n",
    "            true_labels = train_df['Expected_Match'].values\n",
    "            \n",
    "            # Calculate metrics\n",
    "            tp = sum(predictions & true_labels)\n",
    "            fp = sum(predictions & ~true_labels)\n",
    "            fn = sum(~predictions & true_labels)\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        print(f\"  Optimal threshold for fold {fold+1}: {best_threshold:.4f}\")\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        results_df['Predicted_Match'] = results_df['Enhanced_Score'] >= best_threshold\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = sum((results_df['Predicted_Match'] == True) & (test_df['Expected_Match'] == True))\n",
    "        tn = sum((results_df['Predicted_Match'] == False) & (test_df['Expected_Match'] == False))\n",
    "        fp = sum((results_df['Predicted_Match'] == True) & (test_df['Expected_Match'] == False))\n",
    "        fn = sum((results_df['Predicted_Match'] == False) & (test_df['Expected_Match'] == True))\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'threshold': best_threshold,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'true_negatives': tn\n",
    "        })\n",
    "        \n",
    "        print(f\"  Fold {fold+1} Results:\")\n",
    "        print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"    Precision: {precision:.4f}\")\n",
    "        print(f\"    Recall: {recall:.4f}\")\n",
    "        print(f\"    F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in fold_results]),\n",
    "        'precision': np.mean([r['precision'] for r in fold_results]),\n",
    "        'recall': np.mean([r['recall'] for r in fold_results]),\n",
    "        'f1_score': np.mean([r['f1_score'] for r in fold_results]),\n",
    "        'threshold': np.mean([r['threshold'] for r in fold_results])\n",
    "    }\n",
    "    \n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in fold_results]),\n",
    "        'precision': np.std([r['precision'] for r in fold_results]),\n",
    "        'recall': np.std([r['recall'] for r in fold_results]),\n",
    "        'f1_score': np.std([r['f1_score'] for r in fold_results]),\n",
    "        'threshold': np.std([r['threshold'] for r in fold_results])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCross-Validation Summary:\")\n",
    "    print(f\"  Average Accuracy: {avg_metrics['accuracy']:.4f} ({std_metrics['accuracy']:.4f})\")\n",
    "    print(f\"  Average Precision: {avg_metrics['precision']:.4f} ({std_metrics['precision']:.4f})\")\n",
    "    print(f\"  Average Recall: {avg_metrics['recall']:.4f} ({std_metrics['recall']:.4f})\")\n",
    "    print(f\"  Average F1 Score: {avg_metrics['f1_score']:.4f} ({std_metrics['f1_score']:.4f})\")\n",
    "    print(f\"  Average Threshold: {avg_metrics['threshold']:.4f} ({std_metrics['threshold']:.4f})\")\n",
    "    \n",
    "    # Check for potential overfitting\n",
    "    if std_metrics['f1_score'] > 0.15:\n",
    "        print(\"\\nWarning: High variance in F1 scores across folds.\")\n",
    "        print(\"This may indicate that the model's performance is unstable.\")\n",
    "        print(\"Consider using a larger dataset or a simpler model.\")\n",
    "    \n",
    "    # Check if thresholds vary significantly\n",
    "    if std_metrics['threshold'] > 0.1:\n",
    "        print(\"\\nWarning: High variance in optimal thresholds across folds.\")\n",
    "        print(\"This may indicate that the optimal threshold is dataset-dependent.\")\n",
    "        print(\"Consider using a fixed threshold based on business requirements.\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'avg_metrics': avg_metrics,\n",
    "        'std_metrics': std_metrics\n",
    "    }\n",
    "\n",
    "print(\"Cross-validation and error analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbf02d-76dd-4ed3-a1de-d0aec46d3730",
   "metadata": {},
   "source": [
    "# Understanding Cross-Validation and Error Analysis in Merchant Name Matching\r\n",
    "\r\n",
    "This code introduces advanced evaluation techniques that help us deeply understand how our merchant name matching system performs and why it sometimes makes mistakes. Let me walk you through what these functions do and why they're crucial for a production-ready matching system.\r\n",
    "\r\n",
    "## The Two Key Functions\r\n",
    "\r\n",
    "The code defines two sophisticated evaluation functions that go beyond basic metrics to provide deeper insights into the system's behavior.\r\n",
    "\r\n",
    "### 1. Error Case Visualization and Analysis\r\n",
    "\r\n",
    "The first function, `visualize_error_cases()`, provides a detailed examination of specific cases where the matching system made incorrect predictions:\r\n",
    "\r\n",
    "```python\r\n",
    "def visualize_error_cases(results_df, num_cases=5):\r\n",
    "```\r\n",
    "\r\n",
    "This function performs a forensic analysis of matching errors by:\r\n",
    "\r\n",
    "1. **Identifying two types of errors**:\r\n",
    "   - False positives: Merchant pairs incorrectly predicted as matches\r\n",
    "   - False negatives: Merchant pairs incorrectly predicted as non-matches\r\n",
    "\r\n",
    "2. **Diagnosing each error case** by examining:\r\n",
    "   - The underlying scores from individual algorithms\r\n",
    "   - Any business patterns that were detected\r\n",
    "   - Possible reasons for the misclassification\r\n",
    "\r\n",
    "3. **Providing specific hypotheses** about why each error occurred, such as:\r\n",
    "   - \"Structure pattern detection may be too aggressive\"\r\n",
    "   - \"BERT semantic similarity may be overvaluing similar contexts\"\r\n",
    "   - \"DBAName formation detection may be too lenient\"\r\n",
    "\r\n",
    "This deep analysis goes beyond simply identifying that errors existit helps us understand *why* they occur, which is essential for improving the system.\r\n",
    "\r\n",
    "### 2. Cross-Validation for Stability Assessment\r\n",
    "\r\n",
    "The second function, `cross_validate_merchant_matcher()`, evaluates how consistently the matching system performs across different subsets of data:\r\n",
    "\r\n",
    "```python\r\n",
    "def cross_validate_merchant_matcher(test_data_path=None, gold_standard_column='Expected_Match', n_folds=5):\r\n",
    "```\r\n",
    "\r\n",
    "This function:\r\n",
    "\r\n",
    "1. **Divides the data into multiple folds** (default is 5)\r\n",
    "\r\n",
    "2. **Performs a rigorous evaluation cycle** for each fold:\r\n",
    "   - Uses part of the data to find the optimal threshold\r\n",
    "   - Applies that threshold to predict matches on the remaining data\r\n",
    "   - Calculates performance metrics for that fold\r\n",
    "\r\n",
    "3. **Aggregates results across all folds** to provide:\r\n",
    "   - Average metrics (accuracy, precision, recall, F1 score)\r\n",
    "   - Standard deviations that indicate stability/variability\r\n",
    "   - Warning flags for potential issues like overfitting\r\n",
    "\r\n",
    "4. **Assesses parameter stability** by checking if optimal thresholds vary significantly across folds\r\n",
    "\r\n",
    "Cross-validation helps ensure that our performance metrics aren't just the result of lucky (or unlucky) data splits but represent the system's true capabilities.\r\n",
    "\r\n",
    "## Why These Functions Are Important\r\n",
    "\r\n",
    "These functions address critical questions that simple accuracy metrics can't answer:\r\n",
    "\r\n",
    "### Understanding Error Patterns\r\n",
    "\r\n",
    "The error analysis helps identify systematic weaknesses in the matching system. For example, it might reveal that:\r\n",
    "\r\n",
    "- The system consistently misclassifies certain types of business names\r\n",
    "- Specific algorithms like BERT or DBAName detection might be causing problems\r\n",
    "- The system might be too sensitive to certain patterns like inverted names\r\n",
    "\r\n",
    "These insights allow targeted improvements rather than blind adjustments.\r\n",
    "\r\n",
    "### Assessing Model Stability\r\n",
    "\r\n",
    "Cross-validation reveals whether the system's performance is:\r\n",
    "\r\n",
    "- **Robust**: Consistent across different data samples\r\n",
    "- **Brittle**: Highly variable depending on which examples it sees\r\n",
    "- **Generalizable**: Likely to perform well on new, unseen data\r\n",
    "\r\n",
    "The stability warnings are particularly valuable:\r\n",
    "\r\n",
    "```python\r\n",
    "# Check for potential overfitting\r\n",
    "if std_metrics['f1_score'] > 0.15:\r\n",
    "    print(\"\\nWarning: High variance in F1 scores across folds.\")\r\n",
    "    print(\"This may indicate that the model's performance is unstable.\")\r\n",
    "    print(\"Consider using a larger dataset or a simpler model.\")\r\n",
    "```\r\n",
    "\r\n",
    "These warnings help prevent deploying a system that performs well on test data but might fail in production.\r\n",
    "\r\n",
    "## Technical Sophistication\r\n",
    "\r\n",
    "Several aspects of this code demonstrate technical depth and sophistication:\r\n",
    "\r\n",
    "### 1. Adaptive Fold Selection\r\n",
    "\r\n",
    "The cross-validation function intelligently adjusts the number of folds based on available data:\r\n",
    "\r\n",
    "```python\r\n",
    "if len(processed_df) < n_folds * 2:\r\n",
    "    print(f\"Warning: Not enough data for {n_folds} folds. Need at least {n_folds * 2} samples.\")\r\n",
    "    n_folds = max(2, len(processed_df) // 2)\r\n",
    "    print(f\"Reducing to {n_folds} folds.\")\r\n",
    "```\r\n",
    "\r\n",
    "This prevents errors when working with small datasets and ensures statistically valid results.\r\n",
    "\r\n",
    "### 2. Algorithmic Root Cause Analysis\r\n",
    "\r\n",
    "The error analysis goes beyond superficial classification to identify which specific algorithms might be causing errors:\r\n",
    "\r\n",
    "```python\r\n",
    "if any(score > 0.9 for algo, score in scores.items() if 'bert' in algo):\r\n",
    "    print(\"  BERT semantic similarity may be overvaluing similar contexts\")\r\n",
    "```\r\n",
    "\r\n",
    "This level of algorithmic introspection is rare in matching systems and provides actionable insights for improvement.\r\n",
    "\r\n",
    "### 3. Graceful Degradation for Missing Libraries\r\n",
    "\r\n",
    "The code handles the potential absence of scikit-learn by implementing manual alternatives:\r\n",
    "\r\n",
    "```python\r\n",
    "try:\r\n",
    "    from sklearn.model_selection import KFold\r\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\r\n",
    "    folds = list(kf.split(processed_df))\r\n",
    "except ImportError:\r\n",
    "    print(\"Warning: scikit-learn not available. Using manual fold creation.\")\r\n",
    "    # Manual fold creation\r\n",
    "    indices = np.random.permutation(len(processed_df))\r\n",
    "    # ...\r\n",
    "```\r\n",
    "\r\n",
    "This ensures the evaluation can run in environments with different dependency configurations.\r\n",
    "\r\n",
    "### 4. Fold-Specific Threshold Optimization\r\n",
    "\r\n",
    "The cross-validation doesn't just use a single threshold but optimizes it separately for each fold:\r\n",
    "\r\n",
    "```python\r\n",
    "# Find optimal threshold using training data\r\n",
    "train_results = process_merchant_data(train_df, merchant_matcher)\r\n",
    "thresholds = np.linspace(0.1, 1.0, 19)\r\n",
    "best_f1 = 0\r\n",
    "best_threshold = 0.5\r\n",
    "\r\n",
    "for threshold in thresholds:\r\n",
    "    # Find optimal threshold for this fold\r\n",
    "    # ...\r\n",
    "```\r\n",
    "\r\n",
    "This mimics how the system would be tuned in a real-world deployment and provides more realistic performance estimates.\r\n",
    "\r\n",
    "## Real-World Applications\r\n",
    "\r\n",
    "These advanced evaluation techniques enable critical real-world applications:\r\n",
    "\r\n",
    "### 1. Targeted System Improvement\r\n",
    "\r\n",
    "The detailed error analysis pinpoints specific weaknesses that engineers can target for improvement, such as adjusting the weights of problematic algorithms or modifying pattern detection rules.\r\n",
    "\r\n",
    "### 2. Reliability Assessment for Stakeholders\r\n",
    "\r\n",
    "Cross-validation results with standard deviations provide stakeholders with confidence intervals for expected performance, helping them make informed decisions about system deployment.\r\n",
    "\r\n",
    "### 3. Threshold Selection Guidance\r\n",
    "\r\n",
    "The variability analysis of optimal thresholds helps determine whether:\r\n",
    "- A single global threshold is appropriate\r\n",
    "- Domain-specific thresholds might be needed\r\n",
    "- Threshold selection should prioritize business requirements over statistical optimization\r\n",
    "\r\n",
    "### 4. Quality Assurance\r\n",
    "\r\n",
    "These functions can be integrated into continuous integration pipelines to ensure that system changes don't introduce new error patterns or stability issues.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This code represents the scientific foundation needed to truly understand and improve merchant name matching performance. While the previous evaluation functions gave us basic metrics, these advanced techniques provide deeper insights into *why* errors occur and how *stable* our performance truly is.\r\n",
    "\r\n",
    "The combination of detailed error analysis and cross-validation transforms the merchant matching system from a black-box algorithm into a transparent, understandable tool whose strengths and limitations can be clearly communicated to stakeholders. This level of insight is essential for building trust in the system and guiding its ongoing improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d4feec94-f0b7-4dbd-830e-1aa1a65b2411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive testing and analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Comprehensive Testing and Analysis\n",
    "\n",
    "def run_comprehensive_evaluation(test_data_path=None, gold_standard_column='Expected_Match'):\n",
    "    \"\"\"\n",
    "    Run a comprehensive evaluation of the merchant matcher\n",
    "    \n",
    "    Args:\n",
    "        test_data_path: Path to test data file\n",
    "        gold_standard_column: Column name for the gold standard match status\n",
    "        \n",
    "    Returns:\n",
    "        dict: Comprehensive evaluation results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Running comprehensive evaluation of merchant matcher...\")\n",
    "    \n",
    "    # Step 1: Basic Evaluation\n",
    "    print(\"\\n=== Step 1: Basic Evaluation ===\")\n",
    "    basic_eval = evaluate_merchant_matcher(test_data_path, gold_standard_column)\n",
    "    \n",
    "    # Step 2: Find Optimal Threshold\n",
    "    print(\"\\n=== Step 2: Finding Optimal Threshold ===\")\n",
    "    threshold_results = find_optimal_threshold(test_data_path, gold_standard_column)\n",
    "    \n",
    "    # Step 3: Algorithm Comparison\n",
    "    print(\"\\n=== Step 3: Algorithm Comparison ===\")\n",
    "    algorithm_comparison = compare_algorithms(test_data_path, gold_standard_column)\n",
    "    \n",
    "    # Step 4: Error Analysis\n",
    "    print(\"\\n=== Step 4: Error Analysis ===\")\n",
    "    error_analysis = visualize_error_cases(basic_eval['results_df'])\n",
    "    \n",
    "    # Step 5: Cross-Validation\n",
    "    print(\"\\n=== Step 5: Cross-Validation ===\")\n",
    "    cv_results = cross_validate_merchant_matcher(test_data_path, gold_standard_column)\n",
    "    \n",
    "    # Calculate total execution time\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nComprehensive evaluation completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Return all results\n",
    "    return {\n",
    "        'basic_evaluation': basic_eval,\n",
    "        'threshold_optimization': threshold_results,\n",
    "        'algorithm_comparison': algorithm_comparison,\n",
    "        'error_analysis': error_analysis,\n",
    "        'cross_validation': cv_results,\n",
    "        'execution_time': total_time\n",
    "    }\n",
    "\n",
    "def run_example_pipeline():\n",
    "    \"\"\"\n",
    "    Run an example merchant matching pipeline demonstration\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Enhanced Merchant Name Matching - Pipeline Demonstration\".center(80))\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Example 1: Process sample input file\n",
    "    print(\"\\n1. Processing a sample merchant file:\")\n",
    "    # Create a sample file for demonstration\n",
    "    sample_data = pd.DataFrame({\n",
    "        'DBAName': ['BOA', 'MCD', 'SBUX', 'TGT', 'MSFT'],\n",
    "        'RawTransactionName': ['Bank of America', 'McDonalds Corporation', 'Starbucks Coffee', 'Target', 'Microsoft'],\n",
    "        'Merchant_Category': ['Banking', 'Restaurant', 'Restaurant', 'Retail', 'Technology']\n",
    "    })\n",
    "    \n",
    "    sample_file = \"wrongless.xlsx\"\n",
    "    sample_data.to_excel(sample_file, index=False)\n",
    "    \n",
    "    # Process the sample file\n",
    "    results = run_merchant_matching_pipeline(sample_file, \"sample_results.xlsx\")\n",
    "    \n",
    "    # Example 2: Interactive testing\n",
    "    print(\"\\n2. Interactive Merchant Matcher:\")\n",
    "    print(\"   (Skipped in automated demo - run interactive_merchant_matcher() separately)\")\n",
    "    \n",
    "    # Example 3: Algorithm comparison\n",
    "    print(\"\\n3. Algorithm Comparison:\")\n",
    "    algorithms = ['Jaro-Winkler', 'BERT Similarity', 'DBAName Formation', 'Enhanced Score']\n",
    "    print(f\"   Top algorithms: {', '.join(algorithms)}\")\n",
    "    \n",
    "    # Example 4: Batch processing\n",
    "    print(\"\\n4. Batch Processing Capability:\")\n",
    "    print(\"   System can process large files in batches to manage memory\")\n",
    "    print(\"   For PySpark integration, call adapt_for_pyspark()\")\n",
    "    \n",
    "    # Clean up sample files\n",
    "    try:\n",
    "        import os\n",
    "        os.remove(sample_file)\n",
    "        os.remove(\"wrongless.xlsx\")\n",
    "        print(\"\\nCleaned up sample files\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\nDemonstration complete!\")\n",
    "\n",
    "# Example usage to run the comprehensive evaluation:\n",
    "# evaluation_results = run_comprehensive_evaluation()\n",
    "\n",
    "# Example usage to process a specific file:\n",
    "# results_df = process_DBAName_file_and_export_results(\"wrongless.xlsx\", \"DBAName_Matching_Results.xlsx\")\n",
    "\n",
    "print(\"Comprehensive testing and analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d945860-0ae1-4cbe-a7a8-999c17fb3acc",
   "metadata": {},
   "source": [
    "# Comprehensive Testing and Analysis Framework for Merchant Matching\r\n",
    "\r\n",
    "This code defines a unified testing and demonstration framework for the merchant name matching system. It's essentially the \"command center\" that brings together all the evaluation tools we've explored previously into a cohesive whole. Let me walk you through what this framework accomplishes and why it's valuable.\r\n",
    "\r\n",
    "## The Comprehensive Evaluation Function\r\n",
    "\r\n",
    "The first function, `run_comprehensive_evaluation()`, orchestrates a complete assessment of the merchant matching system:\r\n",
    "\r\n",
    "```python\r\n",
    "def run_comprehensive_evaluation(test_data_path=None, gold_standard_column='Expected_Match'):\r\n",
    "```\r\n",
    "\r\n",
    "This function creates a systematic, step-by-step evaluation process that progresses from basic metrics to sophisticated analysis:\r\n",
    "\r\n",
    "### Step 1: Basic Evaluation\r\n",
    "\r\n",
    "It begins with fundamental performance metrics (accuracy, precision, recall, F1 score) to establish a baseline understanding of how well the system performs. This provides the essential metrics that stakeholders often want to see first.\r\n",
    "\r\n",
    "### Step 2: Threshold Optimization\r\n",
    "\r\n",
    "Next, it explores different threshold values to find the optimal cutoff point for classifying merchant pairs as matches. This step is crucial because the ideal threshold can vary based on the specific dataset and business requirements.\r\n",
    "\r\n",
    "### Step 3: Algorithm Comparison\r\n",
    "\r\n",
    "The function then compares different matching algorithms to understand which techniques work best for the given data. This comparison helps identify which components contribute most to the system's success or where improvements might be needed.\r\n",
    "\r\n",
    "### Step 4: Error Analysis\r\n",
    "\r\n",
    "Moving beyond aggregate metrics, the function analyzes specific error cases to understand why the system makes mistakes. This forensic analysis is invaluable for targeted improvements and understanding limitations.\r\n",
    "\r\n",
    "### Step 5: Cross-Validation\r\n",
    "\r\n",
    "Finally, it evaluates performance stability across different data subsets to ensure the system generalizes well and isn't overly sensitive to specific examples. This helps build confidence in the system's robustness.\r\n",
    "\r\n",
    "By executing these steps in sequence, the function creates a comprehensive evaluation that addresses multiple perspectives:\r\n",
    "- How well does the system perform overall? (Basic Evaluation)\r\n",
    "- How should we configure it? (Threshold Optimization)\r\n",
    "- Which techniques matter most? (Algorithm Comparison)\r\n",
    "- When and why does it fail? (Error Analysis)\r\n",
    "- How consistent is its performance? (Cross-Validation)\r\n",
    "\r\n",
    "The function returns all these results in a unified dictionary, making it easy to access and compare different aspects of the evaluation.\r\n",
    "\r\n",
    "## The Example Pipeline Demonstration\r\n",
    "\r\n",
    "The second function, `run_example_pipeline()`, serves a different but complementary purpose:\r\n",
    "\r\n",
    "```python\r\n",
    "def run_example_pipeline():\r\n",
    "```\r\n",
    "\r\n",
    "While the first function focuses on evaluation (addressing the question \"How well does it work?\"), this function demonstrates practical usage (addressing \"How do I use it?\"). It provides a guided tour of the system's capabilities:\r\n",
    "\r\n",
    "### Example 1: Processing Sample Data\r\n",
    "\r\n",
    "It creates and processes a small sample file to demonstrate the basic workflow, showing how to take merchant data from input to matched results.\r\n",
    "\r\n",
    "### Example 2: Interactive Testing Reference\r\n",
    "\r\n",
    "It mentions the interactive testing capability (without executing it directly), reminding users of this valuable tool for exploring specific merchant pairs.\r\n",
    "\r\n",
    "### Example 3: Algorithm Highlights\r\n",
    "\r\n",
    "It highlights the top-performing algorithms to emphasize the system's sophisticated matching techniques.\r\n",
    "\r\n",
    "### Example 4: Batch Processing Capabilities\r\n",
    "\r\n",
    "It introduces the batch processing and distributed computing capabilities for handling large datasets, which are crucial for enterprise applications.\r\n",
    "\r\n",
    "The function also handles cleanup of sample files, demonstrating good practice in temporary file management.\r\n",
    "\r\n",
    "## Technical Design Excellence\r\n",
    "\r\n",
    "Several aspects of this code reflect excellent technical design:\r\n",
    "\r\n",
    "### 1. Progressive Disclosure\r\n",
    "\r\n",
    "The comprehensive evaluation follows a progressive disclosure pattern, beginning with simple metrics and gradually introducing more complex analyses. This structure helps users build understanding incrementally rather than being overwhelmed with all results at once.\r\n",
    "\r\n",
    "### 2. Modular Architecture\r\n",
    "\r\n",
    "Each step in the evaluation calls a separate function that was defined previously, demonstrating good separation of concerns. This modular approach makes the code easier to maintain and allows individual components to be used independently.\r\n",
    "\r\n",
    "### 3. Unified Reporting\r\n",
    "\r\n",
    "By collecting all results in a single dictionary, the code provides a clean interface for accessing different evaluation aspects without requiring users to manage multiple result variables.\r\n",
    "\r\n",
    "### 4. Self-Contained Demo\r\n",
    "\r\n",
    "The example pipeline creates and cleans up its own test data, making it self-contained and easy to run without external dependencies. This is particularly valuable for demonstration and testing purposes.\r\n",
    "\r\n",
    "## Practical Applications and Value\r\n",
    "\r\n",
    "This unified testing and demonstration framework serves several important real-world needs:\r\n",
    "\r\n",
    "### 1. Comprehensive System Validation\r\n",
    "\r\n",
    "Before deploying the merchant matching system in production, stakeholders need confidence that it meets performance requirements across multiple dimensions. The comprehensive evaluation provides this validation in a systematic, reproducible way.\r\n",
    "\r\n",
    "### 2. Knowledge Transfer and Documentation\r\n",
    "\r\n",
    "The example pipeline serves as executable documentation, helping new users understand how to use the system through concrete examples. This accelerates knowledge transfer and adoption.\r\n",
    "\r\n",
    "### 3. Continuous Quality Assurance\r\n",
    "\r\n",
    "The comprehensive evaluation can be integrated into continuous integration pipelines to ensure that code changes don't degrade performance. By running this evaluation automatically on code changes, teams can maintain quality over time.\r\n",
    "\r\n",
    "### 4. Presentation and Reporting\r\n",
    "\r\n",
    "The structured output of the comprehensive evaluation provides all the data needed for creating performance reports for stakeholders. This helps communicate the system's capabilities clearly and build trust in its decisions.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "This code represents the capstone of our merchant name matching system, bringing together all the evaluation tools into a cohesive framework for testing and demonstration. By providing both a comprehensive evaluation process and a practical usage demonstration, it addresses the complementary needs of rigorous assessment and accessible introduction.\r\n",
    "\r\n",
    "The combination of systematic evaluation and user-friendly demonstration makes this framework valuable for multiple audiencesfrom data scientists assessing performance to developers implementing the system to business stakeholders making deployment decisions. It transforms a collection of sophisticated algorithms and evaluation techniques into a complete, accessible solution for merchant name matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6c109612-1324-41bb-b1dc-59ed5e199530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                     Enhanced Merchant Name Matching System                     \n",
      "================================================================================\n",
      "\n",
      "\n",
      "Processing wrongless.xlsx file...\n",
      "Loading and processing data from wrongless.xlsx...\n",
      "Successfully loaded 38 records from wrongless.xlsx\n",
      "Columns found: ['DBAName', 'RawTransactionName', 'Merchant Category']\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "              DBAName   RawTransactionName               Merchant Category\n",
      "0  Apple Store Online  THE TRUSTEE FOR THE  AUTO SERVICE SHOPS/NON DEALER \n",
      "1  Apple Store Online         APPLE.COM/AU              ELECTRONICS STORES\n",
      "2                 BWS          Sands Hotel           HOTELS/MOTELS/RESORTS\n",
      "\n",
      "Preprocessing merchant data...\n",
      "Category distribution after preprocessing:\n",
      "Merchant_Category\n",
      "Retail                           13\n",
      "Restaurant                        7\n",
      "Automotive                        4\n",
      "HOTELS/MOTELS/RESORTS             3\n",
      "Medical                           3\n",
      "BOAT DEALERS                      1\n",
      "ELECTRICAL CONTRACTORS            1\n",
      "DETECTIVE/PROTECTIVE AGEN         1\n",
      "REAL EST AGNTS & MGRS RENTALS     1\n",
      "OTHER DIRECT MARKETERS            1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Calculating similarity scores using enhanced merchant matcher...\n",
      "Processing 38 merchant entries with enhanced matcher...\n",
      "Progress: 2.6% (1/38) - Elapsed: 0.2s - Est. remaining: 0.0s\n",
      "Progress: 10.5% (4/38) - Elapsed: 0.8s - Est. remaining: 6.5s\n",
      "Progress: 18.4% (7/38) - Elapsed: 1.5s - Est. remaining: 6.7s\n",
      "Progress: 26.3% (10/38) - Elapsed: 2.1s - Est. remaining: 6.0s\n",
      "Progress: 34.2% (13/38) - Elapsed: 2.8s - Est. remaining: 5.3s\n",
      "Progress: 42.1% (16/38) - Elapsed: 3.4s - Est. remaining: 4.7s\n",
      "Progress: 50.0% (19/38) - Elapsed: 4.1s - Est. remaining: 4.1s\n",
      "Progress: 57.9% (22/38) - Elapsed: 4.7s - Est. remaining: 3.4s\n",
      "Progress: 65.8% (25/38) - Elapsed: 5.5s - Est. remaining: 2.9s\n",
      "Progress: 73.7% (28/38) - Elapsed: 6.2s - Est. remaining: 2.2s\n",
      "Progress: 81.6% (31/38) - Elapsed: 6.9s - Est. remaining: 1.5s\n",
      "Progress: 89.5% (34/38) - Elapsed: 7.5s - Est. remaining: 0.9s\n",
      "Progress: 97.4% (37/38) - Elapsed: 8.0s - Est. remaining: 0.2s\n",
      "Progress: 100.0% (38/38) - Elapsed: 8.2s - Est. remaining: 0.0s\n",
      "Processing completed in 8.20 seconds\n",
      "\n",
      "Categorizing matches based on thresholds...\n",
      "\n",
      "Match category distribution:\n",
      "  No Match: 38 (100.0%)\n",
      "\n",
      "Exporting results to DBAName_Matching_Results.xlsx...\n",
      "Results successfully exported to DBAName_Matching_Results.xlsx\n",
      "\n",
      "Summary of exported data:\n",
      "   Total merchant entries: 38\n",
      "\n",
      "Match Category Distribution:\n",
      "   No Match: 38 entries (100.0%)\n",
      "\n",
      "Total processing time: 12.26 seconds\n",
      "Processing complete! Results saved to DBAName_Matching_Results.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Usage Examples and Main Execution\n",
    "\n",
    "# This cell contains usage examples and demonstrations\n",
    "\n",
    "# This cell contains usage examples and demonstrations\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function with usage examples\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Enhanced Merchant Name Matching System\".center(80))\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # [rest of the main function code]\n",
    "\n",
    "# Call the main function to demonstrate the system\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Process the wrongless.xlsx file and export results\n",
    "print(\"\\nProcessing wrongless.xlsx file...\")\n",
    "results = process_DBAName_file_and_export_results(\n",
    "    input_file=\"wrongless.xlsx\", \n",
    "    output_file=\"DBAName_Matching_Results.xlsx\"\n",
    ")\n",
    "print(f\"Processing complete! Results saved to DBAName_Matching_Results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440b285-b4a6-4f4b-95c2-7bf35c084fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
