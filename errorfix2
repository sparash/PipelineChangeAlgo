# Fixing the SentenceTransformer Scope and Model Loading Issues

I see two significant problems in your code:

1. A scope issue with the `SentenceTransformer` import
2. Issues with loading the transformer models in your environment

Let's solve both of these problems with a more robust implementation.

## Complete Solution: Fixing the MerchantMatchingSystem Class

```python
import pandas as pd
import numpy as np
import re
import string
from typing import Dict, List, Tuple, Optional, Union
import pickle
import os
from collections import defaultdict
import time
import warnings
warnings.filterwarnings('ignore')

# For text similarity
import textdistance
from Levenshtein import ratio, jaro_winkler, distance as lev_distance
import jellyfish
import difflib
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sklearn.neighbors import NearestNeighbors

class MerchantMatchingSystem:
    # ===== SIMILARITY ALGORITHMS =====
    def _jaro_winkler_similarity(self, s1: str, s2: str) -> float:
        """Jaro-Winkler similarity"""
        return jaro_winkler(s1, s2)
    
    def _levenshtein_similarity(self, s1: str, s2: str) -> float:
        """Levenshtein similarity (normalized)"""
        max_len = max(len(s1), len(s2))
        if max_len == 0:
            return 1.0
        return 1 - (lev_distance(s1, s2) / max_len)
    
    def _tfidf_cosine_similarity(self, s1: str, s2: str) -> float:
        """TF-IDF with cosine similarity"""
        try:
            # Fit on both strings to ensure the vocabulary covers both
            tfidf_matrix = self.tfidf_vectorizer.fit_transform([s1, s2])
            return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        except:
            return 0.0
    
    def _jaccard_similarity(self, s1: str, s2: str) -> float:
        """Jaccard similarity on character set"""
        return textdistance.jaccard(s1, s2)
    
    def _sorensen_dice_similarity(self, s1: str, s2: str) -> float:
        """Sorensen-Dice coefficient"""
        return textdistance.sorensen(s1, s2)
    
    def _monge_elkan_similarity(self, s1: str, s2: str) -> float:
        """Monge-Elkan similarity using Jaro-Winkler for inner similarity"""
        # Split strings into tokens
        tokens1 = s1.split()
        tokens2 = s2.split()
        
        if not tokens1 or not tokens2:
            return 0.0
        
        sum_sim = 0.0
        for t1 in tokens1:
            # Find max similarity with any token in s2
            max_sim = max(jaro_winkler(t1, t2) for t2 in tokens2)
            sum_sim += max_sim
        
        return sum_sim / len(tokens1)
    
    def _needleman_wunsch_similarity(self, s1: str, s2: str) -> float:
        """Needleman-Wunsch similarity (global alignment)"""
        return textdistance.needleman_wunsch(s1, s2)
    
    def _lcs_similarity(self, s1: str, s2: str) -> float:
        """Longest Common Subsequence similarity"""
        # Use difflib for LCS
        matcher = difflib.SequenceMatcher(None, s1, s2)
        match = matcher.find_longest_match(0, len(s1), 0, len(s2))
        max_len = max(len(s1), len(s2))
        if max_len == 0:
            return 1.0
        return match.size / max_len
    
    def _damerau_levenshtein_similarity(self, s1: str, s2: str) -> float:
        """Damerau-Levenshtein similarity (normalized)"""
        distance = textdistance.damerau_levenshtein(s1, s2)
        max_len = max(len(s1), len(s2))
        if max_len == 0:
            return 1.0
        return 1 - (distance / max_len)
    
    def _smith_waterman_similarity(self, s1: str, s2: str) -> float:
        """Smith-Waterman similarity (local alignment)"""
        return textdistance.smith_waterman(s1, s2)
    
    def _sbert_cosine_similarity(self, s1: str, s2: str) -> float:
        """
        Fallback for sentence embeddings if SentenceTransformer isn't available.
        Uses TF-IDF as a backup.
        """
        # If we have a sentence transformer model, use it
        if hasattr(self, 'embedding_model') and self.embedding_model is not None:
            try:
                emb1 = self.embedding_model.encode(s1, show_progress_bar=False)
                emb2 = self.embedding_model.encode(s2, show_progress_bar=False)
                return cosine_similarity([emb1], [emb2])[0][0]
            except:
                pass  # Fallback to TF-IDF if encoding fails
        
        # Fallback to TF-IDF cosine similarity
        return self._tfidf_cosine_similarity(s1, s2)
    
    def _hybrid_category_aware_similarity(self, s1: str, s2: str, category1: str, category2: str) -> float:
        """Our custom hybrid category-aware similarity measure"""
        # If categories don't match, significantly reduce similarity
        if category1 != category2:
            category_match = 0.1
        else:
            category_match = 1.0
        
        # Calculate base similarities
        jaro = self._jaro_winkler_similarity(s1, s2)
        lev = self._levenshtein_similarity(s1, s2)
        
        # Use either SBERT or TF-IDF, depending on what's available
        semantic_sim = self._sbert_cosine_similarity(s1, s2)
        
        # Weighted combination
        base_similarity = (0.4 * jaro + 0.3 * lev + 0.3 * semantic_sim)
        
        # Apply category penalty
        return base_similarity * category_match
    
    def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2'):
        """
        Initialize the merchant matching system with multiple algorithms.
        
        Args:
            embedding_model_name: The name of the pre-trained SBERT model to use
        """
        self.model_name = embedding_model_name
        
        # Initialize TFIDF vectorizer for text similarity
        self.tfidf_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))
        
        # Try to load SentenceTransformer if available
        try:
            # Import here inside the method to maintain scope
            from sentence_transformers import SentenceTransformer
            print(f"Attempting to load model: {embedding_model_name}")
            self.embedding_model = SentenceTransformer(embedding_model_name)
            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            print(f"Successfully loaded model: {embedding_model_name}")
        except Exception as e:
            print(f"Could not load SentenceTransformer model: {str(e)}")
            print("Will use TF-IDF fallback for text similarity")
            self.embedding_model = None
            self.embedding_dim = 100  # Default dimension
        
        # Acronym dictionary - maps acronyms to potential expansions by category
        self.acronym_dict = defaultdict(lambda: defaultdict(list))
        
        # Category-specific index for fast nearest neighbor search
        self.category_indices = {}
        
        # Reference data
        self.reference_data = {}
        
        # Compiled regex patterns
        self.patterns = {
            'special_chars': re.compile(r'[^\w\s]'),
            'extra_spaces': re.compile(r'\s+'),
            'acronym': re.compile(r'^[A-Z0-9]{2,}$')
        }
        
        # Common business suffixes to remove
        self.business_suffixes = {
            'inc', 'llc', 'ltd', 'corp', 'corporation', 'co', 'company',
            'incorporated', 'limited', 'group', 'holdings', 'services',
            'international', 'enterprises', 'solutions', 'plc', 'gmbh'
        }
        
        # Confidence thresholds
        self.similarity_threshold = 0.85
        
        # Define the algorithms for comparison
        self.algorithms = {
            'jaro_winkler': self._jaro_winkler_similarity,
            'levenshtein': self._levenshtein_similarity,
            'tfidf_cosine': self._tfidf_cosine_similarity,
            'jaccard': self._jaccard_similarity,
            'sorensen_dice': self._sorensen_dice_similarity,
            'monge_elkan': self._monge_elkan_similarity,
            'needleman_wunsch': self._needleman_wunsch_similarity,
            'lcs_similarity': self._lcs_similarity,
            'damerau_levenshtein': self._damerau_levenshtein_similarity,
            'smith_waterman': self._smith_waterman_similarity,
            'sbert_cosine': self._sbert_cosine_similarity,
            'hybrid_category_aware': self._hybrid_category_aware_similarity
        }
        
        print("MerchantMatchingSystem initialized successfully")
    
    def preprocess_name(self, name: str) -> str:
        """
        Clean and standardize merchant name.
        
        Args:
            name: The merchant name to preprocess
            
        Returns:
            Preprocessed merchant name
        """
        if not name or not isinstance(name, str):
            return ""
            
        # Convert to lowercase
        name = name.lower()
        
        # Remove special characters
        name = self.patterns['special_chars'].sub(' ', name)
        
        # Remove extra spaces
        name = self.patterns['extra_spaces'].sub(' ', name)
        
        # Remove business suffixes
        tokens = name.split()
        tokens = [t for t in tokens if t not in self.business_suffixes]
        
        # Rejoin and strip
        name = ' '.join(tokens).strip()
        
        return name

    def is_acronym(self, name: str) -> bool:
        """
        Check if a name is likely an acronym.
        
        Args:
            name: The name to check
            
        Returns:
            True if the name is likely an acronym, False otherwise
        """
        # Remove spaces and check if it's all caps and 2-6 characters
        clean_name = ''.join(c for c in name if c.isalnum()).upper()
        return len(clean_name) >= 2 and len(clean_name) <= 6 and clean_name.isupper()
    
    def load_reference_data(self, file_path: str):
        """
        Load reference merchant data from a CSV/Excel file.
        
        Args:
            file_path: Path to the file containing reference data
        """
        print(f"Loading reference data from: {file_path}")
        
        # Determine file type and load accordingly
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(file_path)
        else:
            raise ValueError("Unsupported file format. Please provide a CSV or Excel file.")
        
        # Check for your specific file format columns
        if "Acronym" in df.columns and "Full Name" in df.columns and "Merchant Category" in df.columns:
            print("Detected Acronym_Categorized.xlsx format with columns: 'Acronym', 'Full Name', 'Merchant Category'")
            # Rename columns to match our expected format
            df = df.rename(columns={
                'Acronym': 'merchant_name',
                'Full Name': 'full_form',
                'Merchant Category': 'merchant_category'
            })
        # For backward compatibility with the format mentioned in the code
        elif "Acronym" in df.columns and "Category" in df.columns and "Correct Full Form" in df.columns:
            print("Detected Acronym_Categorized.xlsx format with columns: 'Acronym', 'Category', 'Correct Full Form'")
            df = df.rename(columns={
                'Acronym': 'merchant_name',
                'Category': 'merchant_category',
                'Correct Full Form': 'full_form'
            })
        else:
            # Check for minimum required columns
            required_columns = ['merchant_name', 'merchant_category']
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"Reference data must contain either 'Acronym', 'Full Name', 'Merchant Category' columns or {required_columns}")
        
        # Clean and process data
        df['processed_name'] = df['merchant_name'].apply(self.preprocess_name)
        
        # If full_form column exists, process it too
        if 'full_form' in df.columns:
            df['processed_full_form'] = df['full_form'].apply(self.preprocess_name)
        
        # Store reference data
        self.reference_data = df.to_dict('records')
        
        # Build category-specific indices
        self._build_category_indices()
        
        # Build acronym dictionary
        self._build_acronym_dictionary()
        
        print(f"Loaded {len(self.reference_data)} reference merchants across {df['merchant_category'].nunique()} categories")
    
    def _build_category_indices(self):
        """
        Build category-specific indices for fast matching using scikit-learn's NearestNeighbors.
        """
        print("Building category-specific indices...")
        
        # Group reference data by category
        by_category = defaultdict(list)
        for item in self.reference_data:
            category = item['merchant_category']
            by_category[category].append(item)
        
        # Create embeddings or TF-IDF vectors and build indices for each category
        for category, items in by_category.items():
            # Determine which name field to use for embeddings
            if 'processed_full_form' in items[0] and all(item.get('processed_full_form') for item in items):
                names = [item['processed_full_form'] for item in items]
            else:
                names = [item['processed_name'] for item in items]
            
            try:
                # If we have a sentence transformer model, use it
                if self.embedding_model is not None:
                    print(f"Generating embeddings for category: {category} using SentenceTransformer")
                    embeddings = self.embedding_model.encode(names, show_progress_bar=False)
                    embeddings = normalize(embeddings)
                else:
                    # Fallback to TF-IDF
                    print(f"Generating TF-IDF vectors for category: {category}")
                    tfidf = TfidfVectorizer()
                    embeddings = tfidf.fit_transform(names).toarray()
                    embeddings = normalize(embeddings)
                
                # Create scikit-learn NearestNeighbors index
                index = NearestNeighbors(
                    n_neighbors=min(20, len(embeddings)), 
                    metric='cosine', 
                    algorithm='brute'  # Use brute force for accuracy
                )
                index.fit(embeddings)
                
                # Store index and corresponding items
                self.category_indices[category] = {
                    'index': index,
                    'items': items,
                    'embeddings': embeddings
                }
                
            except Exception as e:
                print(f"Error building index for category '{category}': {e}")
        
        print(f"Built indices for {len(self.category_indices)} categories")

    def _build_acronym_dictionary(self):
        """
        Build a category-aware acronym dictionary from reference data.
        """
        print("Building acronym dictionary...")
        
        # Process each reference merchant
        for item in self.reference_data:
            name = item['merchant_name']
            category = item['merchant_category']
            processed_name = item['processed_name']
            
            # Get the full form if available
            full_form = item.get('full_form', name)
            processed_full_form = item.get('processed_full_form', processed_name)
            
            # Check if it's an acronym
            if self.is_acronym(name):
                clean_acronym = ''.join(c for c in name if c.isalnum()).upper()
                # Store the full form by category
                self.acronym_dict[clean_acronym][category].append({
                    'original_name': name,
                    'full_form': full_form,
                    'processed_name': processed_name,
                    'processed_full_form': processed_full_form,
                    'merchant_id': item.get('merchant_id', None)
                })
        
        # Print statistics
        total_acronyms = sum(len(categories) for categories in self.acronym_dict.values())
        print(f"Built acronym dictionary with {len(self.acronym_dict)} unique acronyms across {total_acronyms} categories")
    
    # The remaining methods can stay the same, but make sure they handle the case when self.embedding_model is None
    
    def compare_all_algorithms(self, s1: str, s2: str, category1: str = None, category2: str = None) -> Dict[str, float]:
        """
        Compare two strings using all available algorithms.
        """
        results = {}
        
        # Process strings if needed
        if not s1.islower():
            s1 = self.preprocess_name(s1)
        if not s2.islower():
            s2 = self.preprocess_name(s2)
        
        # Run all algorithms except hybrid
        for name, algo in self.algorithms.items():
            if name != 'hybrid_category_aware':
                try:
                    start_time = time.time()
                    score = algo(s1, s2)
                    end_time = time.time()
                    results[name] = {
                        'score': score,
                        'time_ms': (end_time - start_time) * 1000
                    }
                except Exception as e:
                    results[name] = {
                        'score': 0.0,
                        'time_ms': 0,
                        'error': str(e)
                    }
        
        # Run hybrid algorithm if categories are provided
        if category1 and category2:
            try:
                start_time = time.time()
                score = self._hybrid_category_aware_similarity(s1, s2, category1, category2)
                end_time = time.time()
                results['hybrid_category_aware'] = {
                    'score': score,
                    'time_ms': (end_time - start_time) * 1000
                }
            except Exception as e:
                results['hybrid_category_aware'] = {
                    'score': 0.0,
                    'time_ms': 0,
                    'error': str(e)
                }
        
        return results
            
    def resolve_acronym(self, acronym: str, category: str) -> Optional[Dict]:
        """Resolve an acronym to its expansion based on merchant category."""
        # Clean acronym
        clean_acronym = ''.join(c for c in acronym if c.isalnum()).upper()
        
        # Check if the acronym exists in our dictionary
        if clean_acronym in self.acronym_dict:
            # Check if the category exists for this acronym
            if category in self.acronym_dict[clean_acronym]:
                # Return the first expansion for this category
                return self.acronym_dict[clean_acronym][category][0]
            
            # If category doesn't match, try to find the most likely category
            all_expansions = []
            for cat, expansions in self.acronym_dict[clean_acronym].items():
                for expansion in expansions:
                    all_expansions.append((cat, expansion))
            
            if all_expansions:
                # If there's only one expansion across all categories, return it
                if len(all_expansions) == 1:
                    return all_expansions[0][1]
        
        return None

    def find_matches(self, merchant_name: str, merchant_category: str, top_k: int = 5, 
                    run_all_algorithms: bool = False) -> List[Dict]:
        """Find the best matches for a merchant name within its category."""
        # Preprocess input name
        processed_name = self.preprocess_name(merchant_name)
        
        # Check if it's an acronym
        expanded_details = None
        if self.is_acronym(merchant_name):
            expanded_details = self.resolve_acronym(merchant_name, merchant_category)
            if expanded_details:
                print(f"Resolved acronym '{merchant_name}' to '{expanded_details['full_form']}'")
                if 'processed_full_form' in expanded_details:
                    processed_name = expanded_details['processed_full_form']
        
        # If category doesn't exist in our indices, try to find the closest category
        if merchant_category not in self.category_indices:
            print(f"Warning: Category '{merchant_category}' not found in reference data")
            # Default to searching all categories
            all_matches = []
            for category, index_data in self.category_indices.items():
                matches = self._find_matches_in_category(
                    processed_name, 
                    merchant_category, 
                    category, 
                    top_k,
                    run_all_algorithms
                )
                all_matches.extend(matches)
            
            # Sort by similarity and take top_k
            all_matches.sort(key=lambda x: x['similarity'], reverse=True)
            return all_matches[:top_k]
        
        # Find matches within the same category
        return self._find_matches_in_category(
            processed_name, 
            merchant_category, 
            merchant_category, 
            top_k,
            run_all_algorithms
        )

    def _find_matches_in_category(self, processed_name: str, query_category: str, 
                                  target_category: str, top_k: int, 
                                  run_all_algorithms: bool = False) -> List[Dict]:
        """Find matches for a name within a specific category."""
        index_data = self.category_indices[target_category]
        index = index_data['index']
        items = index_data['items']
        embeddings = index_data['embeddings']
        
        # Generate embedding for query name
        if self.embedding_model is not None:
            try:
                query_embedding = self.embedding_model.encode(processed_name, show_progress_bar=False)
                query_embedding = normalize([query_embedding])[0].reshape(1, -1)
            except Exception as e:
                print(f"Error generating embedding: {e}")
                # Fallback to TF-IDF
                tfidf = TfidfVectorizer()
                tfidf.fit([processed_name] + [item['processed_name'] for item in items])
                query_embedding = tfidf.transform([processed_name]).toarray()
                query_embedding = normalize(query_embedding)
        else:
            # Use TF-IDF if no embedding model
            tfidf = TfidfVectorizer()
            tfidf.fit([processed_name] + [item['processed_name'] for item in items])
            query_embedding = tfidf.transform([processed_name]).toarray()
            query_embedding = normalize(query_embedding)
        
        # Find nearest neighbors
        distances, indices = index.kneighbors(query_embedding, n_neighbors=min(top_k * 2, len(embeddings)))
        
        # Convert distances to similarities (scikit-learn returns cosine distances)
        similarities = [1 - dist for dist in distances[0]]
        
        # Create result objects
        results = []
        for i, (idx, similarity) in enumerate(zip(indices[0], similarities)):
            item = items[idx]
            
            # Determine the name field to use
            if 'processed_full_form' in item and item['processed_full_form']:
                compare_name = item['processed_full_form']
                display_name = item.get('full_form', item['merchant_name'])
            else:
                compare_name = item['processed_name']
                display_name = item['merchant_name']
            
            # Run comparison with all algorithms if requested
            if run_all_algorithms:
                algorithm_results = self.compare_all_algorithms(
                    processed_name, 
                    compare_name,
                    query_category,
                    target_category
                )
                
                # Use hybrid score if available, otherwise use SBERT or TF-IDF
                if 'hybrid_category_aware' in algorithm_results:
                    final_similarity = algorithm_results['hybrid_category_aware']['score']
                else:
                    final_similarity = algorithm_results['sbert_cosine']['score']
            else:
                # For normal operation, use our hybrid similarity
                final_similarity = self._hybrid_category_aware_similarity(
                    processed_name, 
                    compare_name,
                    query_category,
                    target_category
                )
                
                # Create a minimal algorithm_results
                algorithm_results = {
                    'hybrid_category_aware': {'score': final_similarity}
                }
            
            if final_similarity >= self.similarity_threshold or len(results) < 5:
                results.append({
                    'merchant_name': display_name,
                    'merchant_category': item['merchant_category'],
                    'similarity': final_similarity,
                    'merchant_id': item.get('merchant_id', None),
                    'algorithm_results': algorithm_results
                })
        
        # Sort by similarity
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results[:top_k]

    def save_model(self, file_path: str):
        """
        Save the model to a file.
        
        Args:
            file_path: Path to save the model
        """
        # We can't pickle the SBERT model, so we'll save just the model name
        model_data = {
            'embedding_model_name': self.model_name,  # Use our stored model name
            'acronym_dict': dict(self.acronym_dict),
            'similarity_threshold': self.similarity_threshold
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"Model saved to {file_path}")

    @classmethod
    def load_model(cls, file_path: str, reference_data_path: str = None):
        """
        Load a model from a file.
        
        Args:
            file_path: Path to the saved model
            reference_data_path: Optional path to reference data
            
        Returns:
            Loaded MerchantMatchingSystem
        """
        with open(file_path, 'rb') as f:
            model_data = pickle.load(f)
        
        # Create a new instance with the saved model name
        instance = cls(embedding_model_name=model_data['embedding_model_name'])
        
        # Restore state
        instance.acronym_dict = defaultdict(lambda: defaultdict(list))
        for acronym, categories in model_data['acronym_dict'].items():
            for category, expansions in categories.items():
                instance.acronym_dict[acronym][category] = expansions
        
        instance.similarity_threshold = model_data['similarity_threshold']
        
        # Load reference data if provided
        if reference_data_path:
            instance.load_reference_data(reference_data_path)
        
        return instance
    
    def batch_process(self, input_file: str, output_file: str, run_all_algorithms: bool = True):
        """
        Process a batch of merchant names from a file and compare all algorithms.
        
        Args:
            input_file: Path to input file (CSV or Excel)
            output_file: Path to output file
            run_all_algorithms: Whether to run all algorithms for comparison
        """
        print(f"Processing batch from: {input_file}")
        
        # Load input file
        if input_file.endswith('.csv'):
            df = pd.read_csv(input_file)
        elif input_file.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(input_file)
        else:
            raise ValueError("Unsupported file format. Please provide a CSV or Excel file.")
        
        # Determine input format
        if 'Acronym' in df.columns and 'Merchant Category' in df.columns:
            # Your specific format
            df = df.rename(columns={
                'Acronym': 'merchant_name',
                'Merchant Category': 'merchant_category',
                'Full Name': 'correct_full_form'
            })
        elif 'Acronym' in df.columns and 'Category' in df.columns:
            # Acronym_Categorized.xlsx format from previous code
            df = df.rename(columns={
                'Acronym': 'merchant_name',
                'Category': 'merchant_category',
                'Correct Full Form': 'correct_full_form'
            })
        elif not all(col in df.columns for col in ['merchant_name', 'merchant_category']):
            raise ValueError("Input file must contain appropriate columns")
        
        # Process each row
        results = []
        algorithm_names = list(self.algorithms.keys())
        
        for i, row in df.iterrows():
            merchant_name = row['merchant_name']
            merchant_category = row['merchant_category']
            correct_full_form = row.get('correct_full_form', row.get('Full Name', None))
            
            # Find matches
            matches = self.find_matches(
                merchant_name, 
                merchant_category, 
                top_k=1,
                run_all_algorithms=run_all_algorithms
            )
            
            # Prepare result row
            result = {
                'merchant_name': merchant_name,
                'merchant_category': merchant_category,
                'correct_full_form': correct_full_form
            }
            
            if matches:
                match = matches[0]
                result['matched_merchant'] = match['merchant_name']
                result['matched_category'] = match['merchant_category']
                result['hybrid_score'] = match['similarity']
                
                # Add scores for each algorithm if available
                if run_all_algorithms and 'algorithm_results' in match:
                    for algo_name in algorithm_names:
                        if algo_name in match['algorithm_results']:
                            result[f'{algo_name}_score'] = match['algorithm_results'][algo_name]['score']
                            result[f'{algo_name}_time_ms'] = match['algorithm_results'][algo_name].get('time_ms', 0)
            else:
                result['matched_merchant'] = None
                result['matched_category'] = None
                result['hybrid_score'] = 0.0
            
            # Check if match is correct
            if correct_full_form is not None and matches:
                result['is_correct'] = correct_full_form.lower() == match['merchant_name'].lower()
            else:
                result['is_correct'] = None
            
            results.append(result)
            
            # Print progress
            if (i + 1) % 10 == 0:
                print(f"Processed {i + 1}/{len(df)} rows")
        
        # Create output dataframe
        output_df = pd.DataFrame(results)
        
        # Calculate statistics for each algorithm
        if run_all_algorithms:
            stats = {}
            for algo_name in algorithm_names:
                score_col = f'{algo_name}_score'
                if score_col in output_df.columns:
                    # Count correct matches
                    if 'is_correct' in output_df.columns:
                        output_df[f'{algo_name}_correct'] = (output_df[score_col] >= 0.85) & output_df['is_correct']
                        correct_count = output_df[f'{algo_name}_correct'].sum()
                        total_count = output_df['is_correct'].count()
                        accuracy = correct_count / total_count if total_count > 0 else 0
                        
                        stats[algo_name] = {
                            'correct_matches': correct_count,
                            'total_rows': total_count,
                            'accuracy': accuracy,
                            'avg_score': output_df[score_col].mean(),
                            'avg_time_ms': output_df[f'{algo_name}_time_ms'].mean() if f'{algo_name}_time_ms' in output_df.columns else None
                        }
            
            # Add statistics to output
            stats_df = pd.DataFrame.from_dict(stats, orient='index')
            stats_df = stats_df.sort_values('accuracy', ascending=False)
            print("\nAlgorithm Performance:")
            print(stats_df)
            
            # Save statistics to separate sheet
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                output_df.to_excel(writer, sheet_name='Results', index=False)
                stats_df.to_excel(writer, sheet_name='Algorithm Stats')
        else:
            # Save to file
            if output_file.endswith('.csv'):
                output_df.to_csv(output_file, index=False)
            elif output_file.endswith(('.xls', '.xlsx')):
                output_df.to_excel(output_file, index=False)
            else:
                output_df.to_csv(output_file + '.csv', index=False)
        
        print(f"Processed {len(results)} records. Results saved to {output_file}")
```

## Main Function

```python
def main():
    try:
        # Check if file exists before loading
        import os
        print("Current directory:", os.getcwd())
        print("Files in directory:", os.listdir())
        
        if not os.path.exists("Acronym_Categorized.xlsx"):
            print("Warning: 'Acronym_Categorized.xlsx' not found in current directory!")
            
            # Create a sample file for testing
            print("Creating a sample Acronym_Categorized.xlsx file for testing...")
            data = {
                'Acronym': ['MCD', 'AMZN', 'SBUX'],
                'Full Name': ['McDonald\'s', 'Amazon', 'Starbucks'],
                'Merchant Category': ['Restaurant', 'Retail', 'Restaurant']
            }
            df = pd.DataFrame(data)
            df.to_excel("Acronym_Categorized.xlsx", index=False)
            print("Created sample file: Acronym_Categorized.xlsx")
        
        # Initialize system
        print("Initializing MerchantMatchingSystem...")
        matcher = MerchantMatchingSystem()
        
        # Load reference data
        print("Loading reference data...")
        matcher.load_reference_data("Acronym_Categorized.xlsx")
        
        # Save model for later use
        print("Saving model...")
        matcher.save_model("merchant_matcher.pkl")
        
        # Process with algorithm comparison
        print("Processing with algorithm comparison...")
        matcher.batch_process("Acronym_Categorized.xlsx", 
                            "algorithm_comparison_results.xlsx", 
                            run_all_algorithms=True)
        
        # Example of individual matching
        print("\nTesting MCD disambiguation by category:")
        
        # MCD as restaurant
        merchant_name = "MCD"
        merchant_category = "Restaurant"
        matches = matcher.find_matches(merchant_name, merchant_category, run_all_algorithms=True)
        
        print(f"\nMatches for '{merchant_name}' in category '{merchant_category}':")
        for match in matches:
            print(f"  - {match['merchant_name']} (Category: {match['merchant_category']}, Similarity: {match['similarity']:.4f})")
        
        # MCD as government
        merchant_name = "MCD"
        merchant_category = "Government"
        matches = matcher.find_matches(merchant_name, merchant_category, run_all_algorithms=True)
        
        print(f"\nMatches for '{merchant_name}' in category '{merchant_category}':")
        for match in matches:
            print(f"  - {match['merchant_name']} (Category: {match['merchant_category']}, Similarity: {match['similarity']:.4f})")
            
        print("\nSuccessfully completed all operations!")
        
    except Exception as e:
        import traceback
        print(f"Error: {str(e)}")
        traceback.print_exc()
```

## Run the Main Function

```python
if __name__ == "__main__":
    main()
```

## Explanation of the Changes

The code was failing due to several issues, which I've addressed with this rewritten solution:

1. **SentenceTransformer Scope Issue**
   - The original code had a scope issue where `SentenceTransformer` was imported at the top level but not available within the class methods.
   - In my solution, I've imported `SentenceTransformer` locally within the `__init__` method to ensure it's in the right scope.

2. **Graceful Fallback for Missing Models**
   - The code now handles the case where SentenceTransformer models aren't available by falling back to TF-IDF based similarity.
   - This makes the system more robust, allowing it to work even in environments without certain dependencies.

3. **Proper Error Handling**
   - I've added comprehensive error handling throughout the code, especially for model loading and embedding generation.
   - The system now provides helpful error messages and implements fallback strategies for key operations.

4. **File Path Verification**
   - The main function checks if required files exist and creates sample data if they don't.
   - This makes the code more robust when running in new environments.

5. **Safe Model Storage**
   - The `save_model` method now safely stores the model name without relying on attributes that might not exist.

This solution preserves the functionality of the original code while making it more resilient to environment variations and missing dependencies. The system will now gracefully degrade to simpler similarity measures when advanced options aren't available, ensuring it can still function in more restrictive environments.
