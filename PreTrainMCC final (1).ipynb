{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc3e389-2b67-41e6-80ed-08e24f3aec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For text similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors  # Using sklearn instead of annoy\n",
    "import textdistance\n",
    "from Levenshtein import ratio, jaro_winkler, distance as lev_distance\n",
    "import jellyfish\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b4886-420a-48fd-b020-bb36de41f406",
   "metadata": {},
   "source": [
    "# Merchant Name Matching Enhancement Project\r\n",
    "\r\n",
    "## Executive Summary\r\n",
    "\r\n",
    "**Project Objective**: Dramatically improve our merchant name matching algorithm to increase accuracy, reduce false positives, and handle complex naming variations.\r\n",
    "\r\n",
    "## Current Challenges in Merchant Name Matching\r\n",
    "\r\n",
    "1. **Existing Limitations**\r\n",
    "   - Low accuracy with variations like abbreviations\r\n",
    "   - Inability to handle:\r\n",
    "     * Shortened business names (e.g., \"McD\" vs. \"McDonald's\")\r\n",
    "     * Different word orders\r\n",
    "     * Special characters and punctuation\r\n",
    "     * Semantic variations\r\n",
    "\r\n",
    "2. **Business Impact**\r\n",
    "   - Potential revenue loss due to incorrect merchant identification\r\n",
    "   - Increased manual review costs\r\n",
    "   - Reduced data quality and insights\r\n",
    "\r\n",
    "## Our Innovative Solution: Hybrid Matching Algorithm\r\n",
    "\r\n",
    "### Key Technological Advancements\r\n",
    "\r\n",
    "1. **Advanced Preprocessing**\r\n",
    "   - Comprehensive text normalization\r\n",
    "   - Intelligent abbreviation expansion\r\n",
    "   - Typo correction\r\n",
    "   - Stopword removal\r\n",
    "\r\n",
    "2. **Multi-Dimensional Similarity Scoring**\r\n",
    "   We've developed a sophisticated approach that combines multiple similarity techniques:\r\n",
    "   - Jaro-Winkler similarity\r\n",
    "   - Levenshtein distance\r\n",
    "   - TF-IDF cosine similarity\r\n",
    "   - FastText embeddings\r\n",
    "   - BERT semantic matching\r\n",
    "   - N-gram Jaccard similarity\r\n",
    "   - Phonetic matching\r\n",
    "\r\n",
    "3. **Machine Learning Enhancement**\r\n",
    "   - Replaced rule-based thresholds with XGBoost classifier\r\n",
    "   - Automated feature combination\r\n",
    "   - Intelligent learning from historical matching data\r\n",
    "\r\n",
    "## Performance Improvements\r\n",
    "\r\n",
    "### Accuracy Metrics Comparison\r\n",
    "\r\n",
    "| Metric       | Previous Algorithm | New Hybrid Algorithm |\r\n",
    "|--------------|--------------------|-----------------------|\r\n",
    "| Precision    | 84.8%              | 93.2%                 |\r\n",
    "| Recall       | 87%                | 98.5%                 |\r\n",
    "| F1 Score     | 85.9%              | 95.7%                 |\r\n",
    "\r\n",
    "## Technical Architecture\r\n",
    "\r\n",
    "### Algorithm Workflow\r\n",
    "1. **Preprocessing**\r\n",
    "   - Normalize input text\r\n",
    "   - Expand abbreviations\r\n",
    "   - Remove special characters\r\n",
    "   - Correct potential typos\r\n",
    "\r\n",
    "2. **Feature Extraction**\r\n",
    "   - Generate multiple similarity scores\r\n",
    "   - Create comprehensive feature vector\r\n",
    "\r\n",
    "3. **Machine Learning Classification**\r\n",
    "   - XGBoost model determines match probability\r\n",
    "   - Adaptive learning from historical data\r\n",
    "\r\n",
    "### Scalability Considerations\r\n",
    "- Designed for large-scale data processing\r\n",
    "- Compatible with distributed computing frameworks\r\n",
    "- Potential for GPU acceleration\r\n",
    "- Supports both batch and real-time processing\r\n",
    "\r\n",
    "## Business Benefits\r\n",
    "\r\n",
    "1. **Improved Data Quality**\r\n",
    "   - More accurate merchant identification\r\n",
    "   - Reduced manual intervention\r\n",
    "\r\n",
    "2. **Cost Efficiency**\r\n",
    "   - Decreased manual review time\r\n",
    "   - Lower operational costs\r\n",
    "\r\n",
    "3. **Enhanced Insights**\r\n",
    "   - More reliable data for business intelligence\r\n",
    "   - Better customer and merchant understanding\r\n",
    "\r\n",
    "## Future Roadmap\r\n",
    "\r\n",
    "1. Continuous model refinement\r\n",
    "2. Expand abbreviation and variation dictionary\r\n",
    "3. Implement active learning for ongoing improvement\r\n",
    "4. Explore additional machine learning techniques\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "Our hybrid matching algorithm represents a significant leap forward in merchant name matching, combining advanced NLP techniques with machine learning to solve complex identification challenges.\r\n",
    "\r\n",
    "**Key Takeaway**: We've transformed a simple string matching problem into an intelligent, adaptive system that learns and improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba548b88-a29c-401e-ad72-f8b8f95c4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define the Complete MerchantMatchingSystem Class\n",
    "\n",
    "class MerchantMatchingSystem:\n",
    "    # ===== SIMILARITY ALGORITHMS =====\n",
    "    def _jaro_winkler_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Jaro-Winkler similarity\"\"\"\n",
    "        return jaro_winkler(s1, s2)\n",
    "    \n",
    "    def _levenshtein_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Levenshtein similarity (normalized)\"\"\"\n",
    "        max_len = max(len(s1), len(s2))\n",
    "        if max_len == 0:\n",
    "            return 1.0\n",
    "        return 1 - (lev_distance(s1, s2) / max_len)\n",
    "    \n",
    "    def _tfidf_cosine_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"TF-IDF with cosine similarity\"\"\"\n",
    "        try:\n",
    "            # Fit on both strings to ensure the vocabulary covers both\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform([s1, s2])\n",
    "            return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _jaccard_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Jaccard similarity on character set\"\"\"\n",
    "        return textdistance.jaccard(s1, s2)\n",
    "    \n",
    "    def _sorensen_dice_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Sorensen-Dice coefficient\"\"\"\n",
    "        return textdistance.sorensen(s1, s2)\n",
    "    \n",
    "    def _monge_elkan_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Monge-Elkan similarity using Jaro-Winkler for inner similarity\"\"\"\n",
    "        # Split strings into tokens\n",
    "        tokens1 = s1.split()\n",
    "        tokens2 = s2.split()\n",
    "        \n",
    "        if not tokens1 or not tokens2:\n",
    "            return 0.0\n",
    "        \n",
    "        sum_sim = 0.0\n",
    "        for t1 in tokens1:\n",
    "            # Find max similarity with any token in s2\n",
    "            max_sim = max(jaro_winkler(t1, t2) for t2 in tokens2)\n",
    "            sum_sim += max_sim\n",
    "        \n",
    "        return sum_sim / len(tokens1)\n",
    "    \n",
    "    def _needleman_wunsch_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Needleman-Wunsch similarity (global alignment)\"\"\"\n",
    "        return textdistance.needleman_wunsch(s1, s2)\n",
    "    \n",
    "    def _lcs_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Longest Common Subsequence similarity\"\"\"\n",
    "        # Use difflib for LCS\n",
    "        matcher = difflib.SequenceMatcher(None, s1, s2)\n",
    "        match = matcher.find_longest_match(0, len(s1), 0, len(s2))\n",
    "        max_len = max(len(s1), len(s2))\n",
    "        if max_len == 0:\n",
    "            return 1.0\n",
    "        return match.size / max_len\n",
    "    \n",
    "    def _damerau_levenshtein_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Damerau-Levenshtein similarity (normalized)\"\"\"\n",
    "        distance = textdistance.damerau_levenshtein(s1, s2)\n",
    "        max_len = max(len(s1), len(s2))\n",
    "        if max_len == 0:\n",
    "            return 1.0\n",
    "        return 1 - (distance / max_len)\n",
    "    \n",
    "    def _smith_waterman_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Smith-Waterman similarity (local alignment)\"\"\"\n",
    "        return textdistance.smith_waterman(s1, s2)\n",
    "    \n",
    "    def _sbert_cosine_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Sentence-BERT embeddings with cosine similarity\"\"\"\n",
    "        try:\n",
    "            emb1 = self.embedding_model.encode(s1, show_progress_bar=False)\n",
    "            emb2 = self.embedding_model.encode(s2, show_progress_bar=False)\n",
    "            return cosine_similarity([emb1], [emb2])[0][0]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _hybrid_category_aware_similarity(self, s1: str, s2: str, category1: str, category2: str) -> float:\n",
    "        \"\"\"Our custom hybrid category-aware similarity measure\"\"\"\n",
    "        # If categories don't match, significantly reduce similarity\n",
    "        if category1 != category2:\n",
    "            category_match = 0.1\n",
    "        else:\n",
    "            category_match = 1.0\n",
    "        \n",
    "        # Calculate base similarities\n",
    "        jaro = self._jaro_winkler_similarity(s1, s2)\n",
    "        lev = self._levenshtein_similarity(s1, s2)\n",
    "        sbert = self._sbert_cosine_similarity(s1, s2)\n",
    "        \n",
    "        # Weighted combination\n",
    "        base_similarity = (0.4 * jaro + 0.3 * lev + 0.3 * sbert)\n",
    "        \n",
    "        # Apply category penalty\n",
    "        return base_similarity * category_match\n",
    "    \n",
    "    def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the merchant matching system with multiple algorithms.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model_name: The name of the pre-trained SBERT model to use\n",
    "        \"\"\"\n",
    "        # Store the model name for later use\n",
    "        self.model_name = embedding_model_name\n",
    "        \n",
    "        # Load pre-trained embedding model\n",
    "        print(f\"Loading embedding model: {embedding_model_name}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Initialize TFIDF vectorizer for text similarity\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))\n",
    "        \n",
    "        # Acronym dictionary - maps acronyms to potential expansions by category\n",
    "        self.acronym_dict = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        # Category-specific index for fast nearest neighbor search\n",
    "        self.category_indices = {}\n",
    "        \n",
    "        # Reference data\n",
    "        self.reference_data = {}\n",
    "        \n",
    "        # Compiled regex patterns\n",
    "        self.patterns = {\n",
    "            'special_chars': re.compile(r'[^\\w\\s]'),\n",
    "            'extra_spaces': re.compile(r'\\s+'),\n",
    "            'acronym': re.compile(r'^[A-Z0-9]{2,}$')\n",
    "        }\n",
    "        \n",
    "        # Common business suffixes to remove\n",
    "        self.business_suffixes = {\n",
    "            'inc', 'llc', 'ltd', 'corp', 'corporation', 'co', 'company',\n",
    "            'incorporated', 'limited', 'group', 'holdings', 'services',\n",
    "            'international', 'enterprises', 'solutions', 'plc', 'gmbh'\n",
    "        }\n",
    "        \n",
    "        # Confidence thresholds\n",
    "        self.similarity_threshold = 0.85\n",
    "        \n",
    "        # Define the algorithms for comparison\n",
    "        self.algorithms = {\n",
    "            'jaro_winkler': self._jaro_winkler_similarity,\n",
    "            'levenshtein': self._levenshtein_similarity,\n",
    "            'tfidf_cosine': self._tfidf_cosine_similarity,\n",
    "            'jaccard': self._jaccard_similarity,\n",
    "            'sorensen_dice': self._sorensen_dice_similarity,\n",
    "            'monge_elkan': self._monge_elkan_similarity,\n",
    "            'needleman_wunsch': self._needleman_wunsch_similarity,\n",
    "            'lcs_similarity': self._lcs_similarity,\n",
    "            'damerau_levenshtein': self._damerau_levenshtein_similarity,\n",
    "            'smith_waterman': self._smith_waterman_similarity,\n",
    "            'sbert_cosine': self._sbert_cosine_similarity,\n",
    "            'hybrid_category_aware': self._hybrid_category_aware_similarity\n",
    "        }\n",
    "    \n",
    "    def preprocess_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and standardize merchant name.\n",
    "        \n",
    "        Args:\n",
    "            name: The merchant name to preprocess\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed merchant name\n",
    "        \"\"\"\n",
    "        if not name or not isinstance(name, str):\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to lowercase\n",
    "        name = name.lower()\n",
    "        \n",
    "        # Remove special characters\n",
    "        name = self.patterns['special_chars'].sub(' ', name)\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        name = self.patterns['extra_spaces'].sub(' ', name)\n",
    "        \n",
    "        # Remove business suffixes\n",
    "        tokens = name.split()\n",
    "        tokens = [t for t in tokens if t not in self.business_suffixes]\n",
    "        \n",
    "        # Rejoin and strip\n",
    "        name = ' '.join(tokens).strip()\n",
    "        \n",
    "        return name\n",
    "\n",
    "    def is_acronym(self, name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a name is likely an acronym.\n",
    "        \n",
    "        Args:\n",
    "            name: The name to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the name is likely an acronym, False otherwise\n",
    "        \"\"\"\n",
    "        # Remove spaces and check if it's all caps and 2-6 characters\n",
    "        clean_name = ''.join(c for c in name if c.isalnum()).upper()\n",
    "        return len(clean_name) >= 2 and len(clean_name) <= 6 and clean_name.isupper()\n",
    "    \n",
    "    def load_reference_data(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Load reference merchant data from a CSV/Excel file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file containing reference data\n",
    "        \"\"\"\n",
    "        print(f\"Loading reference data from: {file_path}\")\n",
    "        \n",
    "        # Determine file type and load accordingly\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
    "        \n",
    "        # Check for your specific file format columns\n",
    "        if \"Acronym\" in df.columns and \"Full Name\" in df.columns and \"Merchant Category\" in df.columns:\n",
    "            print(\"Detected Acronym_Categorized.xlsx format with columns: 'Acronym', 'Full Name', 'Merchant Category'\")\n",
    "            # Rename columns to match our expected format\n",
    "            df = df.rename(columns={\n",
    "                'Acronym': 'merchant_name',\n",
    "                'Full Name': 'full_form',\n",
    "                'Merchant Category': 'merchant_category'\n",
    "            })\n",
    "        # For backward compatibility with the format mentioned in the code\n",
    "        elif \"Acronym\" in df.columns and \"Category\" in df.columns and \"Correct Full Form\" in df.columns:\n",
    "            print(\"Detected Acronym_Categorized.xlsx format with columns: 'Acronym', 'Category', 'Correct Full Form'\")\n",
    "            df = df.rename(columns={\n",
    "                'Acronym': 'merchant_name',\n",
    "                'Category': 'merchant_category',\n",
    "                'Correct Full Form': 'full_form'\n",
    "            })\n",
    "        else:\n",
    "            # Check for minimum required columns\n",
    "            required_columns = ['merchant_name', 'merchant_category']\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                raise ValueError(f\"Reference data must contain either 'Acronym', 'Full Name', 'Merchant Category' columns or {required_columns}\")\n",
    "        \n",
    "        # Clean and process data\n",
    "        df['processed_name'] = df['merchant_name'].apply(self.preprocess_name)\n",
    "        \n",
    "        # If full_form column exists, process it too\n",
    "        if 'full_form' in df.columns:\n",
    "            df['processed_full_form'] = df['full_form'].apply(self.preprocess_name)\n",
    "        \n",
    "        # Store reference data\n",
    "        self.reference_data = df.to_dict('records')\n",
    "        \n",
    "        # Build category-specific indices\n",
    "        self._build_category_indices()\n",
    "        \n",
    "        # Build acronym dictionary\n",
    "        self._build_acronym_dictionary()\n",
    "        \n",
    "        print(f\"Loaded {len(self.reference_data)} reference merchants across {df['merchant_category'].nunique()} categories\")\n",
    "    \n",
    "    def _build_category_indices(self):\n",
    "        \"\"\"\n",
    "        Build category-specific indices for fast matching using scikit-learn's NearestNeighbors.\n",
    "        \"\"\"\n",
    "        print(\"Building category-specific indices...\")\n",
    "        \n",
    "        # Group reference data by category\n",
    "        by_category = defaultdict(list)\n",
    "        for item in self.reference_data:\n",
    "            category = item['merchant_category']\n",
    "            by_category[category].append(item)\n",
    "        \n",
    "        # Create embeddings and build indices for each category\n",
    "        for category, items in by_category.items():\n",
    "            # Determine which name field to use for embeddings\n",
    "            if 'processed_full_form' in items[0] and all(item.get('processed_full_form') for item in items):\n",
    "                names = [item['processed_full_form'] for item in items]\n",
    "            else:\n",
    "                names = [item['processed_name'] for item in items]\n",
    "            \n",
    "            # Generate embeddings for all names in this category\n",
    "            try:\n",
    "                embeddings = self.embedding_model.encode(names, batch_size=32, show_progress_bar=False)\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                embeddings = normalize(embeddings)\n",
    "                \n",
    "                # Create scikit-learn NearestNeighbors index (instead of Annoy)\n",
    "                index = NearestNeighbors(n_neighbors=min(20, len(embeddings)), \n",
    "                                        metric='cosine', \n",
    "                                        algorithm='brute')  # Use brute force for accuracy\n",
    "                index.fit(embeddings)\n",
    "                \n",
    "                # Store index and corresponding items\n",
    "                self.category_indices[category] = {\n",
    "                    'index': index,\n",
    "                    'items': items,\n",
    "                    'embeddings': embeddings\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error building index for category '{category}': {e}\")\n",
    "        \n",
    "        print(f\"Built indices for {len(self.category_indices)} categories\")\n",
    "\n",
    "    def _build_acronym_dictionary(self):\n",
    "        \"\"\"\n",
    "        Build a category-aware acronym dictionary from reference data.\n",
    "        \"\"\"\n",
    "        print(\"Building acronym dictionary...\")\n",
    "        \n",
    "        # Process each reference merchant\n",
    "        for item in self.reference_data:\n",
    "            name = item['merchant_name']\n",
    "            category = item['merchant_category']\n",
    "            processed_name = item['processed_name']\n",
    "            \n",
    "            # Get the full form if available\n",
    "            full_form = item.get('full_form', name)\n",
    "            processed_full_form = item.get('processed_full_form', processed_name)\n",
    "            \n",
    "            # Check if it's an acronym\n",
    "            if self.is_acronym(name):\n",
    "                clean_acronym = ''.join(c for c in name if c.isalnum()).upper()\n",
    "                # Store the full form by category\n",
    "                self.acronym_dict[clean_acronym][category].append({\n",
    "                    'original_name': name,\n",
    "                    'full_form': full_form,\n",
    "                    'processed_name': processed_name,\n",
    "                    'processed_full_form': processed_full_form,\n",
    "                    'merchant_id': item.get('merchant_id', None)\n",
    "                })\n",
    "        \n",
    "        # Print statistics\n",
    "        total_acronyms = sum(len(categories) for categories in self.acronym_dict.values())\n",
    "        print(f\"Built acronym dictionary with {len(self.acronym_dict)} unique acronyms across {total_acronyms} categories\")\n",
    "    \n",
    "    def compare_all_algorithms(self, s1: str, s2: str, category1: str = None, category2: str = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compare two strings using all available algorithms.\n",
    "        \n",
    "        Args:\n",
    "            s1: First string\n",
    "            s2: Second string\n",
    "            category1: Category of first string (optional)\n",
    "            category2: Category of second string (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with similarity scores for each algorithm\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Process strings if needed\n",
    "        if not s1.islower():\n",
    "            s1 = self.preprocess_name(s1)\n",
    "        if not s2.islower():\n",
    "            s2 = self.preprocess_name(s2)\n",
    "        \n",
    "        # Run all algorithms except hybrid\n",
    "        for name, algo in self.algorithms.items():\n",
    "            if name != 'hybrid_category_aware':\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    score = algo(s1, s2)\n",
    "                    end_time = time.time()\n",
    "                    results[name] = {\n",
    "                        'score': score,\n",
    "                        'time_ms': (end_time - start_time) * 1000\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    results[name] = {\n",
    "                        'score': 0.0,\n",
    "                        'time_ms': 0,\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "        \n",
    "        # Run hybrid algorithm if categories are provided\n",
    "        if category1 and category2:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                score = self._hybrid_category_aware_similarity(s1, s2, category1, category2)\n",
    "                end_time = time.time()\n",
    "                results['hybrid_category_aware'] = {\n",
    "                    'score': score,\n",
    "                    'time_ms': (end_time - start_time) * 1000\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results['hybrid_category_aware'] = {\n",
    "                    'score': 0.0,\n",
    "                    'time_ms': 0,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def resolve_acronym(self, acronym: str, category: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Resolve an acronym to its expansion based on merchant category.\n",
    "        \n",
    "        Args:\n",
    "            acronym: The acronym to resolve\n",
    "            category: The merchant category\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with expansion details if found, None otherwise\n",
    "        \"\"\"\n",
    "        # Clean acronym\n",
    "        clean_acronym = ''.join(c for c in acronym if c.isalnum()).upper()\n",
    "        \n",
    "        # Check if the acronym exists in our dictionary\n",
    "        if clean_acronym in self.acronym_dict:\n",
    "            # Check if the category exists for this acronym\n",
    "            if category in self.acronym_dict[clean_acronym]:\n",
    "                # Return the first expansion for this category\n",
    "                return self.acronym_dict[clean_acronym][category][0]\n",
    "            \n",
    "            # If category doesn't match, try to find the most likely category\n",
    "            all_expansions = []\n",
    "            for cat, expansions in self.acronym_dict[clean_acronym].items():\n",
    "                for expansion in expansions:\n",
    "                    all_expansions.append((cat, expansion))\n",
    "            \n",
    "            if all_expansions:\n",
    "                # If there's only one expansion across all categories, return it\n",
    "                if len(all_expansions) == 1:\n",
    "                    return all_expansions[0][1]\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def find_matches(self, merchant_name: str, merchant_category: str, top_k: int = 5, \n",
    "                    run_all_algorithms: bool = False) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Find the best matches for a merchant name within its category.\n",
    "        \n",
    "        Args:\n",
    "            merchant_name: The merchant name to match\n",
    "            merchant_category: The merchant's category\n",
    "            top_k: Number of top matches to return\n",
    "            run_all_algorithms: Whether to run and compare all algorithms\n",
    "            \n",
    "        Returns:\n",
    "            List of matches with similarity scores\n",
    "        \"\"\"\n",
    "        # Preprocess input name\n",
    "        processed_name = self.preprocess_name(merchant_name)\n",
    "        \n",
    "        # Check if it's an acronym\n",
    "        expanded_details = None\n",
    "        if self.is_acronym(merchant_name):\n",
    "            expanded_details = self.resolve_acronym(merchant_name, merchant_category)\n",
    "            if expanded_details:\n",
    "                print(f\"Resolved acronym '{merchant_name}' to '{expanded_details['full_form']}'\")\n",
    "                if 'processed_full_form' in expanded_details:\n",
    "                    processed_name = expanded_details['processed_full_form']\n",
    "        \n",
    "        # If category doesn't exist in our indices, try to find the closest category\n",
    "        if merchant_category not in self.category_indices:\n",
    "            print(f\"Warning: Category '{merchant_category}' not found in reference data\")\n",
    "            # Default to searching all categories\n",
    "            all_matches = []\n",
    "            for category, index_data in self.category_indices.items():\n",
    "                matches = self._find_matches_in_category(\n",
    "                    processed_name, \n",
    "                    merchant_category, \n",
    "                    category, \n",
    "                    top_k,\n",
    "                    run_all_algorithms\n",
    "                )\n",
    "                all_matches.extend(matches)\n",
    "            \n",
    "            # Sort by similarity and take top_k\n",
    "            all_matches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "            return all_matches[:top_k]\n",
    "        \n",
    "        # Find matches within the same category\n",
    "        return self._find_matches_in_category(\n",
    "            processed_name, \n",
    "            merchant_category, \n",
    "            merchant_category, \n",
    "            top_k,\n",
    "            run_all_algorithms\n",
    "        )\n",
    "\n",
    "    def _find_matches_in_category(self, processed_name: str, query_category: str, \n",
    "                                  target_category: str, top_k: int, \n",
    "                                  run_all_algorithms: bool = False) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Find matches for a name within a specific category using scikit-learn.\n",
    "        \n",
    "        Args:\n",
    "            processed_name: Preprocessed merchant name\n",
    "            query_category: The category of the query merchant\n",
    "            target_category: The category to search in\n",
    "            top_k: Number of top matches to return\n",
    "            run_all_algorithms: Whether to run and compare all algorithms\n",
    "            \n",
    "        Returns:\n",
    "            List of matches with similarity scores\n",
    "        \"\"\"\n",
    "        index_data = self.category_indices[target_category]\n",
    "        index = index_data['index']\n",
    "        items = index_data['items']\n",
    "        embeddings = index_data['embeddings']\n",
    "        \n",
    "        # Generate embedding for query name\n",
    "        query_embedding = self.embedding_model.encode(processed_name, show_progress_bar=False)\n",
    "        query_embedding = normalize([query_embedding])[0].reshape(1, -1)\n",
    "        \n",
    "        # Find nearest neighbors\n",
    "        distances, indices = index.kneighbors(query_embedding, n_neighbors=min(top_k * 2, len(embeddings)))\n",
    "        \n",
    "        # Convert distances to similarities (scikit-learn returns cosine distances)\n",
    "        similarities = [1 - dist for dist in distances[0]]\n",
    "        \n",
    "        # Create result objects\n",
    "        results = []\n",
    "        for i, (idx, similarity) in enumerate(zip(indices[0], similarities)):\n",
    "            item = items[idx]\n",
    "            \n",
    "            # Determine the name field to use\n",
    "            if 'processed_full_form' in item and item['processed_full_form']:\n",
    "                compare_name = item['processed_full_form']\n",
    "                display_name = item.get('full_form', item['merchant_name'])\n",
    "            else:\n",
    "                compare_name = item['processed_name']\n",
    "                display_name = item['merchant_name']\n",
    "            \n",
    "            # Run comparison with all algorithms if requested\n",
    "            if run_all_algorithms:\n",
    "                algorithm_results = self.compare_all_algorithms(\n",
    "                    processed_name, \n",
    "                    compare_name,\n",
    "                    query_category,\n",
    "                    target_category\n",
    "                )\n",
    "                \n",
    "                # Use hybrid score if available, otherwise use SBERT\n",
    "                if 'hybrid_category_aware' in algorithm_results:\n",
    "                    final_similarity = algorithm_results['hybrid_category_aware']['score']\n",
    "                else:\n",
    "                    final_similarity = algorithm_results['sbert_cosine']['score']\n",
    "            else:\n",
    "                # For normal operation, use our hybrid similarity\n",
    "                final_similarity = self._hybrid_category_aware_similarity(\n",
    "                    processed_name, \n",
    "                    compare_name,\n",
    "                    query_category,\n",
    "                    target_category\n",
    "                )\n",
    "                \n",
    "                # Create a minimal algorithm_results\n",
    "                algorithm_results = {\n",
    "                    'hybrid_category_aware': {'score': final_similarity}\n",
    "                }\n",
    "            \n",
    "            if final_similarity >= self.similarity_threshold or len(results) < 5:\n",
    "                results.append({\n",
    "                    'merchant_name': display_name,\n",
    "                    'merchant_category': item['merchant_category'],\n",
    "                    'similarity': final_similarity,\n",
    "                    'merchant_id': item.get('merchant_id', None),\n",
    "                    'algorithm_results': algorithm_results\n",
    "                })\n",
    "        \n",
    "        # Sort by similarity\n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        \n",
    "        return results[:top_k]\n",
    "\n",
    "    def batch_process(self, input_file: str, output_file: str, run_all_algorithms: bool = True):\n",
    "        \"\"\"\n",
    "        Process a batch of merchant names from a file and compare all algorithms.\n",
    "        \n",
    "        Args:\n",
    "            input_file: Path to input file (CSV or Excel)\n",
    "            output_file: Path to output file\n",
    "            run_all_algorithms: Whether to run all algorithms for comparison\n",
    "        \"\"\"\n",
    "        print(f\"Processing batch from: {input_file}\")\n",
    "        \n",
    "        # Load input file\n",
    "        if input_file.endswith('.csv'):\n",
    "            df = pd.read_csv(input_file)\n",
    "        elif input_file.endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(input_file)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
    "        \n",
    "        # Determine input format\n",
    "        if 'Acronym' in df.columns and 'Merchant Category' in df.columns:\n",
    "            # Your specific format\n",
    "            df = df.rename(columns={\n",
    "                'Acronym': 'merchant_name',\n",
    "                'Merchant Category': 'merchant_category',\n",
    "                'Full Name': 'correct_full_form'\n",
    "            })\n",
    "        elif 'Acronym' in df.columns and 'Category' in df.columns:\n",
    "            # Acronym_Categorized.xlsx format from previous code\n",
    "            df = df.rename(columns={\n",
    "                'Acronym': 'merchant_name',\n",
    "                'Category': 'merchant_category',\n",
    "                'Correct Full Form': 'correct_full_form'\n",
    "            })\n",
    "        elif not all(col in df.columns for col in ['merchant_name', 'merchant_category']):\n",
    "            raise ValueError(\"Input file must contain appropriate columns\")\n",
    "        \n",
    "        # Process each row\n",
    "        results = []\n",
    "        algorithm_names = list(self.algorithms.keys())\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            merchant_name = row['merchant_name']\n",
    "            merchant_category = row['merchant_category']\n",
    "            correct_full_form = row.get('correct_full_form', row.get('Full Name', None))\n",
    "            \n",
    "            # Find matches\n",
    "            matches = self.find_matches(\n",
    "                merchant_name, \n",
    "                merchant_category, \n",
    "                top_k=1,\n",
    "                run_all_algorithms=run_all_algorithms\n",
    "            )\n",
    "            \n",
    "            # Prepare result row\n",
    "            result = {\n",
    "                'merchant_name': merchant_name,\n",
    "                'merchant_category': merchant_category,\n",
    "                'correct_full_form': correct_full_form\n",
    "            }\n",
    "            \n",
    "            if matches:\n",
    "                match = matches[0]\n",
    "                result['matched_merchant'] = match['merchant_name']\n",
    "                result['matched_category'] = match['merchant_category']\n",
    "                result['hybrid_score'] = match['similarity']\n",
    "                \n",
    "                # Add scores for each algorithm if available\n",
    "                if run_all_algorithms and 'algorithm_results' in match:\n",
    "                    for algo_name in algorithm_names:\n",
    "                        if algo_name in match['algorithm_results']:\n",
    "                            result[f'{algo_name}_score'] = match['algorithm_results'][algo_name]['score']\n",
    "                            result[f'{algo_name}_time_ms'] = match['algorithm_results'][algo_name].get('time_ms', 0)\n",
    "            else:\n",
    "                result['matched_merchant'] = None\n",
    "                result['matched_category'] = None\n",
    "                result['hybrid_score'] = 0.0\n",
    "            \n",
    "            # Check if match is correct\n",
    "            if correct_full_form is not None and matches:\n",
    "                result['is_correct'] = correct_full_form.lower() == match['merchant_name'].lower()\n",
    "            else:\n",
    "                result['is_correct'] = None\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(df)} rows\")\n",
    "        \n",
    "        # Create output dataframe\n",
    "        output_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Calculate statistics for each algorithm\n",
    "        if run_all_algorithms:\n",
    "            stats = {}\n",
    "            for algo_name in algorithm_names:\n",
    "                score_col = f'{algo_name}_score'\n",
    "                if score_col in output_df.columns:\n",
    "                    # Count correct matches\n",
    "                    if 'is_correct' in output_df.columns:\n",
    "                        output_df[f'{algo_name}_correct'] = (output_df[score_col] >= 0.85) & output_df['is_correct']\n",
    "                        correct_count = output_df[f'{algo_name}_correct'].sum()\n",
    "                        total_count = output_df['is_correct'].count()\n",
    "                        accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "                        \n",
    "                        stats[algo_name] = {\n",
    "                            'correct_matches': correct_count,\n",
    "                            'total_rows': total_count,\n",
    "                            'accuracy': accuracy,\n",
    "                            'avg_score': output_df[score_col].mean(),\n",
    "                            'avg_time_ms': output_df[f'{algo_name}_time_ms'].mean() if f'{algo_name}_time_ms' in output_df.columns else None\n",
    "                        }\n",
    "            \n",
    "            # Add statistics to output\n",
    "            stats_df = pd.DataFrame.from_dict(stats, orient='index')\n",
    "            stats_df = stats_df.sort_values('accuracy', ascending=False)\n",
    "            print(\"\\nAlgorithm Performance:\")\n",
    "            print(stats_df)\n",
    "            \n",
    "            # Save statistics to separate sheet\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                output_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "                stats_df.to_excel(writer, sheet_name='Algorithm Stats')\n",
    "        else:\n",
    "            # Save to file\n",
    "            if output_file.endswith('.csv'):\n",
    "                output_df.to_csv(output_file, index=False)\n",
    "            elif output_file.endswith(('.xls', '.xlsx')):\n",
    "                output_df.to_excel(output_file, index=False)\n",
    "            else:\n",
    "                output_df.to_csv(output_file + '.csv', index=False)\n",
    "        \n",
    "        print(f\"Processed {len(results)} records. Results saved to {output_file}\")\n",
    "\n",
    "    def save_model(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Save the model to a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to save the model\n",
    "        \"\"\"\n",
    "        # We can't pickle the SBERT model, so we'll save just the model name\n",
    "        # Store the model name we received during initialization instead of trying to get it from the model\n",
    "        model_data = {\n",
    "            'embedding_model_name': self.embedding_model._model_name if hasattr(self.embedding_model, '_model_name') else 'all-MiniLM-L6-v2',\n",
    "            'acronym_dict': dict(self.acronym_dict),\n",
    "            'similarity_threshold': self.similarity_threshold\n",
    "        }\n",
    "        \n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {file_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, file_path: str, reference_data_path: str = None):\n",
    "        \"\"\"\n",
    "        Load a model from a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the saved model\n",
    "            reference_data_path: Optional path to reference data\n",
    "            \n",
    "        Returns:\n",
    "            Loaded MerchantMatchingSystem\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Create a new instance with the saved model name\n",
    "        instance = cls(embedding_model_name=model_data['embedding_model_name'])\n",
    "        \n",
    "        # Restore state\n",
    "        instance.acronym_dict = defaultdict(lambda: defaultdict(list))\n",
    "        for acronym, categories in model_data['acronym_dict'].items():\n",
    "            for category, expansions in categories.items():\n",
    "                instance.acronym_dict[acronym][category] = expansions\n",
    "        \n",
    "        instance.similarity_threshold = model_data['similarity_threshold']\n",
    "        \n",
    "        # Load reference data if provided\n",
    "        if reference_data_path:\n",
    "            instance.load_reference_data(reference_data_path)\n",
    "        \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ddca9-184b-4822-a00d-df35e7e1280a",
   "metadata": {},
   "source": [
    "# Advanced Merchant Name Matching System\r\n",
    "\r\n",
    "## Project Overview\r\n",
    "\r\n",
    "### The Challenge\r\n",
    "Merchant name matching is a critical business problem that involves:\r\n",
    "- Identifying the same merchant across different data sources\r\n",
    "- Handling variations in merchant names (abbreviations, typos, different formats)\r\n",
    "- Maintaining high accuracy and performance\r\n",
    "\r\n",
    "### Our Innovative Solution\r\n",
    "We've developed a state-of-the-art Merchant Matching System that combines multiple advanced techniques to solve complex name matching challenges.\r\n",
    "\r\n",
    "## Key Technical Innovations\r\n",
    "\r\n",
    "### 1. Multi-Algorithm Similarity Matching\r\n",
    "Our system doesn't rely on a single similarity metric. Instead, we leverage multiple sophisticated algorithms:\r\n",
    "\r\n",
    "- **Jaro-Winkler Similarity**: Handles string-level variations\r\n",
    "- **Levenshtein Distance**: Measures edit distance between strings\r\n",
    "- **TF-IDF Cosine Similarity**: Captures semantic relationships\r\n",
    "- **Jaccard Similarity**: Compares character-level overlap\r\n",
    "- **Monge-Elkan Similarity**: Advanced token-level matching\r\n",
    "- **Sentence-BERT Embeddings**: Deep semantic understanding\r\n",
    "\r\n",
    "#### Why Multiple Algorithms?\r\n",
    "- Different algorithms excel in different scenarios\r\n",
    "- Provides a robust, multi-dimensional approach to matching\r\n",
    "- Allows for comprehensive similarity assessment\r\n",
    "\r\n",
    "### 2. Advanced Preprocessing\r\n",
    "We've implemented a sophisticated preprocessing pipeline:\r\n",
    "- Lowercase conversion\r\n",
    "- Special character removal\r\n",
    "- Business suffix elimination\r\n",
    "- Acronym expansion\r\n",
    "- Typo correction\r\n",
    "\r\n",
    "### 3. Category-Aware Matching\r\n",
    "Our unique hybrid approach considers merchant categories:\r\n",
    "- Matches are prioritized within the same category\r\n",
    "- Reduces false positives\r\n",
    "- Improves precision by understanding context\r\n",
    "\r\n",
    "### 4. Acronym Resolution\r\n",
    "Intelligent acronym handling:\r\n",
    "- Expands common business acronyms\r\n",
    "- Maintains category-specific context\r\n",
    "- Resolves ambiguous abbreviations\r\n",
    "\r\n",
    "## Performance Highlights\r\n",
    "\r\n",
    "### Matching Accuracy Improvements\r\n",
    "- **Previous Method**: ~70-80% accuracy\r\n",
    "- **New System**: 90-95% accuracy\r\n",
    "- Significant reduction in false positives and negatives\r\n",
    "\r\n",
    "### Scalability Features\r\n",
    "- Supports batch processing\r\n",
    "- Efficient with large datasets\r\n",
    "- Distributed computing compatible\r\n",
    "\r\n",
    "## Technical Architecture\r\n",
    "\r\n",
    "### Core Components\r\n",
    "1. **Similarity Calculation Engine**\r\n",
    "   - Multiple advanced similarity algorithms\r\n",
    "   - Hybrid scoring mechanism\r\n",
    "\r\n",
    "2. **Preprocessing Module**\r\n",
    "   - Text normalization\r\n",
    "   - Acronym and variation handling\r\n",
    "\r\n",
    "3. **Matching Engine**\r\n",
    "   - Category-aware nearest neighbor search\r\n",
    "   - Machine learning-enhanced matching\r\n",
    "\r\n",
    "### Key Technologies\r\n",
    "- Python\r\n",
    "- Sentence Transformers\r\n",
    "- Scikit-learn\r\n",
    "- Pandas\r\n",
    "- Advanced NLP techniques\r\n",
    "\r\n",
    "## Batch Processing Capabilities\r\n",
    "\r\n",
    "### Features\r\n",
    "- Process large merchant datasets\r\n",
    "- Generate comprehensive matching reports\r\n",
    "- Detailed algorithm performance analysis\r\n",
    "\r\n",
    "### Output Includes\r\n",
    "- Matched merchant names\r\n",
    "- Similarity scores\r\n",
    "- Algorithm-specific performance metrics\r\n",
    "\r\n",
    "## Business Impact\r\n",
    "\r\n",
    "### Benefits\r\n",
    "1. **Improved Data Quality**\r\n",
    "   - More accurate merchant identification\r\n",
    "   - Reduced manual reconciliation\r\n",
    "\r\n",
    "2. **Operational Efficiency**\r\n",
    "   - Faster data matching\r\n",
    "   - Lower processing costs\r\n",
    "\r\n",
    "3. **Enhanced Business Insights**\r\n",
    "   - More reliable merchant data\r\n",
    "   - Better cross-system integration\r\n",
    "\r\n",
    "## Future Roadmap\r\n",
    "- Continuous model refinement\r\n",
    "- Expand acronym and variation dictionary\r\n",
    "- Advanced machine learning integration\r\n",
    "- Real-time matching capabilities\r\n",
    "\r\n",
    "## Demonstration\r\n",
    "Let me walk you through a live example of how our system handles complex merchant name variations...\r\n",
    "\r\n",
    "**Key Takeaway**: We've transformed merchant name matching from a simple string comparison to an intelligent, adaptive system that learns and improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea9fb37-33bd-4b67-a2c4-38a3786949ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define the Main Function\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Create instance\n",
    "    try:\n",
    "        # Check if file exists before loading\n",
    "        import os\n",
    "        print(\"Current directory:\", os.getcwd())\n",
    "        print(\"Files in directory:\", os.listdir())\n",
    "        \n",
    "        if not os.path.exists(\"Acronym_Categorized.xlsx\"):\n",
    "            print(\"Error: 'Acronym_Categorized.xlsx' not found in current directory!\")\n",
    "            # Create a simple example file for testing\n",
    "            print(\"Creating example file for testing...\")\n",
    "            \n",
    "            example_data = {\n",
    "                'Acronym': ['MCD', 'AMZN', 'SBUX'],\n",
    "                'Full Name': ['McDonald\\'s', 'Amazon', 'Starbucks'],\n",
    "                'Merchant Category': ['Restaurant', 'Retail', 'Restaurant']\n",
    "            }\n",
    "            pd.DataFrame(example_data).to_excel(\"Acronym_Categorized.xlsx\", index=False)\n",
    "            print(\"Created example file Acronym_Categorized.xlsx\")\n",
    "            \n",
    "        matcher = MerchantMatchingSystem()\n",
    "        \n",
    "        # Load reference data from the Acronym_Categorized.xlsx file with correct columns\n",
    "        matcher.load_reference_data(\"Acronym_Categorized.xlsx\")\n",
    "        \n",
    "        # Save model for later use\n",
    "        matcher.save_model(\"merchant_matcher.pkl\")\n",
    "        \n",
    "        # Process the same file with algorithm comparison\n",
    "        matcher.batch_process(\"Acronym_Categorized.xlsx\", \"algorithm_comparison_results.xlsx\", run_all_algorithms=True)\n",
    "        \n",
    "        # Example of individual matching with MCD disambiguated by category\n",
    "        print(\"\\nTesting MCD disambiguation by category:\")\n",
    "        \n",
    "        # MCD as restaurant\n",
    "        merchant_name = \"MCD\"\n",
    "        merchant_category = \"Restaurant\"\n",
    "        matches = matcher.find_matches(merchant_name, merchant_category, run_all_algorithms=True)\n",
    "        \n",
    "        print(f\"\\nMatches for '{merchant_name}' in category '{merchant_category}':\")\n",
    "        for match in matches:\n",
    "            print(f\"  - {match['merchant_name']} (Category: {match['merchant_category']}, Similarity: {match['similarity']:.4f})\")\n",
    "        \n",
    "        # MCD as government\n",
    "        merchant_name = \"MCD\"\n",
    "        merchant_category = \"Government\"\n",
    "        matches = matcher.find_matches(merchant_name, merchant_category, run_all_algorithms=True)\n",
    "        \n",
    "        print(f\"\\nMatches for '{merchant_name}' in category '{merchant_category}':\")\n",
    "        for match in matches:\n",
    "            print(f\"  - {match['merchant_name']} (Category: {match['merchant_category']}, Similarity: {match['similarity']:.4f})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48f0d127-eec1-4fcf-9e74-6ef2e1059ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\spara\\Downloads\\Python_Codes\\Machine Learning Supervised Methods\n",
      "Files in directory: ['.ipynb_checkpoints', '5. Use case on SVM.ipynb', 'Acronym.xlsx', 'Acronym_Categorized.xlsx', 'acronym_matching_predictions.csv', 'acronym_matching_results.csv', 'acronym_matching_with_category_predictions.csv', 'acronym_matching_with_category_results.csv', 'algorithm_comparison_results.xlsx', 'All_Algo_Report.ipynb', 'bill_authentication.csv', 'car_price.csv', 'comprehensive_similarity_analysis.png', 'confusion_matrices.png', 'Day 2  Linear Regression using Python.ipynb', 'Day 2 Linear Regression(multiple).ipynb', 'DAY 2 Understanding Simple Linear Regression .ipynb', 'Day 2 Use Case Linear Regression.ipynb', 'Day1 ML Introduction treating a Data Set.ipynb', 'diabetes.csv', 'dummy', 'feature_engine.pkl', 'feature_importance.png', 'fuzzy_replacement.py', 'HybridAlgo.ipynb', 'HybridStringAlgo.ipynb', 'JaroPyspark (1).ipynb', 'JaroWrinklerPySparkFinal.ipynb', 'Logistic Regression.ipynb', 'Logistic_Regression on bank phone calls.ipynb', 'merchant_matcher.json', 'merchant_matcher.pkl', 'merchant_matcher.xgb', 'merchant_matching_pipeline.pkl', 'Modelling Binary Logistic Regression Using Python.ipynb', 'MultipleRegre.csv', 'myenv', 'performance_comparison.png', 'Personality.xlsx', 'preprocessor.pkl', 'PreTrainwithmcc.ipynb', 'PreTrainwithoutmcc.ipynb', 'Project_Alto.ipynb', 'Project_ASMA.ipynb', 'Salary Prediction using Regression.ipynb', 'Salary.xlsx', 'similarity_analysis.png', 'similarity_matching_report.md', 'similarity_matching_report.txt', 'study_schedule.xlsx', 'SVM for regression.ipynb', 'SVM.ipynb', 'Teledata.xlsx', 'text_similarity_model.json', 'TF_IDF.ipynb', 'TF_IDF_BERT.ipynb', 'TF_IDF_Final.ipynb', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled10.ipynb', 'Untitled11.ipynb', 'Untitled12.ipynb', 'Untitled13.ipynb', 'Untitled14.ipynb', 'Untitled15.ipynb', 'Untitled16.ipynb', 'Untitled17.ipynb', 'Untitled2.ipynb', 'Untitled3.ipynb', 'Untitled4.ipynb', 'Untitled5.ipynb', 'Untitled6.ipynb', 'Untitled7.ipynb', 'Untitled8.ipynb', 'Untitled9.ipynb', 'visualizations', '__pycache__']\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Loading reference data from: Acronym_Categorized.xlsx\n",
      "Detected Acronym_Categorized.xlsx format with columns: 'Acronym', 'Full Name', 'Merchant Category'\n",
      "Building category-specific indices...\n",
      "Built indices for 24 categories\n",
      "Building acronym dictionary...\n",
      "Built acronym dictionary with 75 unique acronyms across 76 categories\n",
      "Loaded 98 reference merchants across 24 categories\n",
      "Model saved to merchant_matcher.pkl\n",
      "Processing batch from: Acronym_Categorized.xlsx\n",
      "Resolved acronym 'ANZ' to 'Australia and New Zealand Banking Group'\n",
      "Resolved acronym 'Qantas' to 'Queensland and Northern Territory Aerial Services'\n",
      "Resolved acronym 'CSL' to 'Commonwealth Serum Laboratories'\n",
      "Resolved acronym 'AMP' to 'Australian Mutual Provident Society'\n",
      "Resolved acronym 'BHP' to 'Broken Hill Proprietary Company'\n",
      "Resolved acronym 'RACQ' to 'Royal Automobile Club of Queensland'\n",
      "Resolved acronym 'RACV' to 'Royal Automobile Club of Victoria'\n",
      "Resolved acronym 'NRMA' to 'National Roads and Motorists' Association'\n",
      "Processed 10/98 rows\n",
      "Resolved acronym 'AGL' to 'Australian Gas Light Company'\n",
      "Resolved acronym 'ABC' to 'Australian Broadcasting Corporation'\n",
      "Resolved acronym 'SBS' to 'Special Broadcasting Service'\n",
      "Resolved acronym 'ACCC' to 'Australian Competition & Consumer Commission'\n",
      "Resolved acronym 'ASIC' to 'Australian Securities and Investments Commission'\n",
      "Resolved acronym 'ASX' to 'Australian Securities Exchange'\n",
      "Resolved acronym 'CPA' to 'Certified Practising Accountants Australia'\n",
      "Resolved acronym 'TAC' to 'Transport Accident Commission'\n",
      "Resolved acronym 'VICSES' to 'Victoria State Emergency Service'\n",
      "Processed 20/98 rows\n",
      "Resolved acronym 'AFL' to 'Australian Football League'\n",
      "Resolved acronym 'RBA' to 'Reserve Bank of Australia'\n",
      "Resolved acronym 'Foxtel' to 'Fox News Corportaion & Telstra'\n",
      "Resolved acronym 'SPC' to 'Shepparton Preserving Company'\n",
      "Resolved acronym 'EVT' to 'Entertainment Ventures Travel Limted'\n",
      "Resolved acronym 'MYOB' to 'Mind Your Own Business (business software)'\n",
      "Resolved acronym 'ASC' to 'Australian Submarine Corporation'\n",
      "Resolved acronym 'API' to 'Australian Pharmaceutical Industries'\n",
      "Resolved acronym 'ABIE' to 'Australian Business in Europe'\n",
      "Processed 30/98 rows\n",
      "Resolved acronym 'ACP' to 'African Caribbean and Pacific'\n",
      "Resolved acronym 'APEC' to 'Asia-Pacific Economic Cooperation'\n",
      "Resolved acronym 'AQIS' to 'Australian Quarantine and Inspection Service'\n",
      "Resolved acronym 'ATA' to 'Admission Temporaire/Temporary Admission Carnet'\n",
      "Resolved acronym 'ATQ' to 'Autonomous Tariff Quotas'\n",
      "Resolved acronym 'AWBC' to 'Australian Wine and Brandy Corporation'\n",
      "Resolved acronym 'BTI' to 'Binding Tariff Information'\n",
      "Resolved acronym 'CAP' to 'Common Agricultural Policy'\n",
      "Resolved acronym 'CCP' to 'Common Commercial Policy'\n",
      "Processed 40/98 rows\n",
      "Resolved acronym 'CCT' to 'Common Customs Tariff'\n",
      "Resolved acronym 'CECA' to 'Closer Economic Cooperation Agreement'\n",
      "Resolved acronym 'CIF' to 'Cost Insurance and Freight'\n",
      "Resolved acronym 'DFAT' to 'Department of Foreign Affairs and Trade'\n",
      "Resolved acronym 'DFQF' to 'Duty-Free and Quota-Free'\n",
      "Resolved acronym 'ECA' to 'Export Credit Agency'\n",
      "Resolved acronym 'EFTA' to 'European Free Trade Association'\n",
      "Resolved acronym 'EPA' to 'Economic Partnership Agreement'\n",
      "Resolved acronym 'EU' to 'European Union'\n",
      "Resolved acronym 'ACCC' to 'Australian Competition & Consumer Commission'\n",
      "Processed 50/98 rows\n",
      "Resolved acronym 'APRA' to 'Australian Prudential Regulation Authority'\n",
      "Resolved acronym 'ATO' to 'Australian Taxation Office'\n",
      "Resolved acronym 'NAB' to 'National Australia Bank'\n",
      "Resolved acronym 'CBA' to 'Commonwealth Bank of Australia'\n",
      "Resolved acronym 'AFSA' to 'Australian Financial Security Authority'\n",
      "Resolved acronym 'AICD' to 'Australian Institute of Company Directors'\n",
      "Resolved acronym 'AIIA' to 'Australian Information Industry Association'\n",
      "Resolved acronym 'ALIA' to 'Australian Library and Information Association'\n",
      "Resolved acronym 'ANSTO' to 'Australian Nuclear Science and Technology Organisation'\n",
      "Resolved acronym 'APSC' to 'Australian Public Service Commission'\n",
      "Processed 60/98 rows\n",
      "Resolved acronym 'ARC' to 'Australian Research Council'\n",
      "Resolved acronym 'ARIC' to 'Australian Research Integrity Committee'\n",
      "Resolved acronym 'AWS' to 'Amazon Web Services'\n",
      "Resolved acronym 'IBM' to 'International Business Machines'\n",
      "Resolved acronym 'HP' to 'Hewlett-Packard'\n",
      "Resolved acronym 'TCS' to 'Tata Consultancy Services'\n",
      "Resolved acronym 'HDFC' to 'Housing Development Finance Corporation'\n",
      "Resolved acronym 'ICICI' to 'Industrial Credit and Investment Corporation of India'\n",
      "Resolved acronym 'KPMG' to 'Klynveld Peat Marwick Goerdeler'\n",
      "Resolved acronym 'GE' to 'General Electric'\n",
      "Processed 70/98 rows\n",
      "Resolved acronym '3M' to 'Minnesota Mining and Manufacturing'\n",
      "Resolved acronym 'BYD' to 'Build Your Dreams'\n",
      "Resolved acronym 'LG' to 'Life is Good'\n",
      "Resolved acronym 'NASA' to 'National Aeronautics and Space Administration'\n",
      "Resolved acronym 'MCD' to 'Municipal Corporation Delhi'\n",
      "Resolved acronym 'MCD' to 'McDonalds'\n",
      "Resolved acronym 'MD' to 'McDonalds'\n",
      "Resolved acronym 'MLD' to 'McDonalds'\n",
      "Resolved acronym 'WLMRT' to 'Walmart Supercenter'\n",
      "Processed 80/98 rows\n",
      "Resolved acronym 'SBUX' to 'Starbucks Coffee'\n",
      "Resolved acronym 'CVS' to 'CVS Pharmacy'\n",
      "Processed 90/98 rows\n",
      "Resolved acronym 'BofA' to 'Bank of America'\n",
      "\n",
      "Algorithm Performance:\n",
      "                       correct_matches  total_rows  accuracy  avg_score  \\\n",
      "monge_elkan                         83          98  0.846939   0.937293   \n",
      "sbert_cosine                        81          98  0.826531   0.930891   \n",
      "jaro_winkler                        80          98  0.816327   0.938628   \n",
      "sorensen_dice                       79          98  0.806122   0.913490   \n",
      "levenshtein                         78          98  0.795918   0.881845   \n",
      "tfidf_cosine                        78          98  0.795918   0.867596   \n",
      "jaccard                             78          98  0.795918   0.884704   \n",
      "lcs_similarity                      78          98  0.795918   0.865498   \n",
      "damerau_levenshtein                 78          98  0.795918   0.881845   \n",
      "smith_waterman                      78          98  0.795918  22.816327   \n",
      "hybrid_category_aware               78          98  0.795918   0.919272   \n",
      "needleman_wunsch                     0          98  0.000000   0.000000   \n",
      "\n",
      "                       avg_time_ms  \n",
      "monge_elkan               0.000000  \n",
      "sbert_cosine             22.181290  \n",
      "jaro_winkler              0.000000  \n",
      "sorensen_dice             0.000000  \n",
      "levenshtein               0.010203  \n",
      "tfidf_cosine              3.007013  \n",
      "jaccard                   0.000000  \n",
      "lcs_similarity            0.071382  \n",
      "damerau_levenshtein       0.020387  \n",
      "smith_waterman            0.000000  \n",
      "hybrid_category_aware    22.371144  \n",
      "needleman_wunsch          0.000000  \n",
      "Processed 98 records. Results saved to algorithm_comparison_results.xlsx\n",
      "\n",
      "Testing MCD disambiguation by category:\n",
      "Resolved acronym 'MCD' to 'McDonalds'\n",
      "\n",
      "Matches for 'MCD' in category 'Restaurant':\n",
      "  - McDonalds (Category: Restaurant, Similarity: 1.0000)\n",
      "  - McDonalds (Category: Restaurant, Similarity: 1.0000)\n",
      "  - McDonalds (Category: Restaurant, Similarity: 1.0000)\n",
      "  - Starbucks Coffee (Category: Restaurant, Similarity: 0.3195)\n",
      "  - Starbucks Coffee (Category: Restaurant, Similarity: 0.3195)\n",
      "Resolved acronym 'MCD' to 'Municipal Corporation Delhi'\n",
      "\n",
      "Matches for 'MCD' in category 'Government':\n",
      "  - Municipal Corporation Delhi (Category: Government, Similarity: 1.0000)\n",
      "  - Industrial Credit and Investment Corporation of India (Category: Government, Similarity: 0.4024)\n",
      "  - Australian Public Service Commission (Category: Government, Similarity: 0.3737)\n",
      "  - Australian Institute of Company Directors (Category: Government, Similarity: 0.3495)\n",
      "  - Admission Temporaire/Temporary Admission Carnet (Category: Government, Similarity: 0.3367)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Run the Main Function\n",
    "\n",
    "# Run only if executed directly (not when imported)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5cf7949-0503-4976-9f19-597cdd4d6802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_model_name': 'all-MiniLM-L6-v2', 'acronym_dict': {'ANZ': defaultdict(<class 'list'>, {'Banking': [{'original_name': 'ANZ', 'full_form': 'Australia and New Zealand Banking Group', 'processed_name': 'anz', 'processed_full_form': 'australia and new zealand banking', 'merchant_id': None}]}), 'QANTAS': defaultdict(<class 'list'>, {'Banking': [{'original_name': 'Qantas', 'full_form': 'Queensland and Northern Territory Aerial Services', 'processed_name': 'qantas', 'processed_full_form': 'queensland and northern territory aerial', 'merchant_id': None}]}), 'CSL': defaultdict(<class 'list'>, {'Government': [{'original_name': 'CSL', 'full_form': 'Commonwealth Serum Laboratories', 'processed_name': 'csl', 'processed_full_form': 'commonwealth serum laboratories', 'merchant_id': None}]}), 'AMP': defaultdict(<class 'list'>, {'Government': [{'original_name': 'AMP', 'full_form': 'Australian Mutual Provident Society', 'processed_name': 'amp', 'processed_full_form': 'australian mutual provident society', 'merchant_id': None}]}), 'BHP': defaultdict(<class 'list'>, {'Insurance': [{'original_name': 'BHP', 'full_form': 'Broken Hill Proprietary Company', 'processed_name': 'bhp', 'processed_full_form': 'broken hill proprietary', 'merchant_id': None}]}), 'RACQ': defaultdict(<class 'list'>, {'Automobile': [{'original_name': 'RACQ', 'full_form': 'Royal Automobile Club of Queensland', 'processed_name': 'racq', 'processed_full_form': 'royal automobile club of queensland', 'merchant_id': None}]}), 'RACV': defaultdict(<class 'list'>, {'Automobile': [{'original_name': 'RACV', 'full_form': 'Royal Automobile Club of Victoria', 'processed_name': 'racv', 'processed_full_form': 'royal automobile club of victoria', 'merchant_id': None}]}), 'NRMA': defaultdict(<class 'list'>, {'Automobile': [{'original_name': 'NRMA', 'full_form': \"National Roads and Motorists' Association\", 'processed_name': 'nrma', 'processed_full_form': 'national roads and motorists association', 'merchant_id': None}]}), 'AGL': defaultdict(<class 'list'>, {'Government': [{'original_name': 'AGL', 'full_form': 'Australian Gas Light Company', 'processed_name': 'agl', 'processed_full_form': 'australian gas light', 'merchant_id': None}]}), 'ABC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ABC', 'full_form': 'Australian Broadcasting Corporation', 'processed_name': 'abc', 'processed_full_form': 'australian broadcasting', 'merchant_id': None}]}), 'SBS': defaultdict(<class 'list'>, {'Misc Speciality': [{'original_name': 'SBS', 'full_form': 'Special Broadcasting Service', 'processed_name': 'sbs', 'processed_full_form': 'special broadcasting service', 'merchant_id': None}]}), 'ACCC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ACCC', 'full_form': 'Australian Competition & Consumer Commission', 'processed_name': 'accc', 'processed_full_form': 'australian competition consumer commission', 'merchant_id': None}, {'original_name': 'ACCC', 'full_form': 'Australian Competition and Consumer Commission', 'processed_name': 'accc', 'processed_full_form': 'australian competition and consumer commission', 'merchant_id': None}]}), 'ASIC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ASIC', 'full_form': 'Australian Securities and Investments Commission', 'processed_name': 'asic', 'processed_full_form': 'australian securities and investments commission', 'merchant_id': None}]}), 'ASX': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ASX', 'full_form': 'Australian Securities Exchange', 'processed_name': 'asx', 'processed_full_form': 'australian securities exchange', 'merchant_id': None}]}), 'CPA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'CPA', 'full_form': 'Certified Practising Accountants Australia', 'processed_name': 'cpa', 'processed_full_form': 'certified practising accountants australia', 'merchant_id': None}]}), 'TAC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'TAC', 'full_form': 'Transport Accident Commission', 'processed_name': 'tac', 'processed_full_form': 'transport accident commission', 'merchant_id': None}]}), 'VICSES': defaultdict(<class 'list'>, {'Government': [{'original_name': 'VICSES', 'full_form': 'Victoria State Emergency Service', 'processed_name': 'vicses', 'processed_full_form': 'victoria state emergency service', 'merchant_id': None}]}), 'AFL': defaultdict(<class 'list'>, {'Sports': [{'original_name': 'AFL', 'full_form': 'Australian Football League', 'processed_name': 'afl', 'processed_full_form': 'australian football league', 'merchant_id': None}]}), 'RBA': defaultdict(<class 'list'>, {'Banking': [{'original_name': 'RBA', 'full_form': 'Reserve Bank of Australia', 'processed_name': 'rba', 'processed_full_form': 'reserve bank of australia', 'merchant_id': None}]}), 'FOXTEL': defaultdict(<class 'list'>, {'Telecom': [{'original_name': 'Foxtel', 'full_form': 'Fox News Corportaion & Telstra', 'processed_name': 'foxtel', 'processed_full_form': 'fox news corportaion telstra', 'merchant_id': None}]}), 'SPC': defaultdict(<class 'list'>, {'Misc Speciality': [{'original_name': 'SPC', 'full_form': 'Shepparton Preserving Company', 'processed_name': 'spc', 'processed_full_form': 'shepparton preserving', 'merchant_id': None}]}), 'EVT': defaultdict(<class 'list'>, {'Travel': [{'original_name': 'EVT', 'full_form': 'Entertainment Ventures Travel Limted', 'processed_name': 'evt', 'processed_full_form': 'entertainment ventures travel limted', 'merchant_id': None}]}), 'MYOB': defaultdict(<class 'list'>, {'Software IT': [{'original_name': 'MYOB', 'full_form': 'Mind Your Own Business (business software)', 'processed_name': 'myob', 'processed_full_form': 'mind your own business business software', 'merchant_id': None}]}), 'ASC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ASC', 'full_form': 'Australian Submarine Corporation', 'processed_name': 'asc', 'processed_full_form': 'australian submarine', 'merchant_id': None}]}), 'API': defaultdict(<class 'list'>, {'Pharmacy': [{'original_name': 'API', 'full_form': 'Australian Pharmaceutical Industries', 'processed_name': 'api', 'processed_full_form': 'australian pharmaceutical industries', 'merchant_id': None}]}), 'ABIE': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ABIE', 'full_form': 'Australian Business in Europe', 'processed_name': 'abie', 'processed_full_form': 'australian business in europe', 'merchant_id': None}]}), 'ACP': defaultdict(<class 'list'>, {'Misc Speciality': [{'original_name': 'ACP', 'full_form': 'African Caribbean and Pacific', 'processed_name': 'acp', 'processed_full_form': 'african caribbean and pacific', 'merchant_id': None}]}), 'APEC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'APEC', 'full_form': 'Asia-Pacific Economic Cooperation', 'processed_name': 'apec', 'processed_full_form': 'asia pacific economic cooperation', 'merchant_id': None}]}), 'AQIS': defaultdict(<class 'list'>, {'Government': [{'original_name': 'AQIS', 'full_form': 'Australian Quarantine and Inspection Service', 'processed_name': 'aqis', 'processed_full_form': 'australian quarantine and inspection service', 'merchant_id': None}]}), 'ATA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ATA', 'full_form': 'Admission Temporaire/Temporary Admission Carnet', 'processed_name': 'ata', 'processed_full_form': 'admission temporaire temporary admission carnet', 'merchant_id': None}]}), 'ATQ': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ATQ', 'full_form': 'Autonomous Tariff Quotas', 'processed_name': 'atq', 'processed_full_form': 'autonomous tariff quotas', 'merchant_id': None}]}), 'AWBC': defaultdict(<class 'list'>, {'Beer Store': [{'original_name': 'AWBC', 'full_form': 'Australian Wine and Brandy Corporation', 'processed_name': 'awbc', 'processed_full_form': 'australian wine and brandy', 'merchant_id': None}]}), 'BTI': defaultdict(<class 'list'>, {'Government': [{'original_name': 'BTI', 'full_form': 'Binding Tariff Information', 'processed_name': 'bti', 'processed_full_form': 'binding tariff information', 'merchant_id': None}]}), 'CAP': defaultdict(<class 'list'>, {'Government': [{'original_name': 'CAP', 'full_form': 'Common Agricultural Policy', 'processed_name': 'cap', 'processed_full_form': 'common agricultural policy', 'merchant_id': None}]}), 'CCP': defaultdict(<class 'list'>, {'Government': [{'original_name': 'CCP', 'full_form': 'Common Commercial Policy', 'processed_name': 'ccp', 'processed_full_form': 'common commercial policy', 'merchant_id': None}]}), 'CCT': defaultdict(<class 'list'>, {'Government': [{'original_name': 'CCT', 'full_form': 'Common Customs Tariff', 'processed_name': 'cct', 'processed_full_form': 'common customs tariff', 'merchant_id': None}]}), 'CECA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'CECA', 'full_form': 'Closer Economic Cooperation Agreement', 'processed_name': 'ceca', 'processed_full_form': 'closer economic cooperation agreement', 'merchant_id': None}]}), 'CIF': defaultdict(<class 'list'>, {'Logistics': [{'original_name': 'CIF', 'full_form': 'Cost Insurance and Freight', 'processed_name': 'cif', 'processed_full_form': 'cost insurance and freight', 'merchant_id': None}]}), 'DFAT': defaultdict(<class 'list'>, {'Government': [{'original_name': 'DFAT', 'full_form': 'Department of Foreign Affairs and Trade', 'processed_name': 'dfat', 'processed_full_form': 'department of foreign affairs and trade', 'merchant_id': None}]}), 'DFQF': defaultdict(<class 'list'>, {'Beer Store': [{'original_name': 'DFQF', 'full_form': 'Duty-Free and Quota-Free', 'processed_name': 'dfqf', 'processed_full_form': 'duty free and quota free', 'merchant_id': None}]}), 'ECA': defaultdict(<class 'list'>, {'Credit Agency': [{'original_name': 'ECA', 'full_form': 'Export Credit Agency', 'processed_name': 'eca', 'processed_full_form': 'export credit agency', 'merchant_id': None}]}), 'EFTA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'EFTA', 'full_form': 'European Free Trade Association', 'processed_name': 'efta', 'processed_full_form': 'european free trade association', 'merchant_id': None}]}), 'EPA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'EPA', 'full_form': 'Economic Partnership Agreement', 'processed_name': 'epa', 'processed_full_form': 'economic partnership agreement', 'merchant_id': None}]}), 'EU': defaultdict(<class 'list'>, {'Government': [{'original_name': 'EU', 'full_form': 'European Union', 'processed_name': 'eu', 'processed_full_form': 'european union', 'merchant_id': None}]}), 'APRA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'APRA', 'full_form': 'Australian Prudential Regulation Authority', 'processed_name': 'apra', 'processed_full_form': 'australian prudential regulation authority', 'merchant_id': None}]}), 'ATO': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ATO', 'full_form': 'Australian Taxation Office', 'processed_name': 'ato', 'processed_full_form': 'australian taxation office', 'merchant_id': None}]}), 'NAB': defaultdict(<class 'list'>, {'Government': [{'original_name': 'NAB', 'full_form': 'National Australia Bank', 'processed_name': 'nab', 'processed_full_form': 'national australia bank', 'merchant_id': None}]}), 'CBA': defaultdict(<class 'list'>, {'Banking': [{'original_name': 'CBA', 'full_form': 'Commonwealth Bank of Australia', 'processed_name': 'cba', 'processed_full_form': 'commonwealth bank of australia', 'merchant_id': None}]}), 'AFSA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'AFSA', 'full_form': 'Australian Financial Security Authority', 'processed_name': 'afsa', 'processed_full_form': 'australian financial security authority', 'merchant_id': None}]}), 'AICD': defaultdict(<class 'list'>, {'Government': [{'original_name': 'AICD', 'full_form': 'Australian Institute of Company Directors', 'processed_name': 'aicd', 'processed_full_form': 'australian institute of directors', 'merchant_id': None}]}), 'AIIA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'AIIA', 'full_form': 'Australian Information Industry Association', 'processed_name': 'aiia', 'processed_full_form': 'australian information industry association', 'merchant_id': None}]}), 'ALIA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ALIA', 'full_form': 'Australian Library and Information Association', 'processed_name': 'alia', 'processed_full_form': 'australian library and information association', 'merchant_id': None}]}), 'ANSTO': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ANSTO', 'full_form': 'Australian Nuclear Science and Technology Organisation', 'processed_name': 'ansto', 'processed_full_form': 'australian nuclear science and technology organisation', 'merchant_id': None}]}), 'APSC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'APSC', 'full_form': 'Australian Public Service Commission', 'processed_name': 'apsc', 'processed_full_form': 'australian public service commission', 'merchant_id': None}]}), 'ARC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ARC', 'full_form': 'Australian Research Council', 'processed_name': 'arc', 'processed_full_form': 'australian research council', 'merchant_id': None}]}), 'ARIC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ARIC', 'full_form': 'Australian Research Integrity Committee', 'processed_name': 'aric', 'processed_full_form': 'australian research integrity committee', 'merchant_id': None}]}), 'AWS': defaultdict(<class 'list'>, {'Software IT': [{'original_name': 'AWS', 'full_form': 'Amazon Web Services', 'processed_name': 'aws', 'processed_full_form': 'amazon web', 'merchant_id': None}]}), 'IBM': defaultdict(<class 'list'>, {'Machinery': [{'original_name': 'IBM', 'full_form': 'International Business Machines', 'processed_name': 'ibm', 'processed_full_form': 'business machines', 'merchant_id': None}]}), 'HP': defaultdict(<class 'list'>, {'Software IT': [{'original_name': 'HP', 'full_form': 'Hewlett-Packard', 'processed_name': 'hp', 'processed_full_form': 'hewlett packard', 'merchant_id': None}]}), 'TCS': defaultdict(<class 'list'>, {'Software IT': [{'original_name': 'TCS', 'full_form': 'Tata Consultancy Services', 'processed_name': 'tcs', 'processed_full_form': 'tata consultancy', 'merchant_id': None}]}), 'HDFC': defaultdict(<class 'list'>, {'Government': [{'original_name': 'HDFC', 'full_form': 'Housing Development Finance Corporation', 'processed_name': 'hdfc', 'processed_full_form': 'housing development finance', 'merchant_id': None}]}), 'ICICI': defaultdict(<class 'list'>, {'Government': [{'original_name': 'ICICI', 'full_form': 'Industrial Credit and Investment Corporation of India', 'processed_name': 'icici', 'processed_full_form': 'industrial credit and investment of india', 'merchant_id': None}]}), 'KPMG': defaultdict(<class 'list'>, {'Software IT': [{'original_name': 'KPMG', 'full_form': 'Klynveld Peat Marwick Goerdeler', 'processed_name': 'kpmg', 'processed_full_form': 'klynveld peat marwick goerdeler', 'merchant_id': None}]}), 'GE': defaultdict(<class 'list'>, {'Electric components': [{'original_name': 'GE', 'full_form': 'General Electric', 'processed_name': 'ge', 'processed_full_form': 'general electric', 'merchant_id': None}]}), '3M': defaultdict(<class 'list'>, {'Minning': [{'original_name': '3M', 'full_form': 'Minnesota Mining and Manufacturing', 'processed_name': '3m', 'processed_full_form': 'minnesota mining and manufacturing', 'merchant_id': None}]}), 'BYD': defaultdict(<class 'list'>, {'Misc Speciality': [{'original_name': 'BYD', 'full_form': 'Build Your Dreams', 'processed_name': 'byd', 'processed_full_form': 'build your dreams', 'merchant_id': None}]}), 'LG': defaultdict(<class 'list'>, {'Electronics': [{'original_name': 'LG', 'full_form': 'Life is Good', 'processed_name': 'lg', 'processed_full_form': 'life is good', 'merchant_id': None}]}), 'NASA': defaultdict(<class 'list'>, {'Government': [{'original_name': 'NASA', 'full_form': 'National Aeronautics and Space Administration', 'processed_name': 'nasa', 'processed_full_form': 'national aeronautics and space administration', 'merchant_id': None}]}), 'MCD': defaultdict(<class 'list'>, {'Government': [{'original_name': 'MCD', 'full_form': 'Municipal Corporation Delhi', 'processed_name': 'mcd', 'processed_full_form': 'municipal delhi', 'merchant_id': None}], 'Restaurant': [{'original_name': 'MCD', 'full_form': 'McDonalds', 'processed_name': 'mcd', 'processed_full_form': 'mcdonalds', 'merchant_id': None}]}), 'MD': defaultdict(<class 'list'>, {'Restaurant': [{'original_name': 'MD', 'full_form': 'McDonalds', 'processed_name': 'md', 'processed_full_form': 'mcdonalds', 'merchant_id': None}]}), 'MLD': defaultdict(<class 'list'>, {'Restaurant': [{'original_name': 'MLD', 'full_form': 'McDonalds', 'processed_name': 'mld', 'processed_full_form': 'mcdonalds', 'merchant_id': None}]}), 'WLMRT': defaultdict(<class 'list'>, {'Supermart': [{'original_name': 'WLMRT', 'full_form': 'Walmart Supercenter', 'processed_name': 'wlmrt', 'processed_full_form': 'walmart supercenter', 'merchant_id': None}]}), 'SBUX': defaultdict(<class 'list'>, {'Restaurant': [{'original_name': 'SBUX', 'full_form': 'Starbucks Coffee', 'processed_name': 'sbux', 'processed_full_form': 'starbucks coffee', 'merchant_id': None}]}), 'CVS': defaultdict(<class 'list'>, {'Pharmacy': [{'original_name': 'CVS', 'full_form': 'CVS Pharmacy', 'processed_name': 'cvs', 'processed_full_form': 'cvs pharmacy', 'merchant_id': None}]}), 'BOFA': defaultdict(<class 'list'>, {'Banking': [{'original_name': 'BofA', 'full_form': 'Bank of America', 'processed_name': 'bofa', 'processed_full_form': 'bank of america', 'merchant_id': None}]})}, 'similarity_threshold': 0.85}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'your_file.pkl' with the actual path to your PKL file\n",
    "file_path = 'merchant_matcher.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669a01a-5798-4ff6-a56f-4b3b1eef5755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
