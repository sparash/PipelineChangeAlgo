{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52de6ee-f6a8-4f9b-ad75-d6ae1619d082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library available for BERT embeddings\n",
      "Warning: pyahocorasick not available. Using fallback implementation.\n",
      "Using device: cpu\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from Levenshtein import jaro_winkler, ratio as levenshtein_ratio\n",
    "import textdistance\n",
    "from fuzzywuzzy import fuzz\n",
    "import jellyfish\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    transformers_available = True\n",
    "    print(\"Transformers library available for BERT embeddings\")\n",
    "except ImportError:\n",
    "    transformers_available = False\n",
    "    print(\"Warning: transformers library not available. Will use TF-IDF fallback.\")\n",
    "\n",
    "# Try to import pyahocorasick with fallback\n",
    "try:\n",
    "    import pyahocorasick\n",
    "    aho_corasick_available = True\n",
    "    print(\"pyahocorasick is available\")\n",
    "except ImportError:\n",
    "    print(\"Warning: pyahocorasick not available. Using fallback implementation.\")\n",
    "    aho_corasick_available = False\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4c49a8-68b7-4faf-b471-a8c3d90393f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Enhanced BERT Embedder with MPNet Model\n",
    "\n",
    "class EnhancedBERTEmbedder:\n",
    "    \"\"\"\n",
    "    Enhanced BERT embedder using the more powerful MPNet model for better semantic understanding.\n",
    "    Implements advanced pooling strategies, merchant category adaptation, and batching for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2', pooling_strategy='mean', device=None):\n",
    "        \"\"\"\n",
    "        Initialize enhanced BERT embedder with specified pre-trained model and pooling strategy.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained BERT model to use\n",
    "            pooling_strategy (str): Pooling strategy ('mean', 'cls', or 'max')\n",
    "            device: Device to run the model on (cuda or cpu)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.max_sequence_length = 512  # BERT's limit\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.initialized = False\n",
    "        self.category_adapted = False \n",
    "        self.category_embeddings = {}  # Storage for category-specific embeddings\n",
    "        \n",
    "        # Initialize pre-trained model if transformers available\n",
    "        if transformers_available:\n",
    "            try:\n",
    "                print(f\"Loading enhanced BERT model '{model_name}'...\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "                self.model.eval()  # Set to evaluation mode\n",
    "                self.initialized = True\n",
    "                print(f\"Enhanced BERT model loaded successfully on {self.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing BERT model: {e}\")\n",
    "                self.initialized = False\n",
    "        \n",
    "        # Initialize TF-IDF fallback if BERT not available\n",
    "        if not self.initialized:\n",
    "            # Using character n-grams for better handling of typos and abbreviations\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4))\n",
    "            self.tfidf_fitted = False\n",
    "            print(\"Using TF-IDF fallback for embeddings\")\n",
    "    \n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling - take average of all token embeddings\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]  # First element contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def _cls_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        CLS pooling - use the [CLS] token embedding\n",
    "        \"\"\"\n",
    "        return model_output[0][:, 0]\n",
    "    \n",
    "    def _max_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Max pooling - take max of all token embeddings\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "        return torch.max(token_embeddings, 1)[0]\n",
    "    \n",
    "    def _get_pooled_embeddings(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Apply the selected pooling strategy\n",
    "        \"\"\"\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            return self._mean_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'cls':\n",
    "            return self._cls_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            return self._max_pooling(model_output, attention_mask)\n",
    "        else:\n",
    "            # Default to mean pooling\n",
    "            return self._mean_pooling(model_output, attention_mask)\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        Fit the TF-IDF vectorizer on a corpus of texts (only needed for TF-IDF fallback)\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            # Fit TF-IDF vectorizer\n",
    "            self.tfidf_vectorizer.fit(texts)\n",
    "            self.tfidf_fitted = True\n",
    "            print(\"TF-IDF vectorizer fitted on corpus\")\n",
    "\n",
    "    \n",
    "    def encode(self, texts, batch_size=32, show_progress=False, merchant_category=None):\n",
    "        \"\"\"\n",
    "        Encode texts into embeddings using the pre-trained model,\n",
    "        optionally considering merchant category information\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts or single text\n",
    "            batch_size: Batch size for processing\n",
    "            show_progress: Whether to show progress\n",
    "            merchant_category: Optional merchant category to influence encoding\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Embeddings for the texts\n",
    "        \"\"\"\n",
    "        # Handle single text input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Return empty array for empty input\n",
    "        if len(texts) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Use pre-trained BERT if available\n",
    "        if self.initialized:\n",
    "            # Process in batches\n",
    "            all_embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                if show_progress and i % (batch_size * 10) == 0:\n",
    "                    print(f\"Processing batch {i//batch_size + 1}/{(len(texts)//batch_size) + 1}\")\n",
    "                \n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize\n",
    "                encoded_input = self.tokenizer(\n",
    "                    batch_texts, \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=self.max_sequence_length,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Compute token embeddings\n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(**encoded_input)\n",
    "                    batch_embeddings = self._get_pooled_embeddings(model_output, encoded_input['attention_mask'])\n",
    "                    all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            \n",
    "            embeddings = np.vstack(all_embeddings)\n",
    "            \n",
    "            # Apply merchant category adaptation if available\n",
    "            if self.category_adapted and merchant_category and merchant_category in self.category_embeddings:\n",
    "                # Blend with category embedding (subtle influence)\n",
    "                category_embedding = self.category_embeddings[merchant_category]\n",
    "                alpha = 0.1  # Blend factor - subtle influence\n",
    "                embeddings = (1 - alpha) * embeddings + alpha * category_embedding\n",
    "                \n",
    "                # Re-normalize\n",
    "                embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "            \n",
    "            return embeddings\n",
    "        \n",
    "        else:\n",
    "            # Use TF-IDF fallback\n",
    "            if not self.tfidf_fitted:\n",
    "                self.fit(texts)\n",
    "            \n",
    "            return self.tfidf_vectorizer.transform(texts).toarray()\n",
    "    \n",
    "    def compute_similarity(self, text1, text2, merchant_category1=None, merchant_category2=None):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts using the pre-trained model,\n",
    "        optionally considering merchant category information\n",
    "        \n",
    "        Args:\n",
    "            text1: First text\n",
    "            text2: Second text\n",
    "            merchant_category1: Merchant category for first text\n",
    "            merchant_category2: Merchant category for second text\n",
    "            \n",
    "        Returns:\n",
    "            float: Cosine similarity score\n",
    "        \"\"\"\n",
    "        # Get embeddings for both texts, considering merchant categories\n",
    "        emb1 = self.encode(text1, merchant_category=merchant_category1)\n",
    "        emb2 = self.encode(text2, merchant_category=merchant_category2)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        return np.sum(emb1 * emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae4f9ea-b1b1-46da-b14d-26a8bb3270d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Enhanced Merchant Matcher Core Class\n",
    "\n",
    "class EnhancedMerchantMatcher:\n",
    "    \"\"\"\n",
    "    Enhanced matcher with improved pattern recognition for merchant name matching.\n",
    "    Uses multiple similarity algorithms and merchant category-specific patterns.\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_embedder=None):\n",
    "        \"\"\"\n",
    "        Initialize with enhanced BERT embedder.\n",
    "        \n",
    "        Args:\n",
    "            bert_embedder: Enhanced BERT embedder instance\n",
    "        \"\"\"\n",
    "        # Initialize enhanced BERT embedder\n",
    "        self.bert_embedder = bert_embedder\n",
    "        if self.bert_embedder is None and transformers_available:\n",
    "            self.bert_embedder = EnhancedBERTEmbedder()\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        # Initialize trie for approximate matching\n",
    "        self.trie = None\n",
    "        \n",
    "        # Initialize Aho-Corasick automaton only if available\n",
    "        if aho_corasick_available:\n",
    "            self.automaton = pyahocorasick.Automaton()\n",
    "        else:\n",
    "            self.automaton = None\n",
    "        \n",
    "        # Define abbreviation dictionary - comprehensive industry knowledge \n",
    "        self.abbreviations = self._get_abbreviation_dictionary()\n",
    "        \n",
    "        # Merchant category-specific abbreviations \n",
    "        self.merchant_category_abbreviations = self._get_merchant_category_abbreviations()\n",
    "        \n",
    "        # Stop words to remove during preprocessing\n",
    "        self.stopwords = self._get_stopwords()\n",
    "        \n",
    "        # Merchant category-specific stopwords \n",
    "        self.merchant_category_stopwords = self._get_merchant_category_stopwords()\n",
    "        \n",
    "        # Add base weights as a class attribute\n",
    "        self.base_weights = {\n",
    "            'jaro_winkler': 0.10,\n",
    "            'damerau_levenshtein': 0.05,\n",
    "            'tfidf_cosine': 0.05,\n",
    "            'jaccard_bigram': 0.05,\n",
    "            'soundex': 0.05,\n",
    "            'token_sort_ratio': 0.10,\n",
    "            'contains_ratio': 0.10,\n",
    "            'fuzzy_levenshtein': 0.05,\n",
    "            'trie_approximate': 0.10,\n",
    "            'bert_similarity': 0.15,\n",
    "            'aho_corasick': 0.05,\n",
    "            'DBAName_formation': 0.15\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def _get_abbreviation_dictionary(self):\n",
    "        \"\"\"Get comprehensive abbreviation dictionary\"\"\"\n",
    "        return {\n",
    "            # Banking & Financial Institutions\n",
    "            'bofa': 'bank of america', 'b of a': 'bank of america',\n",
    "            'boa': 'bank of america', 'bac': 'bank of america',\n",
    "            'jpm': 'jpmorgan chase', 'jpm chase': 'jpmorgan chase',\n",
    "            'wf': 'wells fargo', 'wfb': 'wells fargo bank',\n",
    "            'citi': 'citibank', 'citi bank': 'citibank',\n",
    "            'gs': 'goldman sachs', 'ms': 'morgan stanley',\n",
    "            'db': 'deutsche bank', 'hsbc': 'hongkong and shanghai banking corporation',\n",
    "            'amex': 'american express', 'usb': 'us bank', 'rbc': 'royal bank of canada',\n",
    "            'pnc': 'pnc financial services', 'td': 'toronto dominion bank',\n",
    "            'bny': 'bank of new york', 'bnyc': 'bank of new york mellon',\n",
    "            'cba': 'commonwealth bank of australia', 'nab': 'national australia bank',\n",
    "            'rba': 'reserve bank of australia', 'westpac': 'western pacific bank',\n",
    "            \n",
    "            # Fast Food & Restaurant Chains\n",
    "            'mcd': 'mcdonalds', 'mcds': 'mcdonalds', 'md': 'mcdonalds',\n",
    "            'bk': 'burger king', 'kfc': 'kentucky fried chicken',\n",
    "            'sbux': 'starbucks', 'sb': 'starbucks',\n",
    "            'tb': 'taco bell', 'wen': 'wendys',\n",
    "            'dq': 'dairy queen', 'ph': 'pizza hut',\n",
    "            'dnkn': 'dunkin donuts', 'cfa': 'chick fil a',\n",
    "            'cmg': 'chipotle mexican grill', 'ihop': 'international house of pancakes',\n",
    "            'tgi': 'tgi fridays', 'tgif': 'tgi fridays',\n",
    "            \n",
    "            # Tech Companies\n",
    "            'msft': 'microsoft', 'aapl': 'apple', 'goog': 'google',\n",
    "            'googl': 'google', 'amzn': 'amazon', 'fb': 'facebook',\n",
    "            'meta': 'meta platforms', 'nflx': 'netflix', 'tsla': 'tesla',\n",
    "            'ibm': 'international business machines', 'csco': 'cisco systems',\n",
    "            'orcl': 'oracle', 'intc': 'intel', 'amd': 'advanced micro devices',\n",
    "            'nvda': 'nvidia', 'adbe': 'adobe', 'crm': 'salesforce',\n",
    "            \n",
    "            # Automotive\n",
    "            'tm': 'toyota motor', 'toyof': 'toyota', 'toyota': 'toyota corporation',\n",
    "            'f': 'ford motor company', 'gm': 'general motors',\n",
    "            'hmc': 'honda motor company', 'hndaf': 'honda',\n",
    "            'nsany': 'nissan', 'bmwyy': 'bmw', 'vwagy': 'volkswagen',\n",
    "            \n",
    "            # Retail companies\n",
    "            'wmt': 'walmart', 'tgt': 'target', 'cost': 'costco',\n",
    "            'hd': 'home depot', 'low': 'lowes', 'bby': 'best buy',\n",
    "            'ebay': 'ebay', 'dg': 'dollar general', 'dltr': 'dollar tree',\n",
    "            \n",
    "            # Government & Organizations\n",
    "            'dhs': 'department of homeland security',\n",
    "            'dod': 'department of defense', 'dos': 'department of state',\n",
    "            'epa': 'environmental protection agency', 'fbi': 'federal bureau of investigation',\n",
    "            'cia': 'central intelligence agency', 'irs': 'internal revenue service',\n",
    "            'fda': 'food and drug administration', 'sec': 'securities and exchange commission',\n",
    "            'usps': 'united states postal service', 'doi': 'department of interior',\n",
    "            'fed': 'federal reserve', 'who': 'world health organization',\n",
    "            'un': 'united nations', 'nato': 'north atlantic treaty organization',\n",
    "            \n",
    "            # Common abbreviations\n",
    "            'j&j': 'johnson & johnson', 'jj': 'johnson johnson', \n",
    "            'jnj': 'johnson and johnson', '7-11': '7-eleven', \n",
    "            '711': '7-eleven', 'intl': 'international',\n",
    "            'corp': 'corporation', 'inc': 'incorporated',\n",
    "            \n",
    "            # Address components\n",
    "            'rd': 'road', 'st': 'street', 'ave': 'avenue', \n",
    "            'blvd': 'boulevard', 'ctr': 'center', 'ln': 'lane', \n",
    "            'dr': 'drive', 'pl': 'place', 'ct': 'court',\n",
    "            'hwy': 'highway', 'pkwy': 'parkway', 'sq': 'square'\n",
    "        }\n",
    "    \n",
    "    def _get_merchant_category_abbreviations(self):\n",
    "        \"\"\"Get merchant category-specific abbreviation dictionaries \"\"\"\n",
    "        return {\n",
    "            'Medical': {\n",
    "                'dr': 'doctor', 'hosp': 'hospital', 'med': 'medical',\n",
    "                'clin': 'clinic', 'pharm': 'pharmacy', 'lab': 'laboratory',\n",
    "                'dept': 'department', 'ctr': 'center', 'inst': 'institute',\n",
    "                'er': 'emergency room', 'icu': 'intensive care unit',\n",
    "                'ob': 'obstetrics', 'gyn': 'gynecology', 'peds': 'pediatrics',\n",
    "                'ortho': 'orthopedics', 'onc': 'oncology', 'neuro': 'neurology'\n",
    "            },\n",
    "            'Government': {\n",
    "                'govt': 'government', 'dept': 'department', 'admin': 'administration',\n",
    "                'auth': 'authority', 'fed': 'federal', 'natl': 'national',\n",
    "                'comm': 'commission', 'sec': 'secretary', 'org': 'organization',\n",
    "                'div': 'division', 'bur': 'bureau', 'off': 'office',\n",
    "                'min': 'ministry', 'reg': 'regional', 'dist': 'district',\n",
    "                'cncl': 'council', 'cmte': 'committee', 'subcmte': 'subcommittee'\n",
    "            },\n",
    "            'Education': {\n",
    "                'univ': 'university', 'coll': 'college', 'acad': 'academy',\n",
    "                'elem': 'elementary', 'sch': 'school', 'inst': 'institute',\n",
    "                'dept': 'department', 'lib': 'library', 'lab': 'laboratory',\n",
    "                'fac': 'faculty', 'prof': 'professor', 'assoc': 'associate',\n",
    "                'asst': 'assistant', 'adm': 'administration', 'stdnt': 'student',\n",
    "                'grad': 'graduate', 'undergrad': 'undergraduate'\n",
    "            },\n",
    "            'Financial': {\n",
    "                'fin': 'financial', 'svcs': 'services', 'mgmt': 'management',\n",
    "                'assoc': 'associates', 'intl': 'international', 'grp': 'group',\n",
    "                'corp': 'corporation', 'cap': 'capital', 'inv': 'investment',\n",
    "                'asset': 'asset management', 'sec': 'securities', 'adv': 'advisors',\n",
    "                'tr': 'trust', 'port': 'portfolio', 'acct': 'account',\n",
    "                'bal': 'balance', 'stmt': 'statement', 'equ': 'equity'\n",
    "            },\n",
    "            'Restaurant': {\n",
    "                'rest': 'restaurant', 'cafe': 'cafeteria', 'grill': 'grillery',\n",
    "                'brew': 'brewery', 'bar': 'bar and grill', 'bbq': 'barbecue',\n",
    "                'deli': 'delicatessen', 'stk': 'steakhouse', 'bf': 'breakfast',\n",
    "                'din': 'dinner', 'chs': 'cheese', 'ckn': 'chicken'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_stopwords(self):\n",
    "        \"\"\"Get general stopwords for preprocessing\"\"\"\n",
    "        return {\n",
    "            'inc', 'llc', 'co', 'ltd', 'corp', 'plc', 'na', 'the', \n",
    "            'and', 'of', 'for', 'in', 'a', 'an', 'by', 'to', 'at',\n",
    "            'corporation', 'incorporated', 'company', 'limited',\n",
    "            'with', 'from', 'as', 'on', 'group', 'services'\n",
    "        }\n",
    "    \n",
    "    def _get_merchant_category_stopwords(self):\n",
    "        \"\"\"Get merchant category-specific stopwords (renamed from _get_domain_stopwords)\"\"\"\n",
    "        return {\n",
    "            'Medical': {'center', 'healthcare', 'medical', 'health', 'care', 'services', 'clinic', 'hospital'},\n",
    "            'Government': {'department', 'office', 'agency', 'bureau', 'division', 'authority', 'administration'},\n",
    "            'Education': {'university', 'college', 'school', 'institute', 'academy', 'education', 'learning'},\n",
    "            'Financial': {'financial', 'services', 'management', 'capital', 'investment', 'banking', 'advisor'},\n",
    "            'Restaurant': {'restaurant', 'cafe', 'diner', 'eatery', 'grill', 'kitchen', 'bar', 'house'}\n",
    "        }\n",
    "    \n",
    "    def enhanced_preprocessing(self, text, merchant_category=None):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with better handling of merchant-specific patterns\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to preprocess\n",
    "            merchant_category (str, optional): Merchant category for specialized processing\n",
    "        \n",
    "        Returns:\n",
    "            str: Preprocessed text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Better handling of punctuation - preserve periods in DBANames\n",
    "        # and apostrophes in business names (e.g., McDonald's)\n",
    "        text = re.sub(r'([^a-z0-9\\'\\.\\&\\-])', ' ', text)\n",
    "        \n",
    "        # Special handling for business name apostrophes\n",
    "        text = re.sub(r'\\'s\\b', 's', text)  # Convert McDonald's to McDonalds\n",
    "        \n",
    "        # Expand common business suffixes\n",
    "        business_suffixes = {\n",
    "            r'\\bco\\b': 'company',\n",
    "            r'\\binc\\b': '',  # Remove Inc entirely\n",
    "            r'\\bltd\\b': 'limited',\n",
    "            r'\\bllc\\b': '',  # Remove LLC entirely\n",
    "            r'\\bcorp\\b': 'corporation',\n",
    "            r'\\bcorporation\\b': '',  # Remove when processing full names for matching\n",
    "            r'\\blimited\\b': '',      # Remove when processing full names for matching\n",
    "            r'\\bcompany\\b': '',      # Remove when processing full names for matching\n",
    "        }\n",
    "        \n",
    "        for suffix, replacement in business_suffixes.items():\n",
    "            text = re.sub(suffix, replacement, text)\n",
    "        \n",
    "        # Replace abbreviations\n",
    "        words = text.split()\n",
    "        \n",
    "        # Apply general abbreviation expansion\n",
    "        words = [self.abbreviations.get(word, word) for word in words]\n",
    "        \n",
    "        # Apply merchant category-specific abbreviation expansion if merchant_category is provided\n",
    "        if merchant_category and merchant_category in self.merchant_category_abbreviations:\n",
    "            words = [self.merchant_category_abbreviations[merchant_category].get(word, word) for word in words]\n",
    "        \n",
    "        # Remove general stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "        \n",
    "        # Remove merchant category-specific stopwords if merchant_category is provided\n",
    "        if merchant_category and merchant_category in self.merchant_category_stopwords:\n",
    "            words = [word for word in words if word not in self.merchant_category_stopwords[merchant_category]]\n",
    "        \n",
    "        # Rejoin words and remove extra spaces\n",
    "        text = ' '.join(words)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_pair(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"Preprocess DBAName and RawTransactionName with their respective merchant categories\"\"\"\n",
    "        DBAName_clean = self.enhanced_preprocessing(DBAName, DBA_Merchant_Category)\n",
    "        RawTransactionName_clean = self.enhanced_preprocessing(RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        return DBAName_clean, RawTransactionName_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380b58a2-5ff5-46ec-9cd9-c0de0f7c2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Similarity Methods for Merchant Matcher\n",
    "\n",
    "class EnhancedMerchantMatcherWithSimilarity(EnhancedMerchantMatcher):\n",
    "    \"\"\"Adding similarity methods to the EnhancedMerchantMatcher class\"\"\"\n",
    "    \n",
    "    def jaro_winkler_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate Jaro-Winkler similarity with enhanced preprocessing\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "            \n",
    "        Returns:\n",
    "            float: Jaro-Winkler similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        return jaro_winkler(DBAName_clean, RawTransactionName_clean)\n",
    "    \n",
    "    def damerau_levenshtein_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate Damerau-Levenshtein similarity, better for handling transpositions\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Damerau-Levenshtein similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Damerau-Levenshtein distance\n",
    "        max_len = max(len(DBAName_clean), len(RawTransactionName_clean))\n",
    "        if max_len == 0:\n",
    "            return 0\n",
    "        \n",
    "        distance = textdistance.damerau_levenshtein.distance(DBAName_clean, RawTransactionName_clean)\n",
    "        similarity = 1 - (distance / max_len)\n",
    "        return max(0, similarity)  # Ensure non-negative\n",
    "    \n",
    "    def tfidf_cosine_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate TF-IDF Cosine similarity for keyword matching\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: TF-IDF cosine similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Fit and transform with TF-IDF\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform([DBAName_clean, RawTransactionName_clean])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return float(max(0, similarity))  # Ensure non-negative\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def jaccard_bigram_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate Jaccard Bigram similarity for character overlaps\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Jaccard bigram similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Create bigrams\n",
    "        def get_bigrams(text):\n",
    "            return [text[i:i+2] for i in range(len(text)-1)]\n",
    "        \n",
    "        DBAName_bigrams = set(get_bigrams(DBAName_clean))\n",
    "        RawTransactionName_bigrams = set(get_bigrams(RawTransactionName_clean))\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        union_size = len(DBAName_bigrams.union(RawTransactionName_bigrams))\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        \n",
    "        intersection_size = len(DBAName_bigrams.intersection(RawTransactionName_bigrams))\n",
    "        return intersection_size / union_size\n",
    "    \n",
    "    def soundex_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate phonetic similarity using Soundex algorithm.\n",
    "        Especially useful for similar-sounding business names.\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Phonetic similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # If either string is empty, return 0\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get the soundex codes for both strings\n",
    "        try:\n",
    "            # For multi-word strings, get soundex for each word\n",
    "            DBAName_words = DBAName_clean.split()\n",
    "            RawTransactionName_words = RawTransactionName_clean.split()\n",
    "            \n",
    "            # Get soundex codes for each word\n",
    "            DBAName_codes = [jellyfish.soundex(word) for word in DBAName_words if len(word) > 1]\n",
    "            RawTransactionName_codes = [jellyfish.soundex(word) for word in RawTransactionName_words if len(word) > 1]\n",
    "            \n",
    "            # Calculate matches between codes\n",
    "            matches = 0\n",
    "            total = max(len(DBAName_codes), len(RawTransactionName_codes))\n",
    "            \n",
    "            if total == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # Count matched codes\n",
    "            for code in DBAName_codes:\n",
    "                if code in RawTransactionName_codes:\n",
    "                    matches += 1\n",
    "                    # Remove the matched code to avoid double counting\n",
    "                    RawTransactionName_codes.remove(code)\n",
    "            \n",
    "            return matches / total\n",
    "        except:\n",
    "            # Fallback if there's an error with the soundex calculation\n",
    "            return 0.0\n",
    "    \n",
    "    def token_sort_ratio_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate Token Sort Ratio using fuzzywuzzy.\n",
    "        Handles word order differences well.\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Token sort ratio similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Token Sort Ratio\n",
    "        ratio = fuzz.token_sort_ratio(DBAName, RawTransactionName) / 100\n",
    "        return ratio\n",
    "    \n",
    "    def contains_ratio_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Check if DBAName is contained in full name or vice versa\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Containment similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Check if DBAName is contained in full name\n",
    "        if DBAName_clean in RawTransactionName_clean:\n",
    "            return 1\n",
    "        \n",
    "        # Check if full name is contained in DBAName\n",
    "        if RawTransactionName_clean in DBAName_clean:\n",
    "            return 0.9\n",
    "        \n",
    "        # Check for partial containment\n",
    "        DBAName_chars = list(DBAName_clean)\n",
    "        RawTransactionName_chars = list(RawTransactionName_clean)\n",
    "        \n",
    "        matches = 0\n",
    "        for char in DBAName_chars:\n",
    "            if char in RawTransactionName_chars:\n",
    "                matches += 1\n",
    "                RawTransactionName_chars.remove(char)  # Remove matched char\n",
    "        \n",
    "        return matches / len(DBAName_chars) if len(DBAName_chars) > 0 else 0\n",
    "    \n",
    "    def fuzzy_levenshtein_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate fuzzy Levenshtein ratio for typo tolerance\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Fuzzy Levenshtein similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Levenshtein ratio (which is already normalized)\n",
    "        similarity = levenshtein_ratio(DBAName_clean, RawTransactionName_clean)\n",
    "        return float(similarity)\n",
    "    def trie_approximate_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Use approximate matching for DBAName formation detection\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Trie approximate similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Extract first letters from each word in full name\n",
    "        words = RawTransactionName_clean.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # Check if DBAName matches first letters\n",
    "        if DBAName_clean.lower() == first_letters.lower():\n",
    "            return 1\n",
    "        \n",
    "        # Calculate similarity for approximate matching\n",
    "        max_len = max(len(DBAName_clean), len(first_letters))\n",
    "        if max_len == 0:\n",
    "            return 0\n",
    "        \n",
    "        distance = levenshtein_distance(DBAName_clean.lower(), first_letters.lower())\n",
    "        similarity = 1 - (distance / max_len)\n",
    "        return max(0, similarity)\n",
    "    \n",
    "    def aho_corasick_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Use Aho-Corasick algorithm for pattern matching\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Aho-Corasick similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        if not aho_corasick_available:\n",
    "            # Fallback implementation when pyahocorasick is not available\n",
    "            matches = 0\n",
    "            remaining_text = RawTransactionName_clean\n",
    "            for c in DBAName_clean:\n",
    "                if c in remaining_text:\n",
    "                    matches += 1\n",
    "                    # Remove matched character to prevent duplicate counting\n",
    "                    idx = remaining_text.find(c)\n",
    "                    remaining_text = remaining_text[:idx] + remaining_text[idx+1:]\n",
    "            \n",
    "            return min(1.0, matches / len(DBAName_clean)) if len(DBAName_clean) > 0 else 0\n",
    "        \n",
    "        # Build automaton\n",
    "        automaton = pyahocorasick.Automaton()\n",
    "        for i, c in enumerate(DBAName_clean):\n",
    "            automaton.add_word(c, (i, c))\n",
    "        automaton.make_automaton()\n",
    "        \n",
    "        # Find matches\n",
    "        matches = 0\n",
    "        for _, (_, c) in automaton.iter(RawTransactionName_clean):\n",
    "            matches += 1\n",
    "        \n",
    "        # Calculate score\n",
    "        if len(DBAName_clean) == 0:\n",
    "            return 0\n",
    "        \n",
    "        return min(1.0, matches / len(DBAName_clean))\n",
    "    \n",
    "    def bert_similarity(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate semantic similarity using BERT embeddings\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: BERT similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # If BERT embedder is not initialized, return 0\n",
    "        if self.bert_embedder is None:\n",
    "            return 0\n",
    "        \n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            # Get embeddings from pre-trained model\n",
    "            emb1 = self.bert_embedder.encode(DBAName_clean)\n",
    "            emb2 = self.bert_embedder.encode(RawTransactionName_clean)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            dot_product = np.sum(emb1 * emb2)\n",
    "            norm1 = np.linalg.norm(emb1)\n",
    "            norm2 = np.linalg.norm(emb2)\n",
    "            \n",
    "            similarity = dot_product / (norm1 * norm2 + 1e-8)\n",
    "            return float(similarity)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in BERT similarity calculation: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def DBAName_formation_score(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate how well the DBAName is formed from the full name\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: DBAName formation score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        # Check if strings are empty\n",
    "        if not DBAName_clean or not RawTransactionName_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Extract first letters from each word in full name\n",
    "        words = RawTransactionName_clean.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        # Standard DBAName formation - first letter of each word\n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # If exact match, return 1\n",
    "        if DBAName_clean.lower() == first_letters.lower():\n",
    "            return 1\n",
    "        \n",
    "        # Check partial match\n",
    "        DBAName_chars = list(DBAName_clean.lower())\n",
    "        first_letters_chars = list(first_letters.lower())\n",
    "        \n",
    "        matches = 0\n",
    "        for char in DBAName_chars:\n",
    "            if char in first_letters_chars:\n",
    "                matches += 1\n",
    "                first_letters_chars.remove(char)  # Remove matched char\n",
    "        \n",
    "        if len(DBAName_chars) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate partial match score\n",
    "        return matches / len(DBAName_chars)\n",
    "    \n",
    "    def enhanced_DBAName_formation_score(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Enhanced DBAName formation score with special handling for common patterns\n",
    "        particularly optimized for business names with prefixes like \"Mc\".\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            float: Enhanced DBAName formation score between 0 and 1\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        \n",
    "        # Basic cleanup\n",
    "        DBAName = DBAName_clean.lower()\n",
    "        RawTransactionName = RawTransactionName_clean.lower()\n",
    "\n",
    "        \n",
    "        # Standard DBAName formation - first letter of each word\n",
    "        words = RawTransactionName.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        # Get first letters\n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # If exact match, return high score\n",
    "        if DBAName == first_letters:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check for consonant-based DBAName (common in business DBANames)\n",
    "        consonants = ''.join([c for c in RawTransactionName if c not in 'aeiou' and c.isalpha()])\n",
    "        consonant_match = 0.0\n",
    "        if len(DBAName) <= len(consonants):\n",
    "            # Check for sequential consonant match\n",
    "            DBAName_position = 0\n",
    "            for i, c in enumerate(consonants):\n",
    "                if DBAName_position < len(DBAName) and c == DBAName[DBAName_position]:\n",
    "                    DBAName_position += 1\n",
    "            \n",
    "            consonant_sequential_match = DBAName_position / len(DBAName) if len(DBAName) > 0 else 0\n",
    "            \n",
    "            # Check for any consonant match\n",
    "            matches = 0\n",
    "            consonants_copy = consonants\n",
    "            for char in DBAName:\n",
    "                if char in consonants_copy:\n",
    "                    matches += 1\n",
    "                    consonants_copy = consonants_copy.replace(char, '', 1)\n",
    "            \n",
    "            consonant_any_match = matches / len(DBAName) if len(DBAName) > 0 else 0\n",
    "            \n",
    "            # Take the better score\n",
    "            consonant_match = max(consonant_sequential_match, consonant_any_match)\n",
    "            \n",
    "            # Give higher scores for strong consonant matches\n",
    "            if consonant_match > 0.7:\n",
    "                return max(0.85, consonant_match)\n",
    "        \n",
    "        # Calculate ordered match score\n",
    "        ordered_match = 0\n",
    "        last_found_index = -1\n",
    "        RawTransactionName_chars = list(RawTransactionName)\n",
    "        \n",
    "        for char in DBAName:\n",
    "            found = False\n",
    "            for i in range(last_found_index + 1, len(RawTransactionName_chars)):\n",
    "                if char == RawTransactionName_chars[i]:\n",
    "                    ordered_match += 1\n",
    "                    last_found_index = i\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            # If we couldn't find the character in order, try looking anywhere\n",
    "            if not found:\n",
    "                for i in range(len(RawTransactionName_chars)):\n",
    "                    if i != last_found_index and char == RawTransactionName_chars[i]:\n",
    "                        ordered_match += 0.5  # Half credit for out-of-order match\n",
    "                        RawTransactionName_chars[i] = '_'  # Mark as used\n",
    "                        break\n",
    "        \n",
    "        ordered_match_score = ordered_match / len(DBAName) if len(DBAName) > 0 else 0\n",
    "        \n",
    "        # Return the best score from different matching strategies\n",
    "        return max(\n",
    "            ordered_match_score * 0.9,  # Ordered match is good but not perfect\n",
    "            consonant_match * 0.9,      # Consonant match is also valuable\n",
    "            0.4                         # Minimum score to prevent too low values\n",
    "        )\n",
    "    \n",
    "    def detect_complex_business_patterns(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Detect business name patterns using generalized logic that works across industries\n",
    "        \"\"\"\n",
    "        DBAName_clean, RawTransactionName_clean = self.preprocess_pair(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        DBAName = DBAName_clean.lower()\n",
    "        RawTransactionName = RawTransactionName_clean.lower()\n",
    "        \n",
    "        patterns = {}\n",
    "        words_DBAName = DBAName.split()\n",
    "        words_full = RawTransactionName.split()\n",
    "        \n",
    "        # General word order inversion pattern\n",
    "        if len(words_DBAName) >= 2 and len(words_full) >= 2:\n",
    "            common_words = set(words_DBAName).intersection(set(words_full))\n",
    "            \n",
    "            if len(common_words) >= 2:\n",
    "                dba_common_indices = [(i, word) for i, word in enumerate(words_DBAName) if word in common_words]\n",
    "                full_common_indices = [(i, word) for i, word in enumerate(words_full) if word in common_words]\n",
    "                \n",
    "                dba_word_order = [word for _, word in dba_common_indices]\n",
    "                full_word_order = [word for _, word in full_common_indices]\n",
    "                \n",
    "                if dba_word_order and full_word_order and dba_word_order != full_word_order:\n",
    "                    if (dba_common_indices[0][0] == 0 and full_common_indices[-1][0] == len(words_full) - 1) or \\\n",
    "                       (full_common_indices[0][0] == 0 and dba_common_indices[-1][0] == len(words_DBAName) - 1):\n",
    "                        patterns['word_order_inversion'] = 1.0\n",
    "                    else:\n",
    "                        patterns['partial_word_order_variation'] = 0.7\n",
    "        \n",
    "        # Connector word pattern\n",
    "        if '&' in RawTransactionName or ' and ' in RawTransactionName:\n",
    "            parts = re.split(r'\\s+&\\s+|\\s+and\\s+', RawTransactionName)\n",
    "            if len(parts) >= 2:\n",
    "                first_letters = ''.join(part[0] for part in parts if part)\n",
    "                if DBAName == first_letters:\n",
    "                    patterns['connector_word_acronym'] = 1.0\n",
    "                elif all(letter in DBAName for letter in first_letters):\n",
    "                    patterns['partial_connector_acronym'] = 0.8\n",
    "        \n",
    "        # Acronym detection\n",
    "        significant_words = [w for w in words_full if len(w) > 2]\n",
    "        if len(significant_words) >= 2 and len(DBAName) >= 2:\n",
    "            first_letters = ''.join(word[0] for word in significant_words)\n",
    "            if DBAName == first_letters:\n",
    "                patterns['complete_acronym_match'] = 1.0\n",
    "            elif DBAName in first_letters:\n",
    "                patterns['partial_acronym_match'] = 0.9\n",
    "            elif len(set(DBAName).intersection(set(first_letters))) / len(DBAName) > 0.7:\n",
    "                patterns['likely_acronym_relation'] = 0.8\n",
    "        \n",
    "        # Descriptor variation\n",
    "        descriptor_terms = set([\n",
    "            'north', 'south', 'east', 'west', 'central', 'metro', 'city', \n",
    "            'downtown', 'regional', 'national', 'global', 'local',\n",
    "            'international', 'worldwide', 'main', 'branch', 'center',\n",
    "            'premier', 'express', 'digital', 'online', 'official'\n",
    "        ])\n",
    "        \n",
    "        dba_has_descriptors = any(term in descriptor_terms for term in words_DBAName)\n",
    "        full_has_descriptors = any(term in descriptor_terms for term in words_full)\n",
    "        \n",
    "        if dba_has_descriptors != full_has_descriptors:\n",
    "            clean_dba = ' '.join([w for w in words_DBAName if w not in descriptor_terms])\n",
    "            clean_full = ' '.join([w for w in words_full if w not in descriptor_terms])\n",
    "            \n",
    "            if clean_dba in clean_full or clean_full in clean_dba:\n",
    "                patterns['descriptor_term_variation'] = 1.0\n",
    "            elif len(clean_dba) > 3 and len(clean_full) > 3 and (clean_dba[:3] == clean_full[:3]):\n",
    "                patterns['potential_descriptor_variation'] = 0.7\n",
    "        \n",
    "        # Substring containment\n",
    "        if len(DBAName) > 3 and len(RawTransactionName) > 3:\n",
    "            if DBAName in RawTransactionName:\n",
    "                ratio = len(DBAName) / len(RawTransactionName)\n",
    "                if ratio > 0.3:\n",
    "                    patterns['significant_substring_match'] = 0.9\n",
    "            elif RawTransactionName in DBAName:\n",
    "                ratio = len(RawTransactionName) / len(DBAName)\n",
    "                if ratio > 0.3:\n",
    "                    patterns['reverse_substring_match'] = 0.8\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def get_all_similarity_scores(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Calculate all similarity scores at once for efficiency\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "        DBA_Merchant_Category (str): Merchant category for the DBAName\n",
    "        RawTransactionName (str): The full name to match against\n",
    "        RawTransaction_Merchant_Category (str, optional): Merchant category for the RawTransactionName\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of algorithm name to score\n",
    "        \"\"\"\n",
    "        # Return empty dictionary if either DBAName or RawTransactionName is None\n",
    "        if DBAName is None or RawTransactionName is None:\n",
    "            return {}\n",
    "        \n",
    "        # Calculate all similarity scores\n",
    "        scores = {\n",
    "            'jaro_winkler': self.jaro_winkler_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'damerau_levenshtein': self.damerau_levenshtein_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'tfidf_cosine': self.tfidf_cosine_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'jaccard_bigram': self.jaccard_bigram_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'soundex': self.soundex_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'token_sort_ratio': self.token_sort_ratio_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'contains_ratio': self.contains_ratio_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'fuzzy_levenshtein': self.fuzzy_levenshtein_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'trie_approximate': self.trie_approximate_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'aho_corasick': self.aho_corasick_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'DBAName_formation': self.DBAName_formation_score(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category),\n",
    "            'enhanced_DBAName_formation': self.enhanced_DBAName_formation_score(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        }\n",
    "        \n",
    "        # Add BERT similarity if available\n",
    "        if self.bert_embedder is not None:\n",
    "            scores['bert_similarity'] = self.bert_similarity(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        \n",
    "        # Add pattern detection scores\n",
    "        pattern_results = self.detect_complex_business_patterns(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        for pattern, score in pattern_results.items():\n",
    "            scores[f'pattern_{pattern}'] = score\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def get_dynamic_weights(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Get dynamically adjusted weights based on merchant name and category characteristics\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName or short name\n",
    "            DBA_Merchant_Category (str): Category for the DBA\n",
    "            RawTransactionName (str): The full merchant name\n",
    "            RawTransaction_Merchant_Category (str, optional): Category for the raw transaction\n",
    "                \n",
    "        Returns:\n",
    "            dict: Dictionary of dynamically adjusted algorithm weights\n",
    "        \"\"\"\n",
    "        # Start with base weights defined in the class\n",
    "        weights = self.base_weights.copy()\n",
    "        \n",
    "        # Adjust weights based on name characteristics\n",
    "        DBAName_len = len(DBAName) if isinstance(DBAName, str) else 0\n",
    "        RawTransactionName_len = len(RawTransactionName) if isinstance(RawTransactionName, str) else 0\n",
    "        \n",
    "            # Define primary_category based on DBA_Merchant_Category (THIS IS THE FIX)\n",
    "        primary_category = DBA_Merchant_Category if DBA_Merchant_Category is not None else \"Unknown\"\n",
    "        \n",
    "        # For very short DBANames (2-3 chars), boost DBAName formation importance\n",
    "        if 2 <= DBAName_len <= 3:\n",
    "            weights['DBAName_formation'] = 0.25\n",
    "            weights['enhanced_DBAName_formation'] = 0.20\n",
    "            weights['bert_similarity'] = 0.15\n",
    "            weights['contains_ratio'] = 0.15\n",
    "            \n",
    "        # For longer DBANames, focus more on semantic similarity\n",
    "        elif DBAName_len >= 4:\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            weights['token_sort_ratio'] = 0.15\n",
    "            \n",
    "        # For very long full names, semantic understanding becomes more important\n",
    "        if RawTransactionName_len > 30:\n",
    "            weights['bert_similarity'] = 0.30\n",
    "            weights['tfidf_cosine'] = 0.15\n",
    "            \n",
    "        # If full name contains \"Bank\" or related terms, boost specific algorithms\n",
    "        if isinstance(RawTransactionName, str) and re.search(r'\\b(bank|credit|financial|capital)\\b', RawTransactionName.lower()):\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            weights['DBAName_formation'] = 0.20\n",
    "            \n",
    "        # If full name contains location indicators (east, west, north, south)\n",
    "        if isinstance(RawTransactionName, str) and re.search(r'\\b(east|west|north|south|central)\\b', RawTransactionName.lower()):\n",
    "            weights['token_sort_ratio'] = 0.20\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            \n",
    "        # Category-specific adjustments - using the primary category (now properly defined)\n",
    "        if primary_category == 'Restaurant':\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            weights['fuzzy_levenshtein'] = 0.15\n",
    "        elif primary_category == 'Banking':\n",
    "            weights['DBAName_formation'] = 0.25\n",
    "            weights['enhanced_DBAName_formation'] = 0.25\n",
    "            weights['bert_similarity'] = 0.20\n",
    "        elif primary_category == 'Government':\n",
    "            weights['bert_similarity'] = 0.25\n",
    "            weights['DBAName_formation'] = 0.20\n",
    "            weights['token_sort_ratio'] = 0.15\n",
    "        elif primary_category == 'Medical':\n",
    "            weights['soundex'] = 0.15\n",
    "            weights['bert_similarity'] = 0.25\n",
    "        elif primary_category == 'Automotive':\n",
    "            weights['contains_ratio'] = 0.15\n",
    "            weights['token_sort_ratio'] = 0.15\n",
    "            weights['bert_similarity'] = 0.20\n",
    "        \n",
    "        # Special case: if categories differ significantly, increase semantic scores\n",
    "        if RawTransaction_Merchant_Category and RawTransaction_Merchant_Category != DBA_Merchant_Category:\n",
    "            category_similarity = self.calculate_category_similarity(DBA_Merchant_Category, RawTransaction_Merchant_Category)\n",
    "            if category_similarity < 0.5:  # Categories are quite different\n",
    "                weights['bert_similarity'] = max(weights.get('bert_similarity', 0.15) * 1.3, 0.30)  # Increase semantic understanding\n",
    "        \n",
    "        # If enhanced DBAName formation is available, use it instead of standard DBAName formation\n",
    "        if 'enhanced_DBAName_formation' not in weights and 'DBAName_formation' in weights:\n",
    "            weights['enhanced_DBAName_formation'] = weights['DBAName_formation']\n",
    "            weights['DBAName_formation'] = weights['DBAName_formation'] * 0.5  # Reduce standard weight\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        weight_sum = sum(weights.values())\n",
    "        return {k: v/weight_sum for k, v in weights.items()}\n",
    "\n",
    "    def calculate_category_similarity(self, category1, category2):\n",
    "        \"\"\"\n",
    "        Calculate similarity between merchant categories using a more flexible approach\n",
    "        that reduces hard-coding and leverages semantic understanding.\n",
    "        \n",
    "        Args:\n",
    "            category1 (str): First merchant category\n",
    "            category2 (str): Second merchant category\n",
    "            \n",
    "        Returns:\n",
    "            float: Similarity score between 0.0 and 1.0\n",
    "        \"\"\"\n",
    "        # Case 1: Handle None values or empty strings\n",
    "        if not category1 or not category2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Normalize categories to lowercase for better matching\n",
    "        cat1 = category1.lower()\n",
    "        cat2 = category2.lower()\n",
    "        \n",
    "        # Case 2: Exact match after normalization\n",
    "        if cat1 == cat2:\n",
    "            return 1.0\n",
    "        \n",
    "        # Case 3: Check for substring relationships (one is contained in the other)\n",
    "        # This handles cases like \"Bank\" vs \"Banking\" or \"Restaurant\" vs \"Fast Food Restaurant\"\n",
    "        if cat1 in cat2 or cat2 in cat1:\n",
    "            # Calculate how much of the shorter string is contained in the longer one\n",
    "            shorter = cat1 if len(cat1) < len(cat2) else cat2\n",
    "            longer = cat2 if len(cat1) < len(cat2) else cat1\n",
    "            overlap_ratio = len(shorter) / len(longer)\n",
    "            \n",
    "            # Only consider it a strong match if there's substantial overlap\n",
    "            if overlap_ratio > 0.5:\n",
    "                return 0.85  # Strong partial match\n",
    "            else:\n",
    "                return 0.7   # Moderate partial match\n",
    "        \n",
    "        # Case 4: Check for word-level overlaps (handles multi-word categories)\n",
    "        words1 = set(cat1.split())\n",
    "        words2 = set(cat2.split())\n",
    "        \n",
    "        common_words = words1.intersection(words2)\n",
    "        if common_words:\n",
    "            # Calculate Jaccard similarity (intersection over union)\n",
    "            similarity = len(common_words) / len(words1.union(words2))\n",
    "            \n",
    "            # Scale to give more weight to word overlaps (min 0.6, max 0.8)\n",
    "            adjusted_similarity = 0.6 + (similarity * 0.2)\n",
    "            return adjusted_similarity\n",
    "        \n",
    "        # Case 5: Use semantic similarity through BERT if available\n",
    "        if self.bert_embedder is not None:\n",
    "            try:\n",
    "                # Use BERT to calculate semantic similarity\n",
    "                semantic_similarity = self.bert_embedder.compute_similarity(category1, category2)\n",
    "                \n",
    "                # Only trust fairly strong semantic relationships\n",
    "                if semantic_similarity > 0.7:\n",
    "                    return semantic_similarity\n",
    "                elif semantic_similarity > 0.5:\n",
    "                    # Somewhat related but not strongly\n",
    "                    return 0.5\n",
    "            except Exception as e:\n",
    "                # Silently handle errors and continue to fallback method\n",
    "                pass\n",
    "        \n",
    "        # Case 6: Fallback to a small set of known category relationships\n",
    "        # This preserves some domain knowledge while minimizing hard-coding\n",
    "        related_categories = {\n",
    "            \"banking\": {\"financial\", \"finance\", \"credit\", \"loan\"},\n",
    "            \"restaurant\": {\"dining\", \"food\", \"cafe\", \"eatery\"},\n",
    "            \"retail\": {\"shop\", \"store\", \"market\"},\n",
    "            \"medical\": {\"health\", \"healthcare\", \"hospital\", \"pharmacy\"},\n",
    "            \"automotive\": {\"car\", \"auto\", \"vehicle\"},\n",
    "            \"technology\": {\"tech\", \"software\", \"electronics\", \"computer\"},\n",
    "            \"education\": {\"school\", \"university\", \"college\", \"academic\"},\n",
    "            \"government\": {\"agency\", \"municipal\", \"federal\", \"state\"}\n",
    "        }\n",
    "        \n",
    "        # Check if categories are related in our minimal set of relationships\n",
    "        for key, related in related_categories.items():\n",
    "            if (cat1 == key and cat2 in related) or (cat2 == key and cat1 in related):\n",
    "                return 0.75  # Related categories\n",
    "        \n",
    "        # Case 7: Default case - categories appear unrelated\n",
    "        return 0.0\n",
    "    \n",
    "    def compute_contextual_score(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Enhanced scoring that enforces category-based constraints and applies pattern-based boosting.\n",
    "        The function balances name similarity with business category matching:\n",
    "        \n",
    "        - If categories match exactly and name similarity is good: score can exceed 0.75\n",
    "        - If categories don't match: score is capped at 0.75 regardless of name similarity\n",
    "        \n",
    "        Pattern detection is used to boost scores when specific meaningful patterns are found.\n",
    "        \"\"\"\n",
    "        # Calculate base weighted score using name similarity algorithms\n",
    "        weighted_score = self.compute_weighted_score(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        \n",
    "        # Check for exact category match\n",
    "        categories_match = (DBA_Merchant_Category == RawTransaction_Merchant_Category)\n",
    "        \n",
    "        # Get all algorithm scores for pattern boosting\n",
    "        all_scores = self.get_all_similarity_scores(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        \n",
    "        # Define a mapping from new pattern names to boost values\n",
    "        pattern_boost_mapping = {\n",
    "            # Old pattern: New pattern equivalents\n",
    "            'word_order_inversion': 0.35,           # Replaces inverted_agency_structure and bank_name_inversion\n",
    "            'partial_word_order_variation': 0.25,\n",
    "            'connector_word_acronym': 0.30,         # Replaces ampersand_DBAName\n",
    "            'partial_connector_acronym': 0.20,      # Replaces partial_ampersand_DBAName\n",
    "            'complete_acronym_match': 0.25,         # Replaces multiword_business_DBAName\n",
    "            'partial_acronym_match': 0.20,\n",
    "            'likely_acronym_relation': 0.15,\n",
    "            'descriptor_term_variation': 0.35,      # Replaces regional_branch_variation\n",
    "            'potential_descriptor_variation': 0.20,\n",
    "            'significant_substring_match': 0.25,\n",
    "            'reverse_substring_match': 0.20\n",
    "        }\n",
    "        \n",
    "        # Apply pattern-based boosting with new pattern names\n",
    "        pattern_boost = 1.0\n",
    "        for algo, score in all_scores.items():\n",
    "            if algo.startswith('pattern_') and score > 0:\n",
    "                # Extract pattern name without the prefix\n",
    "                pattern_name = algo[8:]  # Remove 'pattern_'\n",
    "                \n",
    "                # Find corresponding boost value, default to 0.15 if pattern not in mapping\n",
    "                boost_value = pattern_boost_mapping.get(pattern_name, 0.15)\n",
    "                \n",
    "                # Apply the boost\n",
    "                pattern_boost += boost_value\n",
    "        \n",
    "        # Apply the pattern boost, cap at 1.6 (60% boost max)\n",
    "        boosted_score = min(1.0, weighted_score * min(pattern_boost, 1.6))\n",
    "        \n",
    "        # Apply category-based constraints according to business rule\n",
    "        if categories_match:\n",
    "            # If categories match exactly and name similarity is good (score > 0.60),\n",
    "            # ensure score is at least 0.75\n",
    "            if boosted_score > 0.60:\n",
    "                final_score = max(0.75, boosted_score)\n",
    "            else:\n",
    "                # Categories match but names are too dissimilar, keep original score\n",
    "                final_score = boosted_score\n",
    "        else:\n",
    "            # If categories don't match, cap the score at 0.75 regardless of name similarity\n",
    "            final_score = min(0.75, boosted_score)\n",
    "        \n",
    "        return final_score\n",
    "    \n",
    "    def compute_weighted_score(self, DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category):\n",
    "        \"\"\"\n",
    "        Compute weighted similarity score using domain-specific weights\n",
    "        \n",
    "        Args:\n",
    "            DBAName (str): The DBAName to match\n",
    "            DBA_Merchant_Category (str): Category for the DBA\n",
    "            RawTransactionName (str): The full name to match against\n",
    "            RawTransaction_Merchant_Category (str, optional): Category for the raw transaction\n",
    "            \n",
    "        Returns:\n",
    "            float: Weighted similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Get all similarity scores\n",
    "        all_scores = self.get_all_similarity_scores(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        \n",
    "        # Get dynamic weights based on name characteristics\n",
    "        weights = self.get_dynamic_weights(DBAName, DBA_Merchant_Category, RawTransactionName, RawTransaction_Merchant_Category)\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        weighted_score = 0.0\n",
    "        weights_used = 0.0\n",
    "        \n",
    "        for algo, score in all_scores.items():\n",
    "            if algo in weights:\n",
    "                weighted_score += weights[algo] * score\n",
    "                weights_used += weights[algo]\n",
    "        \n",
    "        # Handle case where some algorithms are missing\n",
    "        if weights_used > 0:\n",
    "            # Normalize by weights actually used\n",
    "            weighted_score /= weights_used\n",
    "        \n",
    "        return weighted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3e042f-7fd9-4bc8-935a-f74b1c252bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedMerchantMatcher(EnhancedMerchantMatcherWithSimilarity):\n",
    "    \"\"\"Optimized version with caching and grouped similarity methods\"\"\"\n",
    "    \n",
    "    def __init__(self, bert_embedder=None):\n",
    "        \"\"\"Initialize with parent constructor\"\"\"\n",
    "        super().__init__(bert_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c96d1f-0361-4640-9fe5-6ca92dae5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Processing Functions\n",
    "        \n",
    "def standardize_column_names(df):\n",
    "    \"\"\"\n",
    "    Standardize column names to ensure consistency\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Map of possible column names to standardized names\n",
    "    column_mappings = {\n",
    "        'Full Name': 'RawTransactionName',\n",
    "        'fullname': 'RawTransactionName',\n",
    "        'full name': 'RawTransactionName',\n",
    "        'Merchant Category': 'RawTransaction_Merchant_Category',\n",
    "        'merchant_category': 'RawTransaction_Merchant_Category',\n",
    "        'Category': 'RawTransaction_Merchant_Category',\n",
    "        'category': 'RawTransaction_Merchant_Category',\n",
    "        'merchant category': 'RawTransaction_Merchant_Category',\n",
    "        'DBAName': 'DBAName',\n",
    "        'Abbreviation': 'DBAName',\n",
    "        'ShortName': 'DBAName',\n",
    "        'Short_Name': 'DBAName',\n",
    "        'short_name': 'DBAName',\n",
    "        'short name': 'DBAName',\n",
    "        # Add DBA_Merchant_Category mappings\n",
    "        'DBA Category': 'DBA_Merchant_Category',\n",
    "        'DBA_Category': 'DBA_Merchant_Category',\n",
    "        'dba category': 'DBA_Merchant_Category',\n",
    "        'dba_category': 'DBA_Merchant_Category',\n",
    "        'DBA Merchant Category': 'DBA_Merchant_Category',\n",
    "        'DBAMerchantCategory': 'DBA_Merchant_Category'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping\n",
    "    for old_name, new_name in column_mappings.items():\n",
    "        if old_name in df_copy.columns:\n",
    "            df_copy.rename(columns={old_name: new_name}, inplace=True)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['DBAName', 'RawTransactionName']\n",
    "    missing_columns = [col for col in required_columns if col not in df_copy.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Required columns {missing_columns} not found in the DataFrame\")\n",
    "    \n",
    "    # Ensure RawTransaction_Merchant_Category exists\n",
    "    if 'RawTransaction_Merchant_Category' not in df_copy.columns:\n",
    "        print(\"Warning: 'RawTransaction_Merchant_Category' column not found. Adding with default value 'Unknown'.\")\n",
    "        df_copy['RawTransaction_Merchant_Category'] = 'Unknown'\n",
    "    \n",
    "    # Ensure DBA_Merchant_Category exists\n",
    "    if 'DBA_Merchant_Category' not in df_copy.columns:\n",
    "        df_copy['DBA_Merchant_Category'] ='Unknown'\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def preprocess_merchant_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess merchant data for matching\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Standardize column names\n",
    "    df = standardize_column_names(df)\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_processed['DBAName'] = df_processed['DBAName'].fillna('').astype(str)\n",
    "    df_processed['RawTransactionName'] = df_processed['RawTransactionName'].fillna('').astype(str)\n",
    "    \n",
    "    # Remove rows with empty DBANames or full names\n",
    "    orig_rows = len(df_processed)\n",
    "    df_processed = df_processed[(df_processed['DBAName'].str.strip() != '') & \n",
    "                                (df_processed['RawTransactionName'].str.strip() != '')]\n",
    "    \n",
    "    if len(df_processed) < orig_rows:\n",
    "        print(f\"Removed {orig_rows - len(df_processed)} rows with empty DBANames or full names\")\n",
    "    \n",
    "    return df_processed\n",
    "    \n",
    "# IMPORTANT: This function should NOT be indented under any other function\n",
    "def process_merchant_data(merchant_df, merchant_matcher):\n",
    "    \"\"\"\n",
    "    Process merchant data with separate categories for DBA and RawTransaction\n",
    "    \"\"\"\n",
    "    # Initialize results DataFrame\n",
    "    results_df = merchant_df.copy()\n",
    "    results_df['Basic_Score'] = 0.0\n",
    "    results_df['Enhanced_Score'] = 0.0\n",
    "    \n",
    "    # Define the batch size and start time for progress tracking\n",
    "    batch_size = 20  # Default batch size for progress reporting\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process each merchant entry\n",
    "    for idx, row in results_df.iterrows():\n",
    "        DBAName = row['DBAName']\n",
    "        DBA_Merchant_Category = row['DBA_Merchant_Category']\n",
    "        RawTransactionName = row['RawTransactionName']\n",
    "        RawTransaction_Merchant_Category = row['RawTransaction_Merchant_Category']\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        try:\n",
    "            basic_score = merchant_matcher.compute_weighted_score(\n",
    "                DBAName, DBA_Merchant_Category, \n",
    "                RawTransactionName, RawTransaction_Merchant_Category\n",
    "            )\n",
    "            \n",
    "            enhanced_score = merchant_matcher.compute_contextual_score(\n",
    "                DBAName, DBA_Merchant_Category,\n",
    "                RawTransactionName, RawTransaction_Merchant_Category\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            basic_score = 0.0\n",
    "            enhanced_score = 0.0\n",
    "        \n",
    "        # Store scores\n",
    "        results_df.at[idx, 'Basic_Score'] = basic_score\n",
    "        results_df.at[idx, 'Enhanced_Score'] = enhanced_score\n",
    "        \n",
    "        # Show progress\n",
    "        if idx % batch_size == 0 or idx == len(results_df) - 1:\n",
    "            progress = (idx + 1) / len(results_df) * 100\n",
    "            elapsed = time.time() - start_time\n",
    "            remaining = elapsed / (idx + 1) * (len(results_df) - idx - 1) if idx > 0 else 0\n",
    "            print(f\"Progress: {progress:.1f}% ({idx+1}/{len(results_df)}) - \"\n",
    "                  f\"Elapsed: {elapsed:.1f}s - Est. remaining: {remaining:.1f}s\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Processing completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75b65b43-e816-43a9-893d-3fadd4fc59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Comprehensive Analysis and Pipeline Functions\n",
    "\n",
    "def run_comprehensive_analysis(input_file, matcher=None):\n",
    "    \"\"\"\n",
    "    Run a comprehensive analysis of merchant matching results\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to merchant data file\n",
    "        matcher: Optional merchant matcher object to use for analysis\n",
    "        \n",
    "    Returns:\n",
    "        dict: Comprehensive analysis results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Running comprehensive analysis on {input_file}...\")\n",
    "    \n",
    "    # Check if matcher was provided, otherwise use global merchant_matcher\n",
    "    if matcher is None:\n",
    "        # Use the global one, but warn if it's None\n",
    "        global merchant_matcher\n",
    "        if merchant_matcher is None:\n",
    "            print(\"Warning: No merchant matcher provided and global merchant_matcher is None.\")\n",
    "    else:\n",
    "        # Use the provided matcher\n",
    "        merchant_matcher = matcher\n",
    "    \n",
    "    # Step 6: Visualize Results\n",
    "    print(\"\\n=== Step 6: Visualizing Results ===\")\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs('analysis_output', exist_ok=True)\n",
    "        \n",
    "        # 1. Score Distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(results_df['Enhanced_Score'], bins=20, kde=True)\n",
    "        plt.title('Distribution of Enhanced Matching Scores')\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('analysis_output/score_distribution.png')\n",
    "        \n",
    "        # 2. Score distribution by range\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        score_labels = []\n",
    "        score_counts = []\n",
    "        \n",
    "        for min_val, max_val, label in score_ranges:\n",
    "            count = len(results_df[(results_df['Enhanced_Score'] >= min_val) & \n",
    "                                  (results_df['Enhanced_Score'] <= max_val)])\n",
    "            score_labels.append(label)\n",
    "            score_counts.append(count)\n",
    "        \n",
    "        plt.bar(score_labels, score_counts)\n",
    "        plt.title('Score Distribution by Range')\n",
    "        plt.xlabel('Score Range')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('analysis_output/score_distribution_ranges.png')\n",
    "        \n",
    "        # 3. Score by DBAName Length\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='DBAName_Length', y='Enhanced_Score', data=results_df)\n",
    "        plt.title('Score by DBAName Length')\n",
    "        plt.xlabel('DBAName Length')\n",
    "        plt.ylabel('Enhanced Score')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('analysis_output/score_by_length.png')\n",
    "        \n",
    "        print(\"Visualizations saved to 'analysis_output' directory\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualizations: {e}\")\n",
    "    \n",
    "    # Calculate total execution time\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nComprehensive analysis completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Return all results\n",
    "    return {\n",
    "        #'basic_statistics': {\n",
    "         #   'avg_score': avg_score,\n",
    "          #  'median_score': median_score,\n",
    "           # 'std_score': std_score,\n",
    "            #'min_score': min_score,\n",
    "            #'max_score': max_score\n",
    "        #},\n",
    "        ##'score_distribution': score_distribution,\n",
    "        ##'algorithm_analysis': algorithm_scores,\n",
    "        #'category_analysis': category_analysis,\n",
    "        #'length_analysis': length_groups,\n",
    "        'execution_time': total_time,\n",
    "        #'results_df': results_df\n",
    "    }\n",
    "\n",
    "def run_merchant_pipeline(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Run the merchant matching pipeline on an actual file (no synthetic data)\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input file with merchant data\n",
    "        output_file: Path to save results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Results of merchant matching\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Enhanced Merchant Name Matching - Pipeline\".center(80))\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Step 1: Load and preprocess data\n",
    "    print(f\"\\n1. Loading data from {input_file}...\")\n",
    "    try:\n",
    "        if input_file.lower().endswith('.csv'):\n",
    "            merchant_df = pd.read_csv(input_file)\n",
    "        else:\n",
    "            merchant_df = pd.read_excel(input_file)\n",
    "        \n",
    "        print(f\"Successfully loaded {len(merchant_df)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "    \n",
    "    print(\"\\n2. Preprocessing data...\")\n",
    "    processed_df = preprocess_merchant_data(merchant_df)\n",
    "    print(f\"Preprocessed {len(processed_df)} merchant records\")\n",
    "    \n",
    "    # Step 3: Process with merchant matcher\n",
    "    print(\"\\n3. Running merchant matching algorithms...\")\n",
    "    results_df = process_merchant_data(processed_df, merchant_matcher)\n",
    "\n",
    "        # Calculate basic statistics\n",
    "    avg_score = results_df['Enhanced_Score'].mean()\n",
    "    median_score = results_df['Enhanced_Score'].median()\n",
    "    std_score = results_df['Enhanced_Score'].std()\n",
    "    min_score = results_df['Enhanced_Score'].min()\n",
    "    max_score = results_df['Enhanced_Score'].max()\n",
    "\n",
    "\n",
    "    print(f\"Score Statistics:\")\n",
    "    print(f\"  Average Score: {avg_score:.4f}\")\n",
    "    print(f\"  Median Score: {median_score:.4f}\")\n",
    "    print(f\"  Standard Deviation: {std_score:.4f}\")\n",
    "    print(f\"  Range: {min_score:.4f} - {max_score:.4f}\")\n",
    "    \n",
    "    # Step 4: Print score distribution statistics\n",
    "    print(\"\\n4. Score distribution statistics:\")\n",
    "    score_ranges = [\n",
    "        (0.0, 0.5, 'Low (0.0-0.5)'),\n",
    "        (0.5, 0.65, 'Moderate (0.5-0.65)'),\n",
    "        (0.65, 0.75, 'Medium (0.65-0.75)'),\n",
    "        (0.75, 0.85, 'High (0.75-0.85)'),\n",
    "        (0.85, 0.95, 'Very High (0.85-0.95)'),\n",
    "        (0.95, 1.0, 'Excellent (0.95-1.0)')\n",
    "    ]\n",
    "    \n",
    "    for min_val, max_val, label in score_ranges:\n",
    "        count = len(results_df[(results_df['Enhanced_Score'] >= min_val) & \n",
    "                             (results_df['Enhanced_Score'] <= max_val)])\n",
    "        percentage = count / len(results_df) * 100\n",
    "        print(f\"  {label}: {count} entries ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    if output_file:\n",
    "        print(f\"\\n5. Saving results to {output_file}...\")\n",
    "        try:\n",
    "            # Create a writer for Excel output\n",
    "            with pd.ExcelWriter(output_file) as writer:\n",
    "                # Main results sheet\n",
    "                results_df.to_excel(writer, sheet_name=\"Matching_Results\", index=False)\n",
    "                \n",
    "                # Summary sheet - score distribution instead of categories\n",
    "                score_data = []\n",
    "                for min_val, max_val, label in score_ranges:\n",
    "                    count = len(results_df[(results_df['Enhanced_Score'] >= min_val) & \n",
    "                                         (results_df['Enhanced_Score'] <= max_val)])\n",
    "                    percentage = count / len(results_df) * 100\n",
    "                    score_data.append({\n",
    "                        'Score_Range': label,\n",
    "                        'Count': count,\n",
    "                        'Percentage': percentage\n",
    "                    })\n",
    "                \n",
    "                summary_df = pd.DataFrame(score_data)\n",
    "                summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "                \n",
    "                # Sample analysis sheet (top algorithms for sample entries)\n",
    "                sample_size = min(50, len(results_df))\n",
    "                sample_df = results_df.nlargest(sample_size, 'Enhanced_Score')\n",
    "                \n",
    "                analysis_rows = []\n",
    "                for _, row in sample_df.iterrows():\n",
    "                    # Get algorithm scores\n",
    "                    all_scores = merchant_matcher.get_all_similarity_scores(\n",
    "                        row['DBAName'],\n",
    "                        row.get('DBA_Merchant_Category', row.get('Merchant_Category', None)),\n",
    "                        row['RawTransactionName'],\n",
    "                        row.get('RawTransaction_Merchant_Category', row.get('Merchant_Category', None))\n",
    "                    )\n",
    "                    \n",
    "                    analysis_row = {\n",
    "                        'DBAName': row['DBAName'],\n",
    "                        'RawTransactionName': row['RawTransactionName'],\n",
    "                        'Enhanced_Score': row['Enhanced_Score']\n",
    "                    }\n",
    "                    \n",
    "                    for algo, score in all_scores.items():\n",
    "                        analysis_row[algo] = score\n",
    "                    \n",
    "                    analysis_rows.append(analysis_row)\n",
    "                \n",
    "                if analysis_rows:\n",
    "                    analysis_df = pd.DataFrame(analysis_rows)\n",
    "                    analysis_df.to_excel(writer, sheet_name=\"Sample_Analysis\", index=False)\n",
    "                \n",
    "            print(f\"Results successfully saved to {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "    \n",
    "    # Return the results DataFrame\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bd57910-f3e1-4533-9a0f-ac7ff08010f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "              Enhanced Merchant Name Matching - Optimized Pipeline              \n",
      "================================================================================\n",
      "\n",
      "\n",
      "Step 1: Initializing optimized matcher...\n",
      "Loading enhanced BERT model 'sentence-transformers/all-mpnet-base-v2'...\n",
      "Enhanced BERT model loaded successfully on cpu\n",
      "Optimized merchant matcher initialized successfully with caching and grouped algorithms!\n",
      "\n",
      "Step 2-5: Running merchant pipeline...\n",
      "\n",
      "================================================================================\n",
      "                   Enhanced Merchant Name Matching - Pipeline                   \n",
      "================================================================================\n",
      "\n",
      "\n",
      "1. Loading data from Acronym_extramcc.xlsx...\n",
      "Successfully loaded 273 records\n",
      "\n",
      "2. Preprocessing data...\n",
      "Preprocessed 273 merchant records\n",
      "\n",
      "3. Running merchant matching algorithms...\n",
      "Progress: 0.4% (1/273) - Elapsed: 0.3s - Est. remaining: 0.0s\n",
      "Progress: 7.7% (21/273) - Elapsed: 8.4s - Est. remaining: 101.3s\n",
      "Progress: 15.0% (41/273) - Elapsed: 14.7s - Est. remaining: 83.3s\n",
      "Progress: 22.3% (61/273) - Elapsed: 21.5s - Est. remaining: 74.6s\n",
      "Progress: 29.7% (81/273) - Elapsed: 28.1s - Est. remaining: 66.5s\n",
      "Progress: 37.0% (101/273) - Elapsed: 34.7s - Est. remaining: 59.0s\n",
      "Progress: 44.3% (121/273) - Elapsed: 43.8s - Est. remaining: 55.1s\n",
      "Progress: 51.6% (141/273) - Elapsed: 50.5s - Est. remaining: 47.3s\n",
      "Progress: 59.0% (161/273) - Elapsed: 57.6s - Est. remaining: 40.1s\n",
      "Progress: 66.3% (181/273) - Elapsed: 64.1s - Est. remaining: 32.6s\n",
      "Progress: 73.6% (201/273) - Elapsed: 72.0s - Est. remaining: 25.8s\n",
      "Progress: 81.0% (221/273) - Elapsed: 78.6s - Est. remaining: 18.5s\n",
      "Progress: 88.3% (241/273) - Elapsed: 85.6s - Est. remaining: 11.4s\n",
      "Progress: 95.6% (261/273) - Elapsed: 93.0s - Est. remaining: 4.3s\n",
      "Progress: 100.0% (273/273) - Elapsed: 100.0s - Est. remaining: 0.0s\n",
      "Processing completed in 100.02 seconds\n",
      "Score Statistics:\n",
      "  Average Score: 0.5800\n",
      "  Median Score: 0.5367\n",
      "  Standard Deviation: 0.2184\n",
      "  Range: 0.1245 - 1.0000\n",
      "\n",
      "4. Score distribution statistics:\n",
      "  Low (0.0-0.5): 110 entries (40.29%)\n",
      "  Moderate (0.5-0.65): 54 entries (19.78%)\n",
      "  Medium (0.65-0.75): 53 entries (19.41%)\n",
      "  High (0.75-0.85): 76 entries (27.84%)\n",
      "  Very High (0.85-0.95): 18 entries (6.59%)\n",
      "  Excellent (0.95-1.0): 13 entries (4.76%)\n",
      "\n",
      "5. Saving results to Merchant_Matching_Results.xlsx...\n",
      "Results successfully saved to Merchant_Matching_Results.xlsx\n",
      "\n",
      "Step 7: Running comprehensive analysis with visualizations...\n",
      "Running comprehensive analysis on Acronym_extramcc.xlsx...\n",
      "\n",
      "=== Step 6: Visualizing Results ===\n",
      "Error creating visualizations: name 'results_df' is not defined\n",
      "\n",
      "Comprehensive analysis completed in 0.00 seconds\n",
      "Comprehensive analysis complete. Visualizations saved to 'analysis_output' directory.\n",
      "\n",
      "Visualizations generated:\n",
      "  - Score distribution: analysis_output/score_distribution.png\n",
      "  - Category distribution: analysis_output/category_distribution.png\n",
      "  - Score by length: analysis_output/score_by_length.png\n",
      "\n",
      "Merchant matching pipeline completed in 109.92 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: Usage Examples and Main Execution\n",
    "\n",
    "# This cell contains usage examples and demonstrations\n",
    "merchant_matcher = None\n",
    "DBA_Merchant_Category = None\n",
    "RawTransaction_Merchant_Category = None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the optimized merchant matching pipeline with proper error handling.\n",
    "    Uses real data files only, with no synthetic data generation.\n",
    "    Includes comprehensive analysis with visualizations.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Dictionary to store all results\n",
    "   # results = {\n",
    "    #    'results_df': None,\n",
    "     #   'comprehensive_analysis': None\n",
    "    #}\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Enhanced Merchant Name Matching - Optimized Pipeline\".center(80))\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Check for input file\n",
    "        input_file = \"Acronym_extramcc.xlsx\"\n",
    "        output_file = \"Merchant_Matching_Results.xlsx\"\n",
    "        #If you ever need to change the file paths based on user input or configuration,\n",
    "        #having them defined in main() makes this easier\n",
    "        \n",
    "        # Step 1: Initialize optimized matcher\n",
    "        print(\"\\nStep 1: Initializing optimized matcher...\")\n",
    "        bert_embedder = EnhancedBERTEmbedder(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "        \n",
    "        # Use the OptimizedMerchantMatcher instead of EnhancedMerchantMatcherWithSimilarity\n",
    "        global merchant_matcher  # Make it available to run_merchant_pipeline\n",
    "        merchant_matcher = OptimizedMerchantMatcher(bert_embedder=bert_embedder)\n",
    "        \n",
    "        print(f\"Optimized merchant matcher initialized successfully with caching and grouped algorithms!\")\n",
    "        \n",
    "        # Step 2-5: Run the merchant pipeline (replaces duplicate code)\n",
    "        print(\"\\nStep 2-5: Running merchant pipeline...\")\n",
    "        results_df = run_merchant_pipeline(input_file, output_file)\n",
    "        #results['results_df'] = results_df\n",
    "\n",
    "        # Step 6: Run comprehensive analysis with visualizations\n",
    "        print(\"\\nStep 7: Running comprehensive analysis with visualizations...\")\n",
    "        comprehensive_results = run_comprehensive_analysis(input_file, merchant_matcher)\n",
    "        #results['comprehensive_analysis'] = comprehensive_results\n",
    "        print(\"Comprehensive analysis complete. Visualizations saved to 'analysis_output' directory.\")\n",
    "        \n",
    "        # Step 7: Highlight visualization locations\n",
    "        print(\"\\nVisualizations generated:\")\n",
    "        print(\"  - Score distribution: analysis_output/score_distribution.png\")\n",
    "        print(\"  - Category distribution: analysis_output/category_distribution.png\") \n",
    "        print(\"  - Score by length: analysis_output/score_by_length.png\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in merchant matching pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Calculate total execution time\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nMerchant matching pipeline completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    #return results\n",
    "\n",
    "# Add the following at the end of the script to execute when run directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440b285-b4a6-4f4b-95c2-7bf35c084fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
