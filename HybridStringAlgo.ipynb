{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d35a7e9-b631-4e96-9338-176f13b66ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import jellyfish  # For Jaro-Winkler and Soundex\n",
    "import Levenshtein  # For Levenshtein and Damerau-Levenshtein\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d07b509-96c2-4980-9d68-cd3d74443ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "class MerchantNamePreprocessor:\n",
    "    def __init__(self):\n",
    "        # Comprehensive abbreviation mapping\n",
    "        self.abbreviation_map = {\n",
    "            # Banking and Financial\n",
    "            'bofa': 'bank of america', 'b of a': 'bank of america', 'boa': 'bank of america',\n",
    "            'wf': 'wells fargo', 'wfb': 'wells fargo bank', 'citi': 'citibank',\n",
    "            'amex': 'american express', 'chase': 'jpmorgan chase', 'jpm': 'jpmorgan chase',\n",
    "            'pnc': 'pnc bank', 'usb': 'us bank', 'usaa': 'united services automobile association',\n",
    "            \n",
    "            # Retail\n",
    "            'wm': 'walmart', 'wmt': 'walmart', 'tgt': 'target', \n",
    "            'amzn': 'amazon', 'costco': 'costco wholesale',\n",
    "            'hd': 'home depot', 'low': 'lowes', 'bby': 'best buy',\n",
    "            \n",
    "            # Food & Beverage\n",
    "            'mcd': 'mcdonalds', 'mcds': 'mcdonalds', 'bk': 'burger king',\n",
    "            'sbux': 'starbucks', 'sb': 'starbucks', 'kfc': 'kentucky fried chicken',\n",
    "            'tbell': 'taco bell', 'dq': 'dairy queen', 'ihop': 'international house of pancakes',\n",
    "            \n",
    "            # Gas & Convenience\n",
    "            '7-11': '7-eleven', '711': '7-eleven', 'bp': 'british petroleum',\n",
    "            'chev': 'chevron', 'esso': 'exxon mobil', 'exm': 'exxon mobil',\n",
    "            \n",
    "            # Address terminology\n",
    "            'st': 'street', 'rd': 'road', 'dr': 'drive', 'ave': 'avenue',\n",
    "            'blvd': 'boulevard', 'ln': 'lane', 'ct': 'court', 'hwy': 'highway',\n",
    "            'plz': 'plaza', 'sq': 'square', 'ctr': 'center'\n",
    "        }\n",
    "        \n",
    "        # Business entity terms to remove\n",
    "        self.business_terms = {\n",
    "            'inc', 'incorporated', 'llc', 'ltd', 'limited', 'corp', 'corporation',\n",
    "            'co', 'company', 'group', 'holdings', 'plc', 'enterprises', 'partners', \n",
    "            'lp', 'international', 'worldwide', 'global', 'national', 'regional',\n",
    "            'services', 'solutions', 'industries', 'technologies', 'systems'\n",
    "        }\n",
    "        \n",
    "        # Common prefixes to standardize\n",
    "        self.prefixes = {\n",
    "            'the ': '', 'a ': '', 'an ': ''\n",
    "        }\n",
    "\n",
    "    def preprocess(self, name):\n",
    "        \"\"\"\n",
    "        Apply comprehensive preprocessing to merchant names to standardize them\n",
    "        \"\"\"\n",
    "        if not name or not isinstance(name, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        name = name.lower()\n",
    "        \n",
    "        # Replace ampersands and plus signs with 'and'\n",
    "        name = name.replace('&', ' and ').replace('+', ' and ')\n",
    "        \n",
    "        # Remove punctuation (except spaces)\n",
    "        name = re.sub(r'[^\\w\\s]', ' ', name)\n",
    "        \n",
    "        # Replace multiple spaces with a single space\n",
    "        name = re.sub(r'\\s+', ' ', name)\n",
    "        \n",
    "        # Remove leading/trailing spaces\n",
    "        name = name.strip()\n",
    "        \n",
    "        # Remove common prefixes\n",
    "        for prefix, replacement in self.prefixes.items():\n",
    "            if name.startswith(prefix):\n",
    "                name = replacement + name[len(prefix):]\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = name.split()\n",
    "        \n",
    "        # Expand abbreviations\n",
    "        expanded_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.abbreviation_map:\n",
    "                expanded_tokens.extend(self.abbreviation_map[token].split())\n",
    "            else:\n",
    "                expanded_tokens.append(token)\n",
    "        \n",
    "        # Remove business entity terms (usually at the end)\n",
    "        filtered_tokens = []\n",
    "        for token in expanded_tokens:\n",
    "            if token not in self.business_terms:\n",
    "                filtered_tokens.append(token)\n",
    "        \n",
    "        # Join back to string\n",
    "        processed_name = ' '.join(filtered_tokens)\n",
    "        \n",
    "        return processed_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34285796-5055-4818-bb33-0ec6709d4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# featur extract with similarity method\n",
    "class SimilarityFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(analyzer='word')\n",
    "        self.preprocessor = MerchantNamePreprocessor()\n",
    "    \n",
    "    def extract_features(self, name1, name2):\n",
    "        \"\"\"\n",
    "        Extract multiple similarity features between two merchant names\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary of similarity features\n",
    "        \"\"\"\n",
    "        # Preprocess both names\n",
    "        processed_name1 = self.preprocessor.preprocess(name1)\n",
    "        processed_name2 = self.preprocessor.preprocess(name2)\n",
    "        \n",
    "        # If either name is empty after preprocessing, return low similarity\n",
    "        if not processed_name1 or not processed_name2:\n",
    "            return {\n",
    "                'jaro_winkler': 0.0,\n",
    "                'damerau_levenshtein': 0.0,\n",
    "                'tfidf_cosine': 0.0,\n",
    "                'jaccard_bigram': 0.0,\n",
    "                'soundex_match': 0.0,\n",
    "                'token_sort_ratio': 0.0,\n",
    "                'contains_ratio': 0.0\n",
    "            }\n",
    "        \n",
    "        # 1. Jaro-Winkler similarity\n",
    "        jaro_winkler = jellyfish.jaro_winkler_similarity(processed_name1, processed_name2)\n",
    "        \n",
    "        # 2. Damerau-Levenshtein distance (normalized to similarity)\n",
    "        dl_distance = Levenshtein.distance(processed_name1, processed_name2)\n",
    "        max_len = max(len(processed_name1), len(processed_name2))\n",
    "        damerau_levenshtein = 1 - (dl_distance / max_len if max_len > 0 else 0)\n",
    "        \n",
    "        # 3. TF-IDF Cosine Similarity\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform([processed_name1, processed_name2])\n",
    "            tfidf_cosine = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        except:\n",
    "            tfidf_cosine = 0.0\n",
    "        \n",
    "        # 4. Jaccard similarity of character bigrams\n",
    "        def get_bigrams(text):\n",
    "            return set(text[i:i+2] for i in range(len(text)-1))\n",
    "        \n",
    "        bigrams1 = get_bigrams(processed_name1)\n",
    "        bigrams2 = get_bigrams(processed_name2)\n",
    "        jaccard_bigram = len(bigrams1 & bigrams2) / len(bigrams1 | bigrams2) if len(bigrams1 | bigrams2) > 0 else 0\n",
    "        \n",
    "        # 5. Soundex phonetic matching\n",
    "        soundex1 = jellyfish.soundex(processed_name1.split()[0] if processed_name1.split() else \"\")\n",
    "        soundex2 = jellyfish.soundex(processed_name2.split()[0] if processed_name2.split() else \"\")\n",
    "        soundex_match = 1.0 if soundex1 == soundex2 else 0.0\n",
    "        \n",
    "        # 6. Token sort ratio (handles word order variation)\n",
    "        sorted_name1 = ' '.join(sorted(processed_name1.split()))\n",
    "        sorted_name2 = ' '.join(sorted(processed_name2.split()))\n",
    "        token_sort_ratio = jellyfish.jaro_winkler_similarity(sorted_name1, sorted_name2)\n",
    "        \n",
    "        # 7. Contains ratio (checks if one name is contained within the other)\n",
    "        contains_ratio = 0.0\n",
    "        if processed_name1 in processed_name2 or processed_name2 in processed_name1:\n",
    "            min_len = min(len(processed_name1), len(processed_name2))\n",
    "            max_len = max(len(processed_name1), len(processed_name2))\n",
    "            contains_ratio = min_len / max_len if max_len > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'jaro_winkler': jaro_winkler,\n",
    "            'damerau_levenshtein': damerau_levenshtein,\n",
    "            'tfidf_cosine': tfidf_cosine,\n",
    "            'jaccard_bigram': jaccard_bigram,\n",
    "            'soundex_match': soundex_match,\n",
    "            'token_sort_ratio': token_sort_ratio,\n",
    "            'contains_ratio': contains_ratio\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c0f7c4-9c7e-47dc-a7a1-013615b5f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train\n",
    "class MerchantNameMatcher:\n",
    "    def __init__(self):\n",
    "        self.feature_extractor = SimilarityFeatureExtractor()\n",
    "        self.model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.1,\n",
    "            objective='binary:logistic',\n",
    "            random_state=42\n",
    "        )\n",
    "        self.feature_names = [\n",
    "            'jaro_winkler', 'damerau_levenshtein', 'tfidf_cosine',\n",
    "            'jaccard_bigram', 'soundex_match', 'token_sort_ratio', 'contains_ratio'\n",
    "        ]\n",
    "    \n",
    "    def prepare_training_data(self, df, name1_col, name2_col, label_col):\n",
    "        \"\"\"\n",
    "        Extract features from merchant name pairs and prepare training data\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with merchant name pairs and match labels\n",
    "            name1_col: Column name for first merchant name\n",
    "            name2_col: Column name for second merchant name\n",
    "            label_col: Column name for match label (1 = match, 0 = no match)\n",
    "            \n",
    "        Returns:\n",
    "            X: Feature matrix\n",
    "            y: Target vector\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = df[label_col].values\n",
    "        \n",
    "        logger.info(f\"Extracting features from {len(df)} merchant name pairs...\")\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            features = self.feature_extractor.extract_features(row[name1_col], row[name2_col])\n",
    "            X.append([features[feature] for feature in self.feature_names])\n",
    "            \n",
    "            # Log progress every 1000 rows\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                logger.info(f\"Processed {i} rows...\")\n",
    "        \n",
    "        return np.array(X), y\n",
    "    \n",
    "    def train(self, df, name1_col, name2_col, label_col, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train the ensemble model on labeled merchant name pairs\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with merchant name pairs and match labels\n",
    "            name1_col: Column name for first merchant name\n",
    "            name2_col: Column name for second merchant name\n",
    "            label_col: Column name for match label (1 = match, 0 = no match)\n",
    "            test_size: Proportion of data to use for testing\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        logger.info(\"Preparing training data...\")\n",
    "        X, y = self.prepare_training_data(df, name1_col, name2_col, label_col)\n",
    "        \n",
    "        # Split into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training model on {len(X_train)} samples...\")\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        logger.info(f\"Evaluating model on {len(X_test)} samples...\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        logger.info(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "        logger.info(\"\\nClassification Report:\\n\" + classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Also perform cross-validation for robustness\n",
    "        logger.info(\"Performing 5-fold cross-validation...\")\n",
    "        cv_scores = cross_val_score(self.model, X, y, cv=5, scoring='f1')\n",
    "        logger.info(f\"Cross-validated F1 scores: {cv_scores}\")\n",
    "        logger.info(f\"Mean CV F1 score: {cv_scores.mean():.4f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        importance = self.model.feature_importances_\n",
    "        for i, feature_name in enumerate(self.feature_names):\n",
    "            logger.info(f\"Feature importance - {feature_name}: {importance[i]:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'cv_f1_mean': cv_scores.mean(),\n",
    "            'feature_importance': dict(zip(self.feature_names, importance))\n",
    "        }\n",
    "    \n",
    "    def predict(self, name1, name2):\n",
    "        \"\"\"\n",
    "        Predict whether two merchant names match\n",
    "        \n",
    "        Returns:\n",
    "            float: Probability of match (0-1)\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor.extract_features(name1, name2)\n",
    "        X = [[features[feature] for feature in self.feature_names]]\n",
    "        return self.model.predict_proba(X)[0][1]\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model to a file\"\"\"\n",
    "        self.model.save_model(filepath)\n",
    "        logger.info(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model from a file\"\"\"\n",
    "        self.model.load_model(filepath)\n",
    "        logger.info(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64a54006-70f0-4ffb-92c4-58178ae7472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:57:06,421 - INFO - Creating sample dataset...\n",
      "2025-03-20 16:57:06,442 - INFO - Sample dataset created with 1000 rows\n",
      "2025-03-20 16:57:06,447 - INFO - Match ratio: 0.48\n",
      "2025-03-20 16:57:06,449 - INFO - Preparing training data...\n",
      "2025-03-20 16:57:06,450 - INFO - Extracting features from 1000 merchant name pairs...\n",
      "2025-03-20 16:57:08,671 - INFO - Training model on 800 samples...\n",
      "2025-03-20 16:57:08,720 - INFO - Evaluating model on 200 samples...\n",
      "2025-03-20 16:57:08,732 - INFO - Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "2025-03-20 16:57:08,742 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       105\n",
      "           1       1.00      1.00      1.00        95\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n",
      "2025-03-20 16:57:08,743 - INFO - Performing 5-fold cross-validation...\n",
      "2025-03-20 16:57:08,904 - INFO - Cross-validated F1 scores: [1. 1. 1. 1. 1.]\n",
      "2025-03-20 16:57:08,906 - INFO - Mean CV F1 score: 1.0000\n",
      "2025-03-20 16:57:08,909 - INFO - Feature importance - jaro_winkler: 0.5669\n",
      "2025-03-20 16:57:08,910 - INFO - Feature importance - damerau_levenshtein: 0.0015\n",
      "2025-03-20 16:57:08,910 - INFO - Feature importance - tfidf_cosine: 0.0000\n",
      "2025-03-20 16:57:08,911 - INFO - Feature importance - jaccard_bigram: 0.1788\n",
      "2025-03-20 16:57:08,912 - INFO - Feature importance - soundex_match: 0.2521\n",
      "2025-03-20 16:57:08,913 - INFO - Feature importance - token_sort_ratio: 0.0007\n",
      "2025-03-20 16:57:08,914 - INFO - Feature importance - contains_ratio: 0.0000\n",
      "2025-03-20 16:57:08,919 - INFO - Model saved to merchant_matcher.json\n",
      "2025-03-20 16:57:08,921 - INFO - \n",
      "Testing model with example pairs:\n",
      "2025-03-20 16:57:08,928 - INFO - 'Bank of America, N.A.' vs 'BOFA': Match probability = 0.9989\n",
      "2025-03-20 16:57:08,935 - INFO - 'MacDonald's' vs 'McD': Match probability = 0.9989\n",
      "2025-03-20 16:57:08,941 - INFO - 'Walmart SuperCenter' vs 'Wal-mart': Match probability = 0.9933\n",
      "2025-03-20 16:57:08,946 - INFO - '7-11' vs '7 Eleven': Match probability = 0.9742\n",
      "2025-03-20 16:57:08,952 - INFO - 'Bank of America' vs 'Wells Fargo': Match probability = 0.0014\n",
      "2025-03-20 16:57:08,958 - INFO - 'Target' vs 'Walmart': Match probability = 0.0019\n"
     ]
    }
   ],
   "source": [
    "# test sample data\n",
    "def create_sample_dataset(size=1000, match_ratio=0.5):\n",
    "    \"\"\"Create a synthetic dataset for testing purposes\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Sample merchant names\n",
    "    merchants = [\n",
    "        \"Bank of America\", \"BOFA\", \"B of A\", \"Bank of America, N.A.\",\n",
    "        \"McDonald's\", \"McDonalds\", \"McD\", \"McDonald's Restaurant\",\n",
    "        \"Walmart\", \"Wal-Mart\", \"Walmart Supercenter\", \"Walmart Inc.\",\n",
    "        \"Starbucks\", \"Starbucks Coffee\", \"SBUX\", \"Starbucks Corp\",\n",
    "        \"Target\", \"Target Store\", \"TGT\", \"Target Corporation\",\n",
    "        \"Home Depot\", \"The Home Depot\", \"HD\", \"Home Depot Inc.\",\n",
    "        \"Wells Fargo\", \"Wells Fargo Bank\", \"WF\", \"Wells Fargo & Co.\",\n",
    "        \"CVS\", \"CVS Pharmacy\", \"CVS Health\", \"CVS Caremark\",\n",
    "        \"7-Eleven\", \"7-11\", \"7 Eleven\", \"7-Eleven Inc.\",\n",
    "        \"Amazon\", \"Amazon.com\", \"AMZN\", \"Amazon Inc.\"\n",
    "    ]\n",
    "    \n",
    "    # Generate pairs and labels\n",
    "    pairs = []\n",
    "    for _ in range(size):\n",
    "        if np.random.random() < match_ratio:  # Create a matching pair\n",
    "            merchant_group = np.random.randint(0, len(merchants) // 4)\n",
    "            idx1 = merchant_group * 4 + np.random.randint(0, 4)\n",
    "            idx2 = merchant_group * 4 + np.random.randint(0, 4)\n",
    "            pairs.append((merchants[idx1], merchants[idx2], 1))\n",
    "        else:  # Create a non-matching pair\n",
    "            group1 = np.random.randint(0, len(merchants) // 4)\n",
    "            group2 = np.random.randint(0, len(merchants) // 4)\n",
    "            while group1 == group2:\n",
    "                group2 = np.random.randint(0, len(merchants) // 4)\n",
    "            idx1 = group1 * 4 + np.random.randint(0, 4)\n",
    "            idx2 = group2 * 4 + np.random.randint(0, 4)\n",
    "            pairs.append((merchants[idx1], merchants[idx2], 0))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(pairs, columns=['name1', 'name2', 'is_match'])\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate merchant name matching\"\"\"\n",
    "    logger.info(\"Creating sample dataset...\")\n",
    "    df = create_sample_dataset(size=1000, match_ratio=0.5)\n",
    "    \n",
    "    logger.info(f\"Sample dataset created with {len(df)} rows\")\n",
    "    logger.info(f\"Match ratio: {df['is_match'].mean():.2f}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    matcher = MerchantNameMatcher()\n",
    "    metrics = matcher.train(df, 'name1', 'name2', 'is_match', test_size=0.2)\n",
    "    \n",
    "    # Save the model\n",
    "    matcher.save_model(\"merchant_matcher.json\")\n",
    "    \n",
    "    # Test with some example pairs\n",
    "    test_pairs = [\n",
    "        (\"Bank of America, N.A.\", \"BOFA\"),\n",
    "        (\"MacDonald's\", \"McD\"),\n",
    "        (\"Walmart SuperCenter\", \"Wal-mart\"),\n",
    "        (\"7-11\", \"7 Eleven\"),\n",
    "        (\"Bank of America\", \"Wells Fargo\"),\n",
    "        (\"Target\", \"Walmart\")\n",
    "    ]\n",
    "    \n",
    "    logger.info(\"\\nTesting model with example pairs:\")\n",
    "    for name1, name2 in test_pairs:\n",
    "        match_prob = matcher.predict(name1, name2)\n",
    "        logger.info(f\"'{name1}' vs '{name2}': Match probability = {match_prob:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65d39d1a-a8e3-450f-b4a9-ab0b1f3839fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark for large scale\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, FloatType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "class SparkMerchantNameMatcher:\n",
    "    def __init__(self, spark):\n",
    "        \"\"\"\n",
    "        Initialize the SparkMerchantNameMatcher\n",
    "        \n",
    "        Args:\n",
    "            spark: SparkSession instance\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.preprocessor = MerchantNamePreprocessor()\n",
    "        self.matcher = MerchantNameMatcher()\n",
    "        \n",
    "        # Register UDFs\n",
    "        self.preprocess_udf = udf(self.preprocessor.preprocess, StringType())\n",
    "        \n",
    "        # Define schema for features\n",
    "        self.features_schema = ArrayType(FloatType())\n",
    "    \n",
    "    def extract_features_udf(self, name1, name2):\n",
    "        \"\"\"UDF to extract features from a pair of merchant names\"\"\"\n",
    "        features = self.matcher.feature_extractor.extract_features(name1, name2)\n",
    "        return [float(features[feature]) for feature in self.matcher.feature_names]\n",
    "    \n",
    "    def prepare_pipeline(self):\n",
    "        \"\"\"Register necessary UDFs for the pipeline\"\"\"\n",
    "        # Register extract_features_udf\n",
    "        extract_features_udf_registered = udf(self.extract_features_udf, self.features_schema)\n",
    "        self.spark.udf.register(\"extract_features\", extract_features_udf_registered)\n",
    "        \n",
    "        # Load trained model parameters\n",
    "        # In a real implementation, you would broadcast the model parameters\n",
    "        # For now, we'll assume the model is loaded in the matcher\n",
    "        \n",
    "        # Register predict UDF\n",
    "        def predict_match(features):\n",
    "            # Convert features to numpy array\n",
    "            features_np = np.array(features).reshape(1, -1)\n",
    "            # Use loaded model to predict\n",
    "            return float(self.matcher.model.predict_proba(features_np)[0][1])\n",
    "        \n",
    "        predict_udf = udf(predict_match, DoubleType())\n",
    "        self.spark.udf.register(\"predict_match\", predict_udf)\n",
    "    \n",
    "    def process_merchant_pairs(self, df, name1_col, name2_col):\n",
    "        \"\"\"\n",
    "        Process merchant name pairs and predict matches\n",
    "        \n",
    "        Args:\n",
    "            df: Spark DataFrame with merchant name pairs\n",
    "            name1_col: Column name for first merchant name\n",
    "            name2_col: Column name for second merchant name\n",
    "            \n",
    "        Returns:\n",
    "            Spark DataFrame with match predictions\n",
    "        \"\"\"\n",
    "        # Prepare pipeline\n",
    "        self.prepare_pipeline()\n",
    "        \n",
    "        # Preprocess names\n",
    "        df = df.withColumn(\"processed_name1\", self.preprocess_udf(col(name1_col)))\n",
    "        df = df.withColumn(\"processed_name2\", self.preprocess_udf(col(name2_col)))\n",
    "        \n",
    "        # Extract features\n",
    "        df = df.withColumn(\n",
    "            \"features\", \n",
    "            F.expr(\"extract_features(processed_name1, processed_name2)\")\n",
    "        )\n",
    "        \n",
    "        # Predict match probability\n",
    "        df = df.withColumn(\n",
    "            \"match_probability\", \n",
    "            F.expr(\"predict_match(features)\")\n",
    "        )\n",
    "        \n",
    "        # Add binary prediction based on threshold (usually 0.5)\n",
    "        df = df.withColumn(\n",
    "            \"is_match_predicted\",\n",
    "            F.when(col(\"match_probability\") >= 0.5, True).otherwise(False)\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def batch_process(self, df, name1_col, name2_col, batch_size=10000):\n",
    "        \"\"\"\n",
    "        Process large datasets in batches to avoid memory issues\n",
    "        \n",
    "        Args:\n",
    "            df: Spark DataFrame with merchant name pairs\n",
    "            name1_col: Column name for first merchant name\n",
    "            name2_col: Column name for second merchant name\n",
    "            batch_size: Number of rows to process in each batch\n",
    "            \n",
    "        Returns:\n",
    "            Spark DataFrame with match predictions\n",
    "        \"\"\"\n",
    "        # Get total number of rows\n",
    "        total_rows = df.count()\n",
    "        \n",
    "        # Calculate number of batches\n",
    "        num_batches = (total_rows + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Process in batches\n",
    "        results = []\n",
    "        for i in range(num_batches):\n",
    "            batch = df.limit(batch_size).offset(i * batch_size)\n",
    "            batch_results = self.process_merchant_pairs(batch, name1_col, name2_col)\n",
    "            results.append(batch_results)\n",
    "        \n",
    "        # Union all batches\n",
    "        if results:\n",
    "            return results[0].unionAll(*results[1:])\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1bb308-fe73-4248-a3bc-176a8e132761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monitor continuous improvement\n",
    "class MerchantMatcherMonitor:\n",
    "    def __init__(self, db_connection=None):\n",
    "        \"\"\"\n",
    "        Initialize the monitoring system\n",
    "        \n",
    "        Args:\n",
    "            db_connection: Connection to database for logging\n",
    "        \"\"\"\n",
    "        self.db_connection = db_connection\n",
    "        self.metrics_history = []\n",
    "    \n",
    "    def log_prediction(self, name1, name2, predicted_probability, actual_match=None):\n",
    "        \"\"\"\n",
    "        Log a single prediction for monitoring\n",
    "        \n",
    "        Args:\n",
    "            name1: First merchant name\n",
    "            name2: Second merchant name\n",
    "            predicted_probability: Predicted match probability\n",
    "            actual_match: Actual match status if known (for feedback)\n",
    "        \"\"\"\n",
    "        timestamp = pd.Timestamp.now()\n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'name1': name1,\n",
    "            'name2': name2,\n",
    "            'predicted_probability': predicted_probability,\n",
    "            'predicted_match': predicted_probability >= 0.5,\n",
    "            'actual_match': actual_match\n",
    "        }\n",
    "        \n",
    "        # Store in memory\n",
    "        self.metrics_history.append(log_entry)\n",
    "        \n",
    "        # If database connection exists, store there too\n",
    "        if self.db_connection:\n",
    "            # Implementation depends on database type\n",
    "            pass\n",
    "    \n",
    "    def log_batch_predictions(self, df, name1_col, name2_col, prob_col, actual_col=None):\n",
    "        \"\"\"\n",
    "        Log batch predictions for monitoring\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with predictions\n",
    "            name1_col: Column name for first merchant name\n",
    "            name2_col: Column name for second merchant name\n",
    "            prob_col: Column name for predicted probability\n",
    "            actual_col: Column name for actual match status if available\n",
    "        \"\"\"\n",
    "        timestamp = pd.Timestamp.now()\n",
    "        \n",
    "        # Extract relevant columns\n",
    "        log_df = df[[name1_col, name2_col, prob_col]].copy()\n",
    "        log_df['timestamp'] = timestamp\n",
    "        log_df['predicted_match'] = log_df[prob_col] >= 0.5\n",
    "        \n",
    "        if actual_col and actual_col in df.columns:\n",
    "            log_df['actual_match'] = df[actual_col]\n",
    "        \n",
    "        # Append to history\n",
    "        self.metrics_history.extend(log_df.to_dict('records'))\n",
    "        \n",
    "        # If database connection exists, store there too\n",
    "        if self.db_connection:\n",
    "            # Implementation depends on database type\n",
    "            pass\n",
    "    \n",
    "    def calculate_metrics(self, period='day'):\n",
    "        \"\"\"\n",
    "        Calculate performance metrics for a given time period\n",
    "        \n",
    "        Args:\n",
    "            period: Time period for aggregation ('hour', 'day', 'week', 'month')\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with aggregated metrics\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame\n",
    "        metrics_df = pd.DataFrame(self.metrics_history)\n",
    "        \n",
    "        # Skip if no data or no actual labels\n",
    "        if metrics_df.empty or 'actual_match' not in metrics_df.columns:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Filter rows with actual labels\n",
    "        metrics_df = metrics_df.dropna(subset=['actual_match'])\n",
    "        \n",
    "        # Add time period column\n",
    "        if period == 'hour':\n",
    "            metrics_df['period'] = metrics_df['timestamp'].dt.strftime('%Y-%m-%d %H:00')\n",
    "        elif period == 'day':\n",
    "            metrics_df['period'] = metrics_df['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "        elif period == 'week':\n",
    "            metrics_df['period'] = metrics_df['timestamp'].dt.strftime('%Y-%W')\n",
    "        elif period == 'month':\n",
    "            metrics_df['period'] = metrics_df['timestamp'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # Group by period and calculate metrics\n",
    "        agg_metrics = metrics_df.groupby('period').apply(lambda x: pd.Series({\n",
    "            'count': len(x),\n",
    "            'precision': precision_score(x['actual_match'], x['predicted_match']),\n",
    "            'recall': recall_score(x['actual_match'], x['predicted_match']),\n",
    "            'f1': f1_score(x['actual_match'], x['predicted_match']),\n",
    "            'match_rate': x['predicted_match'].mean()\n",
    "        }))\n",
    "        \n",
    "        return agg_metrics\n",
    "    \n",
    "    def identify_problematic_pairs(self, min_confidence=0.9, max_confidence=0.1):\n",
    "        \"\"\"\n",
    "        Identify merchant name pairs that might need manual review\n",
    "        \n",
    "        Args:\n",
    "            min_confidence: Minimum confidence threshold for false negatives\n",
    "            max_confidence: Maximum confidence threshold for false positives\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with problematic pairs\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame\n",
    "        metrics_df = pd.DataFrame(self.metrics_history)\n",
    "        \n",
    "        # Skip if no data or no actual labels\n",
    "        if metrics_df.empty or 'actual_match' not in metrics_df.columns:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Filter rows with actual labels\n",
    "        metrics_df = metrics_df.dropna(subset=['actual_match'])\n",
    "        \n",
    "        # Identify false positives with high confidence\n",
    "        false_positives = metrics_df[\n",
    "            (metrics_df['predicted_probability'] >= min_confidence) & \n",
    "            (~metrics_df['actual_match'])\n",
    "        ]\n",
    "        \n",
    "        # Identify false negatives with low confidence\n",
    "        false_negatives = metrics_df[\n",
    "            (metrics_df['predicted_probability'] <= max_confidence) & \n",
    "            (metrics_df['actual_match'])\n",
    "        ]\n",
    "        \n",
    "        # Combine and sort by confidence\n",
    "        problematic = pd.concat([false_positives, false_negatives])\n",
    "        problematic = problematic.sort_values('predicted_probability', ascending=False)\n",
    "        \n",
    "        return problematic\n",
    "    \n",
    "    def trigger_retraining_alert(self, f1_threshold=0.9, window_size=1000):\n",
    "        \"\"\"\n",
    "        Check if model performance has degraded and trigger retraining alert\n",
    "        \n",
    "        Args:\n",
    "            f1_threshold: Minimum acceptable F1 score\n",
    "            window_size: Number of recent predictions to consider\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if retraining is recommended\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame\n",
    "        metrics_df = pd.DataFrame(self.metrics_history)\n",
    "        \n",
    "        # Skip if no data or no actual labels\n",
    "        if metrics_df.empty or 'actual_match' not in metrics_df.columns:\n",
    "            return False\n",
    "        \n",
    "        # Filter rows with actual labels\n",
    "        metrics_df = metrics_df.dropna(subset=['actual_match'])\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if len(metrics_df) < window_size:\n",
    "            return False\n",
    "        \n",
    "        # Get recent predictions\n",
    "        recent = metrics_df.sort_values('timestamp', ascending=False).head(window_size)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        current_f1 = f1_score(recent['actual_match'], recent['predicted_match'])\n",
    "        \n",
    "        # Trigger alert if F1 is below threshold\n",
    "        if current_f1 < f1_threshold:\n",
    "            logger.warning(f\"Model performance has degraded. Current F1: {current_f1:.4f}, Threshold: {f1_threshold:.4f}\")\n",
    "            logger.warning(\"Retraining is recommended.\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "def monitoring_example():\n",
    "    \"\"\"Example of how to use the monitoring system\"\"\"\n",
    "    # Create monitor\n",
    "    monitor = MerchantMatcherMonitor()\n",
    "    \n",
    "    # Log some predictions\n",
    "    monitor.log_prediction(\"Bank of America\", \"BOFA\", 0.95, True)\n",
    "    monitor.log_prediction(\"Bank of America\", \"Wells Fargo\", 0.05, False)\n",
    "    monitor.log_prediction(\"McDonald's\", \"McD\", 0.92, True)\n",
    "    monitor.log_prediction(\"Walmart\", \"Target\", 0.03, False)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = monitor.calculate_metrics()\n",
    "    logger.info(f\"Performance metrics:\\n{metrics}\")\n",
    "    \n",
    "    # Identify problematic pairs\n",
    "    problematic = monitor.identify_problematic_pairs()\n",
    "    logger.info(f\"Problematic pairs:\\n{problematic}\")\n",
    "    \n",
    "    # Check if retraining is needed\n",
    "    retraining_needed = monitor.trigger_retraining_alert()\n",
    "    logger.info(f\"Retraining needed: {retraining_needed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e3a76ce-00c3-42b3-bb7d-cd15f3005331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:58:27,600 - INFO - Creating sample dataset...\n",
      "2025-03-20 16:58:27,654 - INFO - Initializing and training matcher...\n",
      "2025-03-20 16:58:27,656 - INFO - Preparing training data...\n",
      "2025-03-20 16:58:27,657 - INFO - Extracting features from 5000 merchant name pairs...\n",
      "2025-03-20 16:58:30,090 - INFO - Processed 1000 rows...\n",
      "2025-03-20 16:58:32,259 - INFO - Processed 2000 rows...\n",
      "2025-03-20 16:58:34,389 - INFO - Processed 3000 rows...\n",
      "2025-03-20 16:58:36,646 - INFO - Processed 4000 rows...\n",
      "2025-03-20 16:58:39,048 - INFO - Training model on 4000 samples...\n",
      "2025-03-20 16:58:39,113 - INFO - Evaluating model on 1000 samples...\n",
      "2025-03-20 16:58:39,123 - INFO - Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "2025-03-20 16:58:39,135 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       484\n",
      "           1       1.00      1.00      1.00       516\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "2025-03-20 16:58:39,136 - INFO - Performing 5-fold cross-validation...\n",
      "2025-03-20 16:58:39,445 - INFO - Cross-validated F1 scores: [1. 1. 1. 1. 1.]\n",
      "2025-03-20 16:58:39,446 - INFO - Mean CV F1 score: 1.0000\n",
      "2025-03-20 16:58:39,447 - INFO - Feature importance - jaro_winkler: 0.8096\n",
      "2025-03-20 16:58:39,448 - INFO - Feature importance - damerau_levenshtein: 0.0014\n",
      "2025-03-20 16:58:39,449 - INFO - Feature importance - tfidf_cosine: 0.0000\n",
      "2025-03-20 16:58:39,450 - INFO - Feature importance - jaccard_bigram: 0.0536\n",
      "2025-03-20 16:58:39,450 - INFO - Feature importance - soundex_match: 0.1332\n",
      "2025-03-20 16:58:39,451 - INFO - Feature importance - token_sort_ratio: 0.0022\n",
      "2025-03-20 16:58:39,452 - INFO - Feature importance - contains_ratio: 0.0000\n",
      "2025-03-20 16:58:39,456 - INFO - Model saved to merchant_matcher.json\n",
      "2025-03-20 16:58:39,457 - INFO - Initializing monitoring system...\n",
      "2025-03-20 16:58:39,457 - INFO - Processing test data...\n",
      "2025-03-20 16:58:39,477 - INFO - Making predictions...\n",
      "2025-03-20 16:58:43,040 - INFO - Logging predictions to monitor...\n",
      "2025-03-20 16:58:43,058 - INFO - Calculating performance metrics...\n",
      "2025-03-20 16:58:43,093 - INFO - Performance metrics:\n",
      "             count  precision  recall   f1  match_rate\n",
      "period                                                \n",
      "2025-03-20  1000.0        1.0     1.0  1.0       0.476\n",
      "2025-03-20 16:58:43,095 - INFO - Identifying problematic pairs...\n",
      "2025-03-20 16:58:43,110 - INFO - Found 0 problematic pairs for review\n",
      "2025-03-20 16:58:43,111 - INFO - Checking if model retraining is needed...\n",
      "2025-03-20 16:58:43,123 - INFO - Retraining needed: False\n",
      "2025-03-20 16:58:43,124 - INFO - Large-scale processing with PySpark would be initiated here...\n",
      "2025-03-20 16:58:43,125 - INFO - (This part is conceptual and would require an actual Spark cluster)\n"
     ]
    }
   ],
   "source": [
    "# complete usage\n",
    "def full_example():\n",
    "    \"\"\"Complete example of merchant name matching workflow\"\"\"\n",
    "    # 1. Load your labeled dataset\n",
    "    # In a real scenario, you would load your actual data\n",
    "    logger.info(\"Creating sample dataset...\")\n",
    "    df = create_sample_dataset(size=5000, match_ratio=0.5)\n",
    "    \n",
    "    # 2. Initialize and train the merchant name matcher\n",
    "    logger.info(\"Initializing and training matcher...\")\n",
    "    matcher = MerchantNameMatcher()\n",
    "    metrics = matcher.train(df, 'name1', 'name2', 'is_match', test_size=0.2)\n",
    "    \n",
    "    # 3. Save the trained model\n",
    "    matcher.save_model(\"merchant_matcher.json\")\n",
    "    \n",
    "    # 4. Initialize a monitoring system\n",
    "    logger.info(\"Initializing monitoring system...\")\n",
    "    monitor = MerchantMatcherMonitor()\n",
    "    \n",
    "    # 5. Process some test data\n",
    "    logger.info(\"Processing test data...\")\n",
    "    test_df = create_sample_dataset(size=1000, match_ratio=0.5)\n",
    "    \n",
    "    # 6. Make predictions\n",
    "    logger.info(\"Making predictions...\")\n",
    "    predictions = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        prob = matcher.predict(row['name1'], row['name2'])\n",
    "        predictions.append({\n",
    "            'name1': row['name1'],\n",
    "            'name2': row['name2'],\n",
    "            'predicted_probability': prob,\n",
    "            'actual_match': row['is_match']\n",
    "        })\n",
    "    \n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # 7. Log predictions to monitor\n",
    "    logger.info(\"Logging predictions to monitor...\")\n",
    "    monitor.log_batch_predictions(\n",
    "        pred_df, 'name1', 'name2', 'predicted_probability', 'actual_match'\n",
    "    )\n",
    "    \n",
    "    # 8. Calculate performance metrics\n",
    "    logger.info(\"Calculating performance metrics...\")\n",
    "    metrics = monitor.calculate_metrics()\n",
    "    logger.info(f\"Performance metrics:\\n{metrics}\")\n",
    "    \n",
    "    # 9. Identify problematic pairs for review\n",
    "    logger.info(\"Identifying problematic pairs...\")\n",
    "    problematic = monitor.identify_problematic_pairs()\n",
    "    logger.info(f\"Found {len(problematic)} problematic pairs for review\")\n",
    "    \n",
    "    # 10. Check if model retraining is needed\n",
    "    logger.info(\"Checking if model retraining is needed...\")\n",
    "    retraining_needed = monitor.trigger_retraining_alert(f1_threshold=0.9)\n",
    "    logger.info(f\"Retraining needed: {retraining_needed}\")\n",
    "    \n",
    "    # 11. For large-scale processing with PySpark (conceptual)\n",
    "    logger.info(\"Large-scale processing with PySpark would be initiated here...\")\n",
    "    logger.info(\"(This part is conceptual and would require an actual Spark cluster)\")\n",
    "    \n",
    "    # In a real scenario, you would:\n",
    "    # 1. Initialize a SparkSession\n",
    "    # 2. Load data into Spark DataFrame\n",
    "    # 3. Use SparkMerchantNameMatcher to process the data\n",
    "    # 4. Save results and update monitoring\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    full_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a093cfd-9bd6-4691-a91b-753499a6b137",
   "metadata": {},
   "source": [
    "Summary\n",
    "This implementation:\n",
    "\n",
    "Preprocesses merchant names with a comprehensive approach for abbreviations, business terms, and standardization\n",
    "Extracts multiple similarity features leveraging Jaro-Winkler, Damerau-Levenshtein, TF-IDF, Soundex, and others\n",
    "Trains an ensemble model using XGBoost to learn the optimal combination of features\n",
    "Provides a PySpark implementation for large-scale processing\n",
    "Implements monitoring and feedback loops to track performance and enable continuous improvement\n",
    "\n",
    "The approach is designed to achieve:\n",
    "\n",
    "Higher accuracy than individual algorithms like Jaro-Winkler or TF-IDF alone\n",
    "Better handling of abbreviations and word order variations\n",
    "Scalability for large datasets through PySpark integration\n",
    "Continuous improvement through monitoring and feedback\n",
    "\n",
    "By integrating this solution, you should achieve F1 scores well above 89%, handling the various merchant name variations effectively while maintaining good performance on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b6d62-34ee-41ae-ab6b-bc99b8276521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
