# Fixing the Feature Dimensionality Mismatch in MerchantMatchingSystem

The error message indicates a dimensionality mismatch between your TF-IDF vectors:

```
X has 15 features, but NearestNeighbors is expecting 16 features as input.
```

This is occurring because you're creating different TF-IDF vectorizers at different stages of your code, resulting in inconsistent feature dimensions. The vectorizer used to build your category indices has a different vocabulary (and thus different dimensions) than the one you're using to encode query strings.

## The Root Cause

When working with TF-IDF vectors, the number of features depends on the vocabulary built during the fitting process. If you fit the vectorizer on one set of data and then use a different vectorizer for your queries, the feature dimensions likely won't match.

Let's fix this by ensuring consistent vectorization throughout the system.

## Solution: Store and Reuse Vectorizers

Here's how to update your code to fix the dimensionality mismatch:

1. Store the TF-IDF vectorizers for each category
2. Reuse them when generating query embeddings
3. Ensure consistent feature dimensions

### Updated Code for Category Index Building

```python
def _build_category_indices(self):
    """
    Build category-specific indices for fast matching using scikit-learn's NearestNeighbors.
    """
    print("Building category-specific indices...")
    
    # Group reference data by category
    by_category = defaultdict(list)
    for item in self.reference_data:
        category = item['merchant_category']
        by_category[category].append(item)
    
    # Store vectorizers for later use
    self.category_vectorizers = {}
    
    # Create embeddings or TF-IDF vectors and build indices for each category
    for category, items in by_category.items():
        # Determine which name field to use for embeddings
        if 'processed_full_form' in items[0] and all(item.get('processed_full_form') for item in items):
            names = [item['processed_full_form'] for item in items]
        else:
            names = [item['processed_name'] for item in items]
        
        try:
            # If we have a sentence transformer model, use it
            if self.embedding_model is not None:
                print(f"Generating embeddings for category: {category} using SentenceTransformer")
                embeddings = self.embedding_model.encode(names, show_progress_bar=False)
                embeddings = normalize(embeddings)
                vectorizer = None
            else:
                # Use TF-IDF as fallback
                print(f"Generating TF-IDF vectors for category: {category}")
                # Create a new vectorizer for this category
                vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))
                # Fit and transform the data
                embeddings = vectorizer.fit_transform(names).toarray()
                embeddings = normalize(embeddings)
                # Store the vectorizer for this category
                self.category_vectorizers[category] = vectorizer
            
            # Create scikit-learn NearestNeighbors index
            index = NearestNeighbors(
                n_neighbors=min(20, len(embeddings)), 
                metric='cosine', 
                algorithm='brute'  # Use brute force for accuracy
            )
            index.fit(embeddings)
            
            # Store index and corresponding items
            self.category_indices[category] = {
                'index': index,
                'items': items,
                'embeddings': embeddings
            }
            
        except Exception as e:
            print(f"Error building index for category '{category}': {e}")
    
    print(f"Built indices for {len(self.category_indices)} categories")
```

### Updated Code for Finding Matches in a Category

```python
def _find_matches_in_category(self, processed_name: str, query_category: str, 
                             target_category: str, top_k: int, 
                             run_all_algorithms: bool = False) -> List[Dict]:
    """Find matches for a name within a specific category."""
    index_data = self.category_indices[target_category]
    index = index_data['index']
    items = index_data['items']
    embeddings = index_data['embeddings']
    
    # Generate embedding for query name
    if self.embedding_model is not None:
        try:
            query_embedding = self.embedding_model.encode(processed_name, show_progress_bar=False)
            query_embedding = normalize([query_embedding])[0].reshape(1, -1)
        except Exception as e:
            print(f"Error generating embedding: {e}")
            # Fall back to TF-IDF if SentenceTransformer fails
            if target_category in self.category_vectorizers:
                vectorizer = self.category_vectorizers[target_category]
                query_embedding = vectorizer.transform([processed_name]).toarray()
            else:
                print(f"No vectorizer available for category {target_category}. Using generic TF-IDF.")
                # If we don't have a vectorizer for this category, create a new one
                # but make sure we fit it with consistent vocabulary
                all_texts = [processed_name] + [item['processed_name'] for item in items]
                vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))
                vectorizer.fit(all_texts)
                query_embedding = vectorizer.transform([processed_name]).toarray()
            query_embedding = normalize(query_embedding)
    else:
        # Use the stored TF-IDF vectorizer for this category
        if target_category in self.category_vectorizers:
            vectorizer = self.category_vectorizers[target_category]
            query_embedding = vectorizer.transform([processed_name]).toarray()
        else:
            print(f"No vectorizer available for category {target_category}. Using generic TF-IDF.")
            # If we don't have a vectorizer for this category, create a new one
            # but make sure we fit it with consistent vocabulary
            all_texts = [processed_name] + [item['processed_name'] for item in items]
            vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))
            vectorizer.fit(all_texts)
            query_embedding = vectorizer.transform([processed_name]).toarray()
        query_embedding = normalize(query_embedding)
    
    # Find nearest neighbors
    try:
        distances, indices = index.kneighbors(query_embedding, n_neighbors=min(top_k * 2, len(embeddings)))
        
        # Convert distances to similarities (scikit-learn returns cosine distances)
        similarities = [1 - dist for dist in distances[0]]
        
        # Create result objects
        results = []
        for i, (idx, similarity) in enumerate(zip(indices[0], similarities)):
            item = items[idx]
            
            # Determine the name field to use
            if 'processed_full_form' in item and item['processed_full_form']:
                compare_name = item['processed_full_form']
                display_name = item.get('full_form', item['merchant_name'])
            else:
                compare_name = item['processed_name']
                display_name = item['merchant_name']
            
            # Run comparison with all algorithms if requested
            if run_all_algorithms:
                algorithm_results = self.compare_all_algorithms(
                    processed_name, 
                    compare_name,
                    query_category,
                    target_category
                )
                
                # Use hybrid score if available, otherwise use SBERT or TF-IDF
                if 'hybrid_category_aware' in algorithm_results:
                    final_similarity = algorithm_results['hybrid_category_aware']['score']
                else:
                    final_similarity = algorithm_results['sbert_cosine']['score']
            else:
                # For normal operation, use our hybrid similarity
                final_similarity = self._hybrid_category_aware_similarity(
                    processed_name, 
                    compare_name,
                    query_category,
                    target_category
                )
                
                # Create a minimal algorithm_results
                algorithm_results = {
                    'hybrid_category_aware': {'score': final_similarity}
                }
            
            if final_similarity >= self.similarity_threshold or len(results) < 5:
                results.append({
                    'merchant_name': display_name,
                    'merchant_category': item['merchant_category'],
                    'similarity': final_similarity,
                    'merchant_id': item.get('merchant_id', None),
                    'algorithm_results': algorithm_results
                })
        
        # Sort by similarity
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results[:top_k]
    
    except ValueError as e:
        # Handle dimensionality mismatch or other issues
        print(f"Error finding matches: {e}")
        print(f"Query embedding shape: {query_embedding.shape}, Expected shape based on index: {embeddings.shape[1]}")
        
        # As a last resort, try using direct string similarity without embeddings
        results = []
        for item in items:
            if 'processed_full_form' in item and item['processed_full_form']:
                compare_name = item['processed_full_form']
                display_name = item.get('full_form', item['merchant_name'])
            else:
                compare_name = item['processed_name']
                display_name = item['merchant_name']
            
            # Use direct string similarity as fallback
            similarity = self._jaro_winkler_similarity(processed_name, compare_name)
            
            results.append({
                'merchant_name': display_name,
                'merchant_category': item['merchant_category'],
                'similarity': similarity,
                'merchant_id': item.get('merchant_id', None),
                'algorithm_results': {'jaro_winkler': {'score': similarity}}
            })
        
        # Sort by similarity
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results[:top_k]
```

### Updated Class Initialization

Also update your `__init__` method to initialize the `category_vectorizers` dictionary:

```python
def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2'):
    """
    Initialize the merchant matching system with multiple algorithms.
    
    Args:
        embedding_model_name: The name of the pre-trained SBERT model to use
    """
    self.model_name = embedding_model_name
    
    # Initialize TFIDF vectorizer for text similarity
    self.tfidf_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))
    
    # Dictionary to store category-specific vectorizers
    self.category_vectorizers = {}
    
    # Try to load SentenceTransformer if available
    # ... rest of your initialization code ...
```

## Explanation of the Solution

The key improvements in this solution are:

1. **Consistent Vectorizers**: We store the TF-IDF vectorizer for each category in a dictionary (`self.category_vectorizers`). This ensures we use the same vectorizer (with the same vocabulary) when processing query strings.

2. **Robust Error Handling**: We've added better error handling in the `_find_matches_in_category` method to gracefully handle dimensionality mismatches with detailed diagnostic information.

3. **Fallback Strategy**: As a last resort, if vectorization fails completely, the system falls back to direct string similarity measures (Jaro-Winkler) that don't rely on vector spaces.

These changes ensure that your embedding vectors maintain consistent dimensions throughout the matching process, solving the "X has 15 features, but NearestNeighbors is expecting 16 features" error.

## Why This Works

TF-IDF vectorizers build their vocabulary based on the documents they're fitted on. When we create separate vectorizers for index building and querying, they develop different vocabularies with different feature dimensions. By storing and reusing the same vectorizer, we ensure that both our indexed documents and our queries live in the same vector space with the same dimensions.

This solution maintains the original behavior of your system while adding robustness for handling edge cases and providing more informative error messages if problems occur.
