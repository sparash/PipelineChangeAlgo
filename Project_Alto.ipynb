{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdf18e0-f3a1-4c23-b225-86d3c952da18",
   "metadata": {},
   "source": [
    "Strategy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f624ad-9f61-4f38-af31-7314e9a03f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Levenshtein import jaro_winkler, distance as lev_distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import FastText\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk import ngrams\n",
    "from fuzzy import DoubleMetaphone\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "from wordcloud import WordCloud\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8470905-55a5-4131-b1d7-94ab12044c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Preprocessing Class\n",
    "class MerchantPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.abbreviations = {\n",
    "            'bofa': 'bank of america', 'b of a': 'bank of america',\n",
    "            'boa': 'bank of america', 'j&j': 'johnson & johnson',\n",
    "            'jj': 'johnson johnson', 'jnj': 'johnson and johnson',\n",
    "            'ibm': 'international business machines', 'amex': 'american express',\n",
    "            'wf': 'wells fargo', 'wm': 'walmart', 'sbux': 'starbucks',\n",
    "            'hd': 'home depot', 'cvs': 'cvs pharmacy', 'mcd': 'mcdonalds',\n",
    "            '7-11': '7-eleven', '711': '7-eleven', 'rd': 'road', \n",
    "            'st': 'street', 'ave': 'avenue', 'blvd': 'boulevard',\n",
    "            'ctr': 'center', 'ln': 'lane', 'dr': 'drive'\n",
    "        }\n",
    "        \n",
    "        self.stopwords = {'inc', 'llc', 'co', 'ltd', 'corp', 'plc', 'na', 'the'}\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "        tokens = text.split()\n",
    "        tokens = [self.abbreviations.get(token, token) for token in tokens]\n",
    "        tokens = [self.correct_typo(token) for token in tokens]\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def correct_typo(self, token, max_distance=2):\n",
    "        if token in self.abbreviations.values():\n",
    "            return token\n",
    "        closest = (token, 0)\n",
    "        for correct in self.abbreviations.values():\n",
    "            dist = lev_distance(token, correct)\n",
    "            if dist < max_distance and dist > closest[1]:\n",
    "                closest = (correct, dist)\n",
    "        return closest[0]\n",
    "\n",
    "# Test preprocessing\n",
    "preprocessor = MerchantPreprocessor()\n",
    "test_samples = [\n",
    "    (\"McDonald's\", \"mcdonalds\"),\n",
    "    (\"Starbucks Coffee Co.\", \"SBUX\"),\n",
    "    (\"123 Main St. NW\", \"123 main street northwest\")\n",
    "]\n",
    "\n",
    "print(\"üîÑ Testing preprocessing:\")\n",
    "for original, expected in test_samples:\n",
    "    cleaned = preprocessor.preprocess(original)\n",
    "    print(f\"Original: {original}\\nCleaned: {cleaned}\\nMatch: {cleaned == expected}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda1eed-36ca-402a-9a85-6b674afde762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Load and Preprocess Data\n",
    "# Load sample data (replace with your actual data)\n",
    "data = {\n",
    "    'original': [\"McDonald's\", \"Walmart Supercenter\", \"Bank of America Corp\"],\n",
    "    'variant': [\"McDonalds\", \"Wal-Mart\", \"BofA\"],\n",
    "    'expected_binary': [1, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess data\n",
    "print(\"üõ† Preprocessing data...\")\n",
    "df['clean_orig'] = df['original'].apply(preprocessor.preprocess)\n",
    "df['clean_var'] = df['variant'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Show preprocessing results\n",
    "print(\"\\nüîç Preprocessing Results:\")\n",
    "display(df[['original', 'clean_orig', 'variant', 'clean_var']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f9bcc-2341-4fab-972e-a785aaf7774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Initialize Feature Engineering\n",
    "print(\"‚öôÔ∏è Initializing feature engines...\")\n",
    "class SimilarityFeatures:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        print(\"Loading FastText model...\")\n",
    "        self.fasttext = FastText.load_fasttext_format('cc.en.300.bin')\n",
    "        print(\"Loading BERT model...\")\n",
    "        self.bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.dmeta = DoubleMetaphone()\n",
    "        \n",
    "    # ... (keep all feature methods from previous answer)\n",
    "\n",
    "feature_engine = SimilarityFeatures()\n",
    "print(\"‚úÖ Feature engines initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d967d-dfc7-413a-82bf-63dc252ed0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Generate Similarity Features\n",
    "print(\"üßÆ Calculating similarity features...\")\n",
    "features = []\n",
    "for _, row in df.iterrows():\n",
    "    features.append(feature_engine.get_features(row['clean_orig'], row['clean_var']))\n",
    "\n",
    "feature_cols = ['jaro_winkler', 'levenshtein', 'tfidf_cos', \n",
    "               'fasttext', 'bert', 'jaccard', 'phonetic']\n",
    "X = pd.DataFrame(features, columns=feature_cols)\n",
    "y = df['expected_binary']\n",
    "\n",
    "print(\"\\nüìä Generated Features Sample:\")\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a239c-9c8e-40bf-9933-33ab595e2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Train-Test Split & Model Training\n",
    "print(\"ü§ñ Training XGBoost model...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac6e5b-bf47-4548-a7e7-a9298779a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Model Evaluation\n",
    "print(\"üìà Model Evaluation:\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìù Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bfc396-8f6d-49fb-905a-ca7036cacb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Feature Importance\n",
    "print(\"üîç Feature Importance:\")\n",
    "xgb.plot_importance(model)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nüìä Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4aeb6d-5219-4730-acc1-d0c934e12070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: Save Model & Preprocessor\n",
    "print(\"üíæ Saving artifacts...\")\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "joblib.dump(feature_engine, 'feature_engine.pkl')\n",
    "model.save_model('merchant_matcher.xgb')\n",
    "\n",
    "print(\"‚úÖ Artifacts saved:\")\n",
    "!ls -lh *.pkl *.xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8f269-59d9-4ce5-a806-47482b4c55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Example Prediction\n",
    "print(\"üîÆ Sample Prediction:\")\n",
    "sample_data = {\n",
    "    'original': ['Home Depot Inc'],\n",
    "    'variant': ['The Home Depot LLC']\n",
    "}\n",
    "\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "sample_df['clean_orig'] = sample_df['original'].apply(preprocessor.preprocess)\n",
    "sample_df['clean_var'] = sample_df['variant'].apply(preprocessor.preprocess)\n",
    "\n",
    "features = []\n",
    "for _, row in sample_df.iterrows():\n",
    "    features.append(feature_engine.get_features(row['clean_orig'], row['clean_var']))\n",
    "\n",
    "X_sample = pd.DataFrame(features, columns=feature_cols)\n",
    "prediction = model.predict_proba(X_sample)[0][1]\n",
    "\n",
    "print(f\"\\nOriginal: {sample_data['original'][0]}\")\n",
    "print(f\"Variant: {sample_data['variant'][0]}\")\n",
    "print(f\"Match Probability: {prediction:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8ae1b-55f5-4506-877d-41651b0970b1",
   "metadata": {},
   "source": [
    "To use this notebook:\r\n",
    "\r\n",
    "Create new cells between these sections as needed\r\n",
    "\r\n",
    "Add !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin for FastText\r\n",
    "\r\n",
    "Install requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d098c33-0f61-4ef6-96d9-4ab7ab8ff544",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim sentence-transformers xgboost Levenshtein fuzzy wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f7f1c-400b-4193-9aed-624128c0ecbe",
   "metadata": {},
   "source": [
    "Run cells sequentially\n",
    "\n",
    "Intermediate outputs will show:\n",
    "\n",
    "Preprocessing results\n",
    "\n",
    "Feature samples\n",
    "\n",
    "Evaluation metrics\n",
    "\n",
    "Visualizations\n",
    "\n",
    "Model artifacts\n",
    "\n",
    "Example predictions\n",
    "\n",
    "Each section can be modified independently:\n",
    "\n",
    "Adjust preprocessing rules in Cell 2\n",
    "\n",
    "Modify/add features in Cell 4\n",
    "\n",
    "Tune model parameters in Cell 6\n",
    "\n",
    "Add new visualizations in Cell 7-8\n",
    "\n",
    "Create new prediction examples in Cell 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75112016-428d-426e-a86e-3df8d1398489",
   "metadata": {},
   "source": [
    "Strategy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7a6c4-14cb-4c92-8880-588ce2895998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "# String similarity metrics\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from Levenshtein import jaro_winkler\n",
    "\n",
    "# Text processing and feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import ngrams\n",
    "import nltk\n",
    "\n",
    "# ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, f1_score, confusion_matrix\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce760a-e066-424b-9f79-190e52b9b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 \n",
    "# Since we don't have the actual data, let's create a synthetic dataset\n",
    "# with merchant name variations that represents real-world challenges\n",
    "\n",
    "def create_sample_data(n_samples=100):\n",
    "    # Original merchant names\n",
    "    merchant_base = [\n",
    "        \"Bank of America\", \"McDonald's\", \"Walmart Supercenter\", \"Home Depot\", \n",
    "        \"Starbucks Coffee\", \"7-Eleven\", \"CVS Pharmacy\", \"Target\", \"Amazon.com\",\n",
    "        \"Costco Wholesale\", \"Apple Store\", \"Walgreens\", \"Best Buy\", \"Shell\",\n",
    "        \"Chevron\", \"AT&T\", \"Verizon Wireless\", \"T-Mobile\", \"Wells Fargo Bank\",\n",
    "        \"Chase Bank\"\n",
    "    ]\n",
    "    \n",
    "    # Create variations with common challenges\n",
    "    merchant_variations = []\n",
    "    expected_matches = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Pick a random merchant\n",
    "        if i < len(merchant_base):\n",
    "            base = merchant_base[i % len(merchant_base)]\n",
    "        else:\n",
    "            base = merchant_base[np.random.randint(0, len(merchant_base))]\n",
    "            \n",
    "        # Randomly create a variation or non-match\n",
    "        is_match = np.random.random() > 0.3  # 70% are matches\n",
    "        \n",
    "        if is_match:\n",
    "            # Create a variation\n",
    "            variation_type = np.random.choice([\n",
    "                'abbreviation', 'typo', 'word_order', 'extra_words', 'missing_words'\n",
    "            ])\n",
    "            \n",
    "            if variation_type == 'abbreviation':\n",
    "                if 'Bank of America' in base:\n",
    "                    variation = 'BOFA'\n",
    "                elif \"McDonald's\" in base:\n",
    "                    variation = \"McD\"\n",
    "                elif \"Starbucks\" in base:\n",
    "                    variation = \"SBUX\"\n",
    "                elif \"Walmart\" in base:\n",
    "                    variation = \"WMT\"\n",
    "                else:\n",
    "                    # Generic abbreviation: first letters of words\n",
    "                    variation = ''.join([word[0] for word in base.split()])\n",
    "            \n",
    "            elif variation_type == 'typo':\n",
    "                # Introduce a random typo\n",
    "                chars = list(base)\n",
    "                pos = np.random.randint(0, len(chars))\n",
    "                if np.random.random() > 0.5:\n",
    "                    # Replace a character\n",
    "                    chars[pos] = np.random.choice(list(string.ascii_letters + ' '))\n",
    "                else:\n",
    "                    # Delete a character\n",
    "                    chars.pop(pos)\n",
    "                variation = ''.join(chars)\n",
    "            \n",
    "            elif variation_type == 'word_order':\n",
    "                # Change word order\n",
    "                words = base.split()\n",
    "                if len(words) > 1:\n",
    "                    np.random.shuffle(words)\n",
    "                    variation = ' '.join(words)\n",
    "                else:\n",
    "                    # If single word, add a typo\n",
    "                    chars = list(base)\n",
    "                    pos = np.random.randint(0, len(chars))\n",
    "                    chars[pos] = np.random.choice(list(string.ascii_letters))\n",
    "                    variation = ''.join(chars)\n",
    "            \n",
    "            elif variation_type == 'extra_words':\n",
    "                # Add extra words\n",
    "                extras = ['Inc', 'LLC', 'Corporation', 'Co', 'Ltd', 'Store', 'Shop', 'Center']\n",
    "                variation = base + ' ' + np.random.choice(extras)\n",
    "            \n",
    "            elif variation_type == 'missing_words':\n",
    "                # Remove words\n",
    "                words = base.split()\n",
    "                if len(words) > 1:\n",
    "                    words.pop(np.random.randint(0, len(words)))\n",
    "                    variation = ' '.join(words)\n",
    "                else:\n",
    "                    # If single word, truncate\n",
    "                    variation = base[:int(len(base)*0.7)]\n",
    "        else:\n",
    "            # Create a non-match by picking a different merchant\n",
    "            other_merchants = [m for m in merchant_base if m != base]\n",
    "            variation = np.random.choice(other_merchants)\n",
    "            \n",
    "        merchant_variations.append((base, variation))\n",
    "        expected_matches.append(int(is_match))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'original': [pair[0] for pair in merchant_variations],\n",
    "        'variant': [pair[1] for pair in merchant_variations],\n",
    "        'expected_binary': expected_matches\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate sample data\n",
    "merchant_data = create_sample_data(200)\n",
    "\n",
    "# Display some examples\n",
    "print(f\"Sample dataset created with {len(merchant_data)} records\")\n",
    "print(\"\\nSome matching examples:\")\n",
    "print(merchant_data[merchant_data['expected_binary'] == 1].head(5))\n",
    "print(\"\\nSome non-matching examples:\")\n",
    "print(merchant_data[merchant_data['expected_binary'] == 0].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924cdf2-a9e3-4ed4-a06a-3d8307f256d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "class MerchantPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Comprehensive abbreviation dictionary\n",
    "        self.abbreviations = {\n",
    "            'bofa': 'bank of america', \n",
    "            'b of a': 'bank of america',\n",
    "            'boa': 'bank of america', \n",
    "            'j&j': 'johnson and johnson',\n",
    "            'jj': 'johnson johnson', \n",
    "            'jnj': 'johnson and johnson',\n",
    "            'ibm': 'international business machines', \n",
    "            'amex': 'american express',\n",
    "            'wf': 'wells fargo', \n",
    "            'wmt': 'walmart', \n",
    "            'sbux': 'starbucks',\n",
    "            'hd': 'home depot', \n",
    "            'cvs': 'cvs pharmacy', \n",
    "            'mcd': 'mcdonalds',\n",
    "            '7-11': '7-eleven', \n",
    "            '711': '7-eleven', \n",
    "            'rd': 'road',\n",
    "            'st': 'street', \n",
    "            'ave': 'avenue', \n",
    "            'blvd': 'boulevard',\n",
    "            'ctr': 'center', \n",
    "            'ln': 'lane', \n",
    "            'dr': 'drive'\n",
    "        }\n",
    "        \n",
    "        # Business entity terms to remove\n",
    "        self.stopwords = {\n",
    "            'inc', 'llc', 'co', 'ltd', 'corp', 'corporation', \n",
    "            'plc', 'na', 'the', 'and', 'of', '&'\n",
    "        }\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Normalize merchant name text\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Abbreviation replacement\n",
    "        cleaned_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.abbreviations:\n",
    "                cleaned_tokens.append(self.abbreviations[token])\n",
    "            else:\n",
    "                cleaned_tokens.append(token)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        cleaned_tokens = [token for token in cleaned_tokens if token not in self.stopwords]\n",
    "        \n",
    "        # Join tokens back into a string\n",
    "        return ' '.join(cleaned_tokens)\n",
    "    \n",
    "    def correct_typo(self, token, max_distance=2):\n",
    "        \"\"\"Attempt to correct typos using Levenshtein distance\"\"\"\n",
    "        if not token or token in self.abbreviations.values():\n",
    "            return token\n",
    "        \n",
    "        best_match = None\n",
    "        min_dist = max_distance + 1\n",
    "        \n",
    "        for correct in self.abbreviations.values():\n",
    "            # Only consider words of similar length to avoid false corrections\n",
    "            if abs(len(token) - len(correct)) <= 2:\n",
    "                dist = levenshtein_distance(token, correct)\n",
    "                if dist < min_dist and dist <= max_distance:\n",
    "                    min_dist = dist\n",
    "                    best_match = correct\n",
    "        \n",
    "        return best_match if best_match else token\n",
    "\n",
    "# Process the sample data\n",
    "preprocessor = MerchantPreprocessor()\n",
    "merchant_data['clean_original'] = merchant_data['original'].apply(preprocessor.preprocess)\n",
    "merchant_data['clean_variant'] = merchant_data['variant'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Display sample of preprocessing results\n",
    "results_df = merchant_data[['original', 'clean_original', 'variant', 'clean_variant', 'expected_binary']]\n",
    "print(\"Example preprocessing results:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Visualize the effect of preprocessing on name length\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([merchant_data['original'].str.len(), merchant_data['clean_original'].str.len()], \n",
    "         bins=20, alpha=0.5, label=['Original', 'Preprocessed'])\n",
    "plt.title('Effect of Preprocessing on Name Length')\n",
    "plt.xlabel('Character Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a51b4e-fbdc-477a-aa6e-0d7fd2433bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "class SimilarityFeatures:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))\n",
    "    \n",
    "    def jaro_winkler_sim(self, s1, s2):\n",
    "        \"\"\"Calculate Jaro-Winkler similarity between two strings\"\"\"\n",
    "        if not s1 or not s2:\n",
    "            return 0.0\n",
    "        return jaro_winkler(s1, s2)\n",
    "    \n",
    "    def levenshtein_sim(self, s1, s2):\n",
    "        \"\"\"Calculate normalized Levenshtein similarity between two strings\"\"\"\n",
    "        if not s1 or not s2:\n",
    "            return 0.0\n",
    "        max_len = max(len(s1), len(s2))\n",
    "        if max_len == 0:\n",
    "            return 1.0  # Both strings are empty\n",
    "        return 1.0 - (levenshtein_distance(s1, s2) / max_len)\n",
    "    \n",
    "    def tfidf_cosine(self, s1, s2):\n",
    "        \"\"\"Calculate TF-IDF cosine similarity between two strings\"\"\"\n",
    "        if not s1 or not s2:\n",
    "            return 0.0\n",
    "        try:\n",
    "            # Fit and transform with both strings\n",
    "            tfidf_matrix = self.tfidf.fit_transform([s1, s2])\n",
    "            return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def jaccard_ngram(self, s1, s2, n=2):\n",
    "        \"\"\"Calculate Jaccard similarity of character n-grams\"\"\"\n",
    "        if not s1 or not s2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Generate character n-grams\n",
    "        s1_ngrams = set(s1[i:i+n] for i in range(len(s1)-n+1))\n",
    "        s2_ngrams = set(s2[i:i+n] for i in range(len(s2)-n+1))\n",
    "        \n",
    "        if not s1_ngrams or not s2_ngrams:\n",
    "            return 0.0\n",
    "            \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(s1_ngrams.intersection(s2_ngrams))\n",
    "        union = len(s1_ngrams.union(s2_ngrams))\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def get_basic_features(self, s1, s2):\n",
    "        \"\"\"Extract all basic string similarity features\"\"\"\n",
    "        return {\n",
    "            'jaro_winkler': self.jaro_winkler_sim(s1, s2),\n",
    "            'levenshtein': self.levenshtein_sim(s1, s2),\n",
    "            'tfidf_cosine': self.tfidf_cosine(s1, s2),\n",
    "            'jaccard_ngram': self.jaccard_ngram(s1, s2)\n",
    "        }\n",
    "\n",
    "# Extract basic features\n",
    "feature_extractor = SimilarityFeatures()\n",
    "\n",
    "# Calculate features for each merchant pair\n",
    "basic_features = []\n",
    "for _, row in merchant_data.iterrows():\n",
    "    features = feature_extractor.get_basic_features(\n",
    "        row['clean_original'], row['clean_variant'])\n",
    "    basic_features.append(features)\n",
    "\n",
    "# Convert to DataFrame and join with original data\n",
    "basic_features_df = pd.DataFrame(basic_features)\n",
    "merchant_data = pd.concat([merchant_data, basic_features_df], axis=1)\n",
    "\n",
    "# Display the features\n",
    "print(\"Sample of extracted basic features:\")\n",
    "print(merchant_data[['clean_original', 'clean_variant', 'jaro_winkler', 'levenshtein', \n",
    "                    'tfidf_cosine', 'jaccard_ngram', 'expected_binary']].head(10))\n",
    "\n",
    "# Visualize the distribution of features by match status\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, feature in enumerate(['jaro_winkler', 'levenshtein', 'tfidf_cosine', 'jaccard_ngram']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    for label, color in [(1, 'green'), (0, 'red')]:\n",
    "        subset = merchant_data[merchant_data['expected_binary'] == label]\n",
    "        plt.hist(subset[feature], bins=20, alpha=0.5, color=color, \n",
    "                 label=f'{\"Match\" if label==1 else \"Non-match\"}')\n",
    "    plt.title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bf6e5-d60c-4dda-9922-d75898aed756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "# Look at feature correlation\n",
    "feature_columns = ['jaro_winkler', 'levenshtein', 'tfidf_cosine', 'jaccard_ngram']\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = merchant_data[feature_columns].corr()\n",
    "\n",
    "# Visualize correlation\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Visualize correlation of features with expected outcome\n",
    "correlations = []\n",
    "for feature in feature_columns:\n",
    "    corr = merchant_data[feature].corr(merchant_data['expected_binary'])\n",
    "    correlations.append((feature, corr))\n",
    "\n",
    "# Sort correlations\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Plot correlation with expected_binary\n",
    "plt.figure(figsize=(10, 6))\n",
    "features = [x[0] for x in correlations]\n",
    "corr_values = [x[1] for x in correlations]\n",
    "plt.barh(features, corr_values, color='skyblue')\n",
    "plt.xlabel('Correlation with Expected Match')\n",
    "plt.title('Feature Correlation with Expected Match')\n",
    "plt.xlim(-1, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature correlations with expected match:\")\n",
    "for feature, corr in correlations:\n",
    "    print(f\"{feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca22655-1629-4a09-b118-8d395528667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "# Create feature matrix and target vector\n",
    "X = merchant_data[feature_columns]\n",
    "y = merchant_data['expected_binary']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Analyze basic thresholds for each feature\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    # Calculate precision, recall, and thresholds\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, X_test[feature])\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    f1_scores = 2 * precision * recall / (precision + recall + 1e-7)  # Avoid division by zero\n",
    "    \n",
    "    # Find best threshold based on F1 score\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "    \n",
    "    # Plot curves\n",
    "    plt.plot(thresholds, precision[:-1], 'b-', label='Precision')\n",
    "    plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "    plt.plot(thresholds, f1_scores[:-1], 'r-', label='F1 Score')\n",
    "    \n",
    "    # Mark best threshold\n",
    "    plt.axvline(x=best_threshold, color='k', linestyle='--', \n",
    "                label=f'Best threshold: {best_threshold:.3f} (F1: {best_f1:.3f})')\n",
    "    \n",
    "    plt.title(f'{feature.replace(\"_\", \" \").title()} Threshold Analysis')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best thresholds for each feature\n",
    "print(\"\\nBest thresholds based on F1 score:\")\n",
    "for feature in feature_columns:\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, X_test[feature])\n",
    "    f1_scores = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "    print(f\"{feature}: threshold = {best_threshold:.3f}, F1 = {best_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24a452-21c5-4397-a7b1-a8ac165c2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "# Create the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(xgb_model)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d85082-8c5b-47a6-aa49-47685352c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "# Find the optimal probability threshold\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "f1_scores = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, 'b-', label='Precision-Recall curve')\n",
    "plt.plot(recall[best_idx], precision[best_idx], 'ro', \n",
    "         label=f'Best threshold: {best_threshold:.3f} (F1: {best_f1:.3f})')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.axhline(y=np.mean(y_test), color='r', linestyle='--', \n",
    "            label=f'Baseline precision: {np.mean(y_test):.3f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Update predictions using best threshold\n",
    "y_pred_optimized = (y_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Print updated classification report\n",
    "print(f\"Classification Report with Optimized Threshold ({best_threshold:.3f}):\")\n",
    "print(classification_report(y_test, y_pred_optimized))\n",
    "\n",
    "# Compare with individual features\n",
    "print(\"\\nComparison of F1 scores:\")\n",
    "print(f\"XGBoost (default threshold): {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"XGBoost (optimized threshold): {f1_score(y_test, y_pred_optimized):.3f}\")\n",
    "\n",
    "for feature in feature_columns:\n",
    "    # Find best threshold for this feature\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, X_test[feature])\n",
    "    f1_scores = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0\n",
    "    \n",
    "    # Make predictions using best threshold\n",
    "    feature_preds = (X_test[feature] >= best_threshold).astype(int)\n",
    "    feature_f1 = f1_score(y_test, feature_preds)\n",
    "    \n",
    "    print(f\"{feature}: {feature_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843b948-a214-47ef-a6f9-16d0fef2ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "# Create DataFrame with test results\n",
    "results_df = X_test.copy()\n",
    "results_df['original'] = merchant_data.loc[X_test.index, 'original'].values\n",
    "results_df['variant'] = merchant_data.loc[X_test.index, 'variant'].values\n",
    "results_df['clean_original'] = merchant_data.loc[X_test.index, 'clean_original'].values\n",
    "results_df['clean_variant'] = merchant_data.loc[X_test.index, 'clean_variant'].values\n",
    "results_df['expected'] = y_test.values\n",
    "results_df['predicted'] = y_pred_optimized\n",
    "results_df['probability'] = y_pred_proba\n",
    "results_df['is_error'] = results_df['expected'] != results_df['predicted']\n",
    "\n",
    "# Look at errors\n",
    "errors_df = results_df[results_df['is_error']]\n",
    "print(f\"Total errors: {len(errors_df)} out of {len(results_df)} test samples ({len(errors_df)/len(results_df)*100:.1f}%)\")\n",
    "\n",
    "# Display some false positives (predicted match, but not a match)\n",
    "print(\"\\nFalse Positives (predicted match, but not a match):\")\n",
    "false_positives = errors_df[errors_df['predicted'] == 1]\n",
    "if len(false_positives) > 0:\n",
    "    print(false_positives[['original', 'variant', 'probability']].head(5))\n",
    "else:\n",
    "    print(\"No false positives\")\n",
    "\n",
    "# Display some false negatives (predicted non-match, but is a match)\n",
    "print(\"\\nFalse Negatives (predicted non-match, but is a match):\")\n",
    "false_negatives = errors_df[errors_df['predicted'] == 0]\n",
    "if len(false_negatives) > 0:\n",
    "    print(false_negatives[['original', 'variant', 'probability']].head(5))\n",
    "else:\n",
    "    print(\"No false negatives\")\n",
    "\n",
    "# Analyze error patterns\n",
    "if len(errors_df) > 0:\n",
    "    # Calculate string lengths\n",
    "    errors_df['orig_len'] = errors_df['original'].str.len()\n",
    "    errors_df['var_len'] = errors_df['variant'].str.len()\n",
    "    errors_df['len_diff'] = abs(errors_df['orig_len'] - errors_df['var_len'])\n",
    "    \n",
    "    # Analyze length difference correlation with errors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(errors_df['len_diff'], errors_df['probability'], c='red', alpha=0.6)\n",
    "    plt.title('Error Analysis: String Length Difference vs. Probability')\n",
    "    plt.xlabel('Absolute Difference in String Length')\n",
    "    plt.ylabel('Predicted Probability')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare error distributions by feature\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(feature_columns):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.scatter(errors_df[feature], errors_df['probability'], c='red', alpha=0.6)\n",
    "        plt.title(f'Errors: {feature.replace(\"_\", \" \").title()} vs. Probability')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Predicted Probability')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555ed41-3a0c-4fd4-bc4c-021f244c44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "# This cell is conceptual/pseudo-code for PySpark implementation\n",
    "# You would need to uncomment and adapt this for actual use with PySpark\n",
    "\n",
    "'''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Merchant Matching\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register UDFs for feature extraction\n",
    "@udf(returnType=StringType())\n",
    "def preprocess_text_udf(text):\n",
    "    # Implement the preprocessing logic here\n",
    "    # This is a simplified version\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "@udf(returnType=FloatType())\n",
    "def jaro_winkler_udf(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    return jaro_winkler(s1, s2)\n",
    "\n",
    "@udf(returnType=FloatType())\n",
    "def levenshtein_sim_udf(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    if max_len == 0:\n",
    "        return 1.0\n",
    "    return 1.0 - (levenshtein_distance(s1, s2) / max_len)\n",
    "\n",
    "# Load data\n",
    "df = spark.read.parquet(\"path/to/merchant_data.parquet\")\n",
    "\n",
    "# Apply preprocessing\n",
    "df = df.withColumn(\"clean_original\", preprocess_text_udf(col(\"original\")))\n",
    "df = df.withColumn(\"clean_variant\", preprocess_text_udf(col(\"variant\")))\n",
    "\n",
    "# Extract features\n",
    "df = df.withColumn(\"jaro_winkler\", jaro_winkler_udf(col(\"clean_original\"), col(\"clean_variant\")))\n",
    "df = df.withColumn(\"levenshtein\", levenshtein_sim_udf(col(\"clean_original\"), col(\"clean_variant\")))\n",
    "\n",
    "# More features would be added here...\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"jaro_winkler\", \"levenshtein\"],  # Add more features as needed\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"expected_binary\",\n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "# Split data\n",
    "training_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Train model\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"probability\",\n",
    "    labelCol=\"expected_binary\"\n",
    ")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"path/to/merchant_matching_model\")\n",
    "'''\n",
    "\n",
    "print(\"This is a conceptual implementation for PySpark.\")\n",
    "print(\"To actually run this code, you would need to:\")\n",
    "print(\"1. Set up a Spark cluster or use a local Spark instance\")\n",
    "print(\"2. Have your merchant data in a compatible format (e.g., Parquet)\")\n",
    "print(\"3. Uncomment and possibly modify the code above\")\n",
    "print(\"4. Add more features as needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc75c4-4ce5-4619-a56a-2d823ae38dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "# Save the model and preprocessor for future use\n",
    "import pickle\n",
    "\n",
    "# Create a directory to save model artifacts\n",
    "import os\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "xgb_model.save_model('model/merchant_matcher.xgb')\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('model/preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "# Save the feature extractor\n",
    "with open('model/feature_extractor.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_extractor, f)\n",
    "\n",
    "# Save the optimal threshold\n",
    "with open('model/optimal_threshold.txt', 'w') as f:\n",
    "    f.write(str(best_threshold))\n",
    "\n",
    "print(\"Model and pipeline components saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aafb48-dd7e-4fb0-89c4-d3d5a2d027ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "# Example of how to load and use the model for new data\n",
    "def load_model_components():\n",
    "    # Load XGBoost model\n",
    "    loaded_model = xgb.XGBClassifier()\n",
    "    loaded_model.load_model('model/merchant_matcher.xgb')\n",
    "    \n",
    "    # Load preprocessor\n",
    "    with open('model/preprocessor.pkl', 'rb') as f:\n",
    "        loaded_preprocessor = pickle.load(f)\n",
    "    \n",
    "    # Load feature extractor\n",
    "    with open('model/feature_extractor.pkl', 'rb') as f:\n",
    "        loaded_feature_extractor = pickle.load(f)\n",
    "    \n",
    "    # Load optimal threshold\n",
    "    with open('model/optimal_threshold.txt', 'r') as f:\n",
    "        loaded_threshold = float(f.read().strip())\n",
    "    \n",
    "    return loaded_model, loaded_preprocessor, loaded_feature_extractor, loaded_threshold\n",
    "\n",
    "# Load components\n",
    "model, preprocessor, feature_extractor, threshold = load_model_components()\n",
    "\n",
    "# Create sample data to test\n",
    "test_pairs = [\n",
    "    (\"Bank of America\", \"BofA\"),\n",
    "    (\"McDonald's\", \"Mcdonald's Restaurant\"),\n",
    "    (\"Walmart\", \"Target\"),  # Non-match\n",
    "    (\"7-Eleven\", \"7-11\"),\n",
    "    (\"Home Depot\", \"The Home Depot Inc\")\n",
    "]\n",
    "\n",
    "# Preprocess and extract features\n",
    "results = []\n",
    "for original, variant in test_pairs:\n",
    "    # Preprocess\n",
    "    clean_original = preprocessor.preprocess(original)\n",
    "    clean_variant = preprocessor.preprocess(variant)\n",
    "    \n",
    "    # Extract features\n",
    "    features = feature_extractor.get_basic_features(clean_original, clean_variant)\n",
    "    \n",
    "    # Convert to format expected by model\n",
    "    feature_array = np.array([features[f] for f in feature_columns]).reshape(1, -1)\n",
    "    \n",
    "    # Predict\n",
    "    probability = model.predict_proba(feature_array)[0, 1]\n",
    "    prediction = 1 if probability >= threshold else 0\n",
    "    \n",
    "    results.append({\n",
    "        \"original\": original,\n",
    "        \"variant\": variant,\n",
    "        \"clean_original\": clean_original,\n",
    "        \"clean_variant\": clean_variant,\n",
    "        \"probability\": probability,\n",
    "        \"prediction\": prediction,\n",
    "        \"is_match\": \"Yes\" if prediction == 1 else \"No\"\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Prediction results for new merchant pairs:\")\n",
    "print(results_df[[\"original\", \"variant\", \"probability\", \"is_match\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5162b6a-5cb4-4d92-8f09-a863e53e02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "# Performance summary of our approach\n",
    "summary = {\n",
    "    \"Model\": \"XGBoost Ensemble\",\n",
    "    \"Features\": \", \".join(feature_columns),\n",
    "    \"Best Threshold\": best_threshold,\n",
    "    \"F1 Score\": f1_score(y_test, y_pred_optimized),\n",
    "    \"Top Feature\": feature_columns[np.argmax(xgb_model.feature_importances_)]\n",
    "}\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nNext Steps for Further Improvement:\")\n",
    "print(\"1. Incorporate advanced semantic features using BERT embeddings\")\n",
    "print(\"2. Add phonetic matching with Soundex or Double Metaphone\")\n",
    "print(\"3. Expand the abbreviation dictionary and preprocessing rules\")\n",
    "print(\"4. Scale with PySpark for larger datasets\")\n",
    "print(\"5. Implement active learning to improve from user feedback\")\n",
    "print(\"6. Deploy as a service with FastAPI or Flask\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
