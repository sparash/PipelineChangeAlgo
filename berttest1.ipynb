{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531f94a3-251b-4fcc-9994-aa963e57f3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library available for BERT embeddings\n",
      "Warning: pyahocorasick not available. Using fallback implementation.\n",
      "Using device: cpu\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Setup\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from Levenshtein import jaro_winkler, ratio as levenshtein_ratio\n",
    "import textdistance\n",
    "from fuzzywuzzy import fuzz\n",
    "import jellyfish\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Import transformers for BERT embeddings\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    transformers_available = True\n",
    "    print(\"Transformers library available for BERT embeddings\")\n",
    "except ImportError:\n",
    "    transformers_available = False\n",
    "    print(\"Warning: transformers library not available. Will use TF-IDF fallback.\")\n",
    "\n",
    "# Try to import pyahocorasick with fallback\n",
    "try:\n",
    "    import pyahocorasick\n",
    "    aho_corasick_available = True\n",
    "    print(\"pyahocorasick is available\")\n",
    "except ImportError:\n",
    "    print(\"Warning: pyahocorasick not available. Using fallback implementation.\")\n",
    "    aho_corasick_available = False\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b551e6b-ce47-4787-bfac-1b1260cc2d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained BERT model 'sentence-transformers/all-MiniLM-L6-v2'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2940d71d7ad044508d39902687fc347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea369277305e4eae8fe41ff2943ea822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2548a257acfa422490e493c4e69201c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0ae55725fd4ba6af2f972344e827f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3922957c751845c3b5c4d8a211d9dc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f2c5553f594544bea4869d9a520dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained BERT model loaded successfully on cpu\n",
      "BERT embedder initialized with pre-trained model!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: BERT Embeddings Class\n",
    "\n",
    "class BERTEmbedder:\n",
    "    \"\"\"\n",
    "    Class for generating BERT embeddings for text data using pre-trained models.\n",
    "    Implements pooling strategies and batching for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2', pooling_strategy='mean', device=None):\n",
    "        \"\"\"\n",
    "        Initialize BERT embedder with specified pre-trained model and pooling strategy.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained BERT model to use\n",
    "            pooling_strategy (str): Pooling strategy ('mean', 'cls', or 'max')\n",
    "            device: Device to run the model on (cuda or cpu)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.max_sequence_length = 512  # BERT's limit\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.initialized = False\n",
    "        \n",
    "        # Initialize pre-trained model if transformers available\n",
    "        if transformers_available:\n",
    "            try:\n",
    "                print(f\"Loading pre-trained BERT model '{model_name}'...\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "                self.model.eval()  # Set to evaluation mode\n",
    "                self.initialized = True\n",
    "                print(f\"Pre-trained BERT model loaded successfully on {self.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing BERT model: {e}\")\n",
    "                self.initialized = False\n",
    "        \n",
    "        # Initialize TF-IDF fallback if BERT not available\n",
    "        if not self.initialized:\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4))\n",
    "            self.tfidf_fitted = False\n",
    "            print(\"Using TF-IDF fallback for embeddings\")\n",
    "    \n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling - take average of all token embeddings\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]  # First element contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def _cls_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        CLS pooling - use the [CLS] token embedding\n",
    "        \"\"\"\n",
    "        return model_output[0][:, 0]\n",
    "    \n",
    "    def _max_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Max pooling - take max of all token embeddings\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "        return torch.max(token_embeddings, 1)[0]\n",
    "    \n",
    "    def _get_pooled_embeddings(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Apply the selected pooling strategy\n",
    "        \"\"\"\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            return self._mean_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'cls':\n",
    "            return self._cls_pooling(model_output, attention_mask)\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            return self._max_pooling(model_output, attention_mask)\n",
    "        else:\n",
    "            # Default to mean pooling\n",
    "            return self._mean_pooling(model_output, attention_mask)\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        Fit the TF-IDF vectorizer on a corpus of texts (only needed for TF-IDF fallback)\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            # Fit TF-IDF vectorizer\n",
    "            self.tfidf_vectorizer.fit(texts)\n",
    "            self.tfidf_fitted = True\n",
    "            print(\"TF-IDF vectorizer fitted on corpus\")\n",
    "    \n",
    "    def encode(self, texts, batch_size=32, show_progress=False):\n",
    "        \"\"\"\n",
    "        Encode texts into embeddings using the pre-trained model\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts or single text\n",
    "            batch_size: Batch size for processing\n",
    "            show_progress: Whether to show progress\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Embeddings for the texts\n",
    "        \"\"\"\n",
    "        # Handle single text input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Return empty array for empty input\n",
    "        if len(texts) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Use pre-trained BERT if available\n",
    "        if self.initialized:\n",
    "            # Process in batches\n",
    "            all_embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                if show_progress and i % (batch_size * 10) == 0:\n",
    "                    print(f\"Processing batch {i//batch_size + 1}/{(len(texts)//batch_size) + 1}\")\n",
    "                \n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize\n",
    "                encoded_input = self.tokenizer(\n",
    "                    batch_texts, \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=self.max_sequence_length,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Compute token embeddings\n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(**encoded_input)\n",
    "                    batch_embeddings = self._get_pooled_embeddings(model_output, encoded_input['attention_mask'])\n",
    "                    all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            \n",
    "            return np.vstack(all_embeddings)\n",
    "        \n",
    "        else:\n",
    "            # Use TF-IDF fallback\n",
    "            if not self.tfidf_fitted:\n",
    "                self.fit(texts)\n",
    "            \n",
    "            return self.tfidf_vectorizer.transform(texts).toarray()\n",
    "    \n",
    "    def compute_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts using the pre-trained model\n",
    "        \n",
    "        Args:\n",
    "            text1: First text\n",
    "            text2: Second text\n",
    "            \n",
    "        Returns:\n",
    "            float: Cosine similarity score\n",
    "        \"\"\"\n",
    "        # Get embeddings for both texts\n",
    "        emb1 = self.encode(text1)\n",
    "        emb2 = self.encode(text2)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        return np.sum(emb1 * emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)\n",
    "\n",
    "# Initialize BERT embedder with pre-trained model\n",
    "bert_embedder = BERTEmbedder(model_name='sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "print(\"BERT embedder initialized with pre-trained model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75101259-9acc-499c-8bf2-ea3b3c48723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced merchant matcher initialized with pre-trained models!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Enhanced Merchant Matcher with Pre-trained Models\n",
    "\n",
    "class EnhancedMerchantMatcher:\n",
    "    \"\"\"\n",
    "    Enhanced matcher using pre-trained models and rule-based scoring\n",
    "    for merchant name and acronym matching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_embedder=None):\n",
    "        \"\"\"\n",
    "        Initialize with optional pre-trained BERT embedder.\n",
    "        \n",
    "        Args:\n",
    "            bert_embedder: Pre-trained BERT embedder instance\n",
    "        \"\"\"\n",
    "        # Initialize pre-trained BERT embedder\n",
    "        self.bert_embedder = bert_embedder\n",
    "        if self.bert_embedder is None and transformers_available:\n",
    "            self.bert_embedder = BERTEmbedder()\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        # Initialize trie for approximate matching\n",
    "        self.trie = None\n",
    "        \n",
    "        # Initialize Aho-Corasick automaton only if available\n",
    "        if aho_corasick_available:\n",
    "            self.automaton = pyahocorasick.Automaton()\n",
    "        else:\n",
    "            self.automaton = None\n",
    "        \n",
    "        # Define abbreviation dictionary - comprehensive industry knowledge \n",
    "        self.abbreviations = {\n",
    "            # Banking & Financial Institutions\n",
    "            'bofa': 'bank of america', 'b of a': 'bank of america',\n",
    "            'boa': 'bank of america', 'bac': 'bank of america',\n",
    "            'jpm': 'jpmorgan chase', 'jpm chase': 'jpmorgan chase',\n",
    "            'wf': 'wells fargo', 'wfb': 'wells fargo bank',\n",
    "            'citi': 'citibank', 'citi bank': 'citibank',\n",
    "            'gs': 'goldman sachs', 'ms': 'morgan stanley',\n",
    "            'db': 'deutsche bank', 'hsbc': 'hongkong and shanghai banking corporation',\n",
    "            'amex': 'american express', 'usb': 'us bank', 'rbc': 'royal bank of canada',\n",
    "            'pnc': 'pnc financial services', 'td': 'toronto dominion bank',\n",
    "            'bny': 'bank of new york', 'bnyc': 'bank of new york mellon',\n",
    "            \n",
    "            # Fast Food & Restaurant Chains\n",
    "            'mcd': 'mcdonalds', 'mcds': 'mcdonalds', 'md': 'mcdonalds',\n",
    "            'bk': 'burger king', 'kfc': 'kentucky fried chicken',\n",
    "            'sbux': 'starbucks', 'sb': 'starbucks',\n",
    "            'tb': 'taco bell', 'wen': 'wendys',\n",
    "            'dq': 'dairy queen', 'ph': 'pizza hut',\n",
    "            'dnkn': 'dunkin donuts', 'cfa': 'chick fil a',\n",
    "            'cmg': 'chipotle mexican grill', 'ihop': 'international house of pancakes',\n",
    "            'tgi': 'tgi fridays', 'tgif': 'tgi fridays',\n",
    "            \n",
    "            # Tech Companies\n",
    "            'msft': 'microsoft', 'aapl': 'apple', 'goog': 'google',\n",
    "            'googl': 'google', 'amzn': 'amazon', 'fb': 'facebook',\n",
    "            'meta': 'meta platforms', 'nflx': 'netflix', 'tsla': 'tesla',\n",
    "            'ibm': 'international business machines', 'csco': 'cisco systems',\n",
    "            'orcl': 'oracle', 'intc': 'intel', 'amd': 'advanced micro devices',\n",
    "            'nvda': 'nvidia', 'adbe': 'adobe', 'crm': 'salesforce',\n",
    "            \n",
    "            # Automotive\n",
    "            'tm': 'toyota motor', 'toyof': 'toyota', 'toyota': 'toyota corporation',\n",
    "            'f': 'ford motor company', 'gm': 'general motors',\n",
    "            'hmc': 'honda motor company', 'hndaf': 'honda',\n",
    "            'nsany': 'nissan', 'bmwyy': 'bmw', 'vwagy': 'volkswagen',\n",
    "            \n",
    "            # Retail companies\n",
    "            'wmt': 'walmart', 'tgt': 'target', 'cost': 'costco',\n",
    "            'hd': 'home depot', 'low': 'lowes', 'bby': 'best buy',\n",
    "            'ebay': 'ebay', 'dg': 'dollar general', 'dltr': 'dollar tree',\n",
    "            \n",
    "            # Common abbreviations\n",
    "            'j&j': 'johnson & johnson', 'jj': 'johnson johnson', \n",
    "            'jnj': 'johnson and johnson', '7-11': '7-eleven', \n",
    "            '711': '7-eleven', 'intl': 'international',\n",
    "            'corp': 'corporation', 'inc': 'incorporated',\n",
    "            \n",
    "            # Address components\n",
    "            'rd': 'road', 'st': 'street', 'ave': 'avenue', \n",
    "            'blvd': 'boulevard', 'ctr': 'center', 'ln': 'lane', \n",
    "            'dr': 'drive', 'pl': 'place', 'ct': 'court',\n",
    "            'hwy': 'highway', 'pkwy': 'parkway', 'sq': 'square'\n",
    "        }\n",
    "        \n",
    "        # Domain-specific abbreviation dictionaries\n",
    "        self.domain_abbreviations = {\n",
    "            'Medical': {\n",
    "                'dr': 'doctor', 'hosp': 'hospital', 'med': 'medical',\n",
    "                'clin': 'clinic', 'pharm': 'pharmacy', 'lab': 'laboratory',\n",
    "                'dept': 'department', 'ctr': 'center', 'inst': 'institute'\n",
    "            },\n",
    "            'Government': {\n",
    "                'govt': 'government', 'dept': 'department', 'admin': 'administration',\n",
    "                'auth': 'authority', 'fed': 'federal', 'natl': 'national',\n",
    "                'comm': 'commission', 'sec': 'secretary', 'org': 'organization'\n",
    "            },\n",
    "            'Education': {\n",
    "                'univ': 'university', 'coll': 'college', 'acad': 'academy',\n",
    "                'elem': 'elementary', 'sch': 'school', 'inst': 'institute',\n",
    "                'dept': 'department', 'lib': 'library', 'lab': 'laboratory'\n",
    "            },\n",
    "            'Financial': {\n",
    "                'fin': 'financial', 'svcs': 'services', 'mgmt': 'management',\n",
    "                'assoc': 'associates', 'intl': 'international', 'grp': 'group',\n",
    "                'corp': 'corporation', 'cap': 'capital', 'inv': 'investment'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Stop words to remove during preprocessing\n",
    "        self.stopwords = {\n",
    "            'inc', 'llc', 'co', 'ltd', 'corp', 'plc', 'na', 'the', \n",
    "            'and', 'of', 'for', 'in', 'a', 'an', 'by', 'to', 'at',\n",
    "            'corporation', 'incorporated', 'company', 'limited'\n",
    "        }\n",
    "        \n",
    "        # Domain-specific stopwords\n",
    "        self.domain_stopwords = {\n",
    "            'Medical': {'center', 'healthcare', 'medical', 'health', 'care', 'services'},\n",
    "            'Government': {'department', 'office', 'agency', 'bureau', 'division'},\n",
    "            'Education': {'university', 'college', 'school', 'institute', 'academy'},\n",
    "            'Financial': {'financial', 'services', 'management', 'capital', 'investment'}\n",
    "        }\n",
    "        \n",
    "        # Pre-defined domain weights for similarity algorithms\n",
    "        self.domain_weights = self._get_domain_weights()\n",
    "    \n",
    "    def _get_domain_weights(self):\n",
    "        \"\"\"Get pre-defined domain weights for each similarity algorithm\"\"\"\n",
    "        return {\n",
    "            'default': {\n",
    "                'jaro_winkler': 0.10,\n",
    "                'damerau_levenshtein': 0.05,\n",
    "                'tfidf_cosine': 0.05,\n",
    "                'jaccard_bigram': 0.05,\n",
    "                'soundex': 0.05,\n",
    "                'token_sort_ratio': 0.10,\n",
    "                'contains_ratio': 0.10,\n",
    "                'fuzzy_levenshtein': 0.05,\n",
    "                'trie_approximate': 0.10,\n",
    "                'bert_similarity': 0.15,\n",
    "                'aho_corasick': 0.05,\n",
    "                'acronym_formation': 0.15\n",
    "            },\n",
    "            'Restaurant': {\n",
    "                'acronym_formation': 0.20,\n",
    "                'bert_similarity': 0.15,\n",
    "                'token_sort_ratio': 0.12,\n",
    "                'contains_ratio': 0.12\n",
    "            },\n",
    "            'Banking': {\n",
    "                'acronym_formation': 0.18,\n",
    "                'bert_similarity': 0.15,\n",
    "                'trie_approximate': 0.12\n",
    "            },\n",
    "            'Automotive': {\n",
    "                'acronym_formation': 0.18,\n",
    "                'contains_ratio': 0.15,\n",
    "                'bert_similarity': 0.12\n",
    "            },\n",
    "            'Medical': {\n",
    "                'soundex': 0.12,\n",
    "                'bert_similarity': 0.15,\n",
    "                'acronym_formation': 0.15\n",
    "            },\n",
    "            'Government': {\n",
    "                'trie_approximate': 0.15,\n",
    "                'acronym_formation': 0.18,\n",
    "                'token_sort_ratio': 0.12\n",
    "            },\n",
    "            'Technology': {\n",
    "                'bert_similarity': 0.15,\n",
    "                'acronym_formation': 0.15,\n",
    "                'tfidf_cosine': 0.12\n",
    "            },\n",
    "            'Education': {\n",
    "                'bert_similarity': 0.15,\n",
    "                'acronym_formation': 0.15,\n",
    "                'token_sort_ratio': 0.12\n",
    "            },\n",
    "            'Retail': {\n",
    "                'bert_similarity': 0.12,\n",
    "                'token_sort_ratio': 0.12,\n",
    "                'acronym_formation': 0.15\n",
    "            },\n",
    "            'Financial': {\n",
    "                'bert_similarity': 0.15,\n",
    "                'acronym_formation': 0.15,\n",
    "                'token_sort_ratio': 0.10\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def preprocess_with_domain(self, text, domain=None):\n",
    "        \"\"\"\n",
    "        Preprocesses text with domain-specific handling\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to preprocess\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            str: Preprocessed text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "        \n",
    "        # Replace abbreviations\n",
    "        words = text.split()\n",
    "        \n",
    "        # Apply general abbreviation expansion\n",
    "        words = [self.abbreviations.get(word, word) for word in words]\n",
    "        \n",
    "        # Apply domain-specific abbreviation expansion if domain is provided\n",
    "        if domain and domain in self.domain_abbreviations:\n",
    "            words = [self.domain_abbreviations[domain].get(word, word) for word in words]\n",
    "        \n",
    "        # Remove general stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "        \n",
    "        # Remove domain-specific stopwords if domain is provided\n",
    "        if domain and domain in self.domain_stopwords:\n",
    "            words = [word for word in words if word not in self.domain_stopwords[domain]]\n",
    "        \n",
    "        # Rejoin words and remove extra spaces\n",
    "        text = ' '.join(words)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_pair(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Preprocess acronym and full name with domain-specific handling\"\"\"\n",
    "        acronym_clean = self.preprocess_with_domain(acronym, domain)\n",
    "        full_name_clean = self.preprocess_with_domain(full_name, domain)\n",
    "        return acronym_clean, full_name_clean\n",
    "    \n",
    "    def jaro_winkler_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Calculate Jaro-Winkler similarity\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        return jaro_winkler(acronym_clean, full_name_clean)\n",
    "    \n",
    "    def damerau_levenshtein_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Calculate Damerau-Levenshtein similarity\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Damerau-Levenshtein distance\n",
    "        max_len = max(len(acronym_clean), len(full_name_clean))\n",
    "        if max_len == 0:\n",
    "            return 0\n",
    "        \n",
    "        distance = textdistance.damerau_levenshtein.distance(acronym_clean, full_name_clean)\n",
    "        similarity = 1 - (distance / max_len)\n",
    "        return max(0, similarity)  # Ensure non-negative\n",
    "    \n",
    "    def tfidf_cosine_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Calculate TF-IDF Cosine similarity\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Fit and transform with TF-IDF\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform([acronym_clean, full_name_clean])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return max(0, similarity)  # Ensure non-negative\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def jaccard_bigram_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Calculate Jaccard Bigram similarity\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Create bigrams\n",
    "        def get_bigrams(text):\n",
    "            return [text[i:i+2] for i in range(len(text)-1)]\n",
    "        \n",
    "        acronym_bigrams = set(get_bigrams(acronym_clean))\n",
    "        full_name_bigrams = set(get_bigrams(full_name_clean))\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        union_size = len(acronym_bigrams.union(full_name_bigrams))\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        \n",
    "        intersection_size = len(acronym_bigrams.intersection(full_name_bigrams))\n",
    "        return intersection_size / union_size\n",
    "    \n",
    "    def soundex_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate phonetic similarity using Soundex algorithm.\n",
    "        \"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # If either string is empty, return 0\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get the soundex codes for both strings\n",
    "        try:\n",
    "            # For multi-word strings, get soundex for each word\n",
    "            acronym_words = acronym_clean.split()\n",
    "            full_name_words = full_name_clean.split()\n",
    "            \n",
    "            # Get soundex codes for each word\n",
    "            acronym_codes = [jellyfish.soundex(word) for word in acronym_words]\n",
    "            full_name_codes = [jellyfish.soundex(word) for word in full_name_words]\n",
    "            \n",
    "            # Calculate matches between codes\n",
    "            matches = 0\n",
    "            total = max(len(acronym_codes), len(full_name_codes))\n",
    "            \n",
    "            for code in acronym_codes:\n",
    "                if code in full_name_codes:\n",
    "                    matches += 1\n",
    "                    # Remove the matched code to avoid double counting\n",
    "                    full_name_codes.remove(code)\n",
    "            \n",
    "            return matches / total if total > 0 else 0.0\n",
    "        except:\n",
    "            # Fallback if there's an error with the soundex calculation\n",
    "            return 0.0\n",
    "    \n",
    "    def token_sort_ratio_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Calculate Token Sort Ratio using fuzzywuzzy\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Token Sort Ratio\n",
    "        ratio = fuzz.token_sort_ratio(acronym_clean, full_name_clean) / 100\n",
    "        return ratio\n",
    "    \n",
    "    def contains_ratio_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Check if acronym is contained in full name\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Check if acronym is contained in full name\n",
    "        if acronym_clean in full_name_clean:\n",
    "            return 1\n",
    "        \n",
    "        # Check for partial containment\n",
    "        acronym_chars = list(acronym_clean)\n",
    "        full_name_chars = list(full_name_clean)\n",
    "        \n",
    "        matches = 0\n",
    "        for char in acronym_chars:\n",
    "            if char in full_name_chars:\n",
    "                matches += 1\n",
    "                full_name_chars.remove(char)  # Remove matched char\n",
    "        \n",
    "        if len(acronym_chars) == 0:\n",
    "            return 0\n",
    "        \n",
    "        return matches / len(acronym_chars)\n",
    "    \n",
    "    def fuzzy_levenshtein_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Calculate fuzzy Levenshtein ratio\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Levenshtein ratio (which is already normalized)\n",
    "        similarity = levenshtein_ratio(acronym_clean, full_name_clean)\n",
    "        return similarity\n",
    "    \n",
    "    def trie_approximate_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Use a trie for approximate matching\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Extract first letters from each word in full name\n",
    "        words = full_name_clean.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # Check if acronym matches first letters\n",
    "        if acronym_clean.lower() == first_letters.lower():\n",
    "            return 1\n",
    "        \n",
    "        # Calculate similarity for approximate matching\n",
    "        max_len = max(len(acronym_clean), len(first_letters))\n",
    "        if max_len == 0:\n",
    "            return 0\n",
    "        \n",
    "        distance = levenshtein_distance(acronym_clean.lower(), first_letters.lower())\n",
    "        similarity = 1 - (distance / max_len)\n",
    "        return max(0, similarity)\n",
    "    \n",
    "    def aho_corasick_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Use Aho-Corasick algorithm for pattern matching\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        if not aho_corasick_available:\n",
    "            # Fallback implementation when pyahocorasick is not available\n",
    "            matches = 0\n",
    "            for c in acronym_clean:\n",
    "                if c in full_name_clean:\n",
    "                    matches += 1\n",
    "                    # Remove matched character to prevent duplicate counting\n",
    "                    full_name_clean = full_name_clean.replace(c, '', 1)\n",
    "            \n",
    "            return min(1.0, matches / len(acronym_clean)) if len(acronym_clean) > 0 else 0\n",
    "        \n",
    "        # Build automaton\n",
    "        automaton = pyahocorasick.Automaton()\n",
    "        for i, c in enumerate(acronym_clean):\n",
    "            automaton.add_word(c, (i, c))\n",
    "        automaton.make_automaton()\n",
    "        \n",
    "        # Find matches\n",
    "        matches = 0\n",
    "        for _, (_, c) in automaton.iter(full_name_clean):\n",
    "            matches += 1\n",
    "        \n",
    "        # Calculate score\n",
    "        if len(acronym_clean) == 0:\n",
    "            return 0\n",
    "        \n",
    "        return min(1.0, matches / len(acronym_clean))\n",
    "    \n",
    "    def bert_similarity(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate semantic similarity using pre-trained BERT embeddings\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: BERT similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # If BERT embedder is not initialized, return 0\n",
    "        if self.bert_embedder is None:\n",
    "            return 0\n",
    "        \n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        \n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            # Get embeddings from pre-trained model\n",
    "            emb1 = self.bert_embedder.encode(acronym_clean)\n",
    "            emb2 = self.bert_embedder.encode(full_name_clean)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.sum(emb1 * emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)\n",
    "            return float(similarity)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in BERT similarity calculation: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def acronym_formation_score(self, acronym, full_name, domain=None):\n",
    "        \"\"\"Calculate how well the acronym is formed from the full name\"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        # Check if strings are empty\n",
    "        if not acronym_clean or not full_name_clean:\n",
    "            return 0\n",
    "        \n",
    "        # Extract first letters from each word in full name\n",
    "        words = full_name_clean.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        # Standard acronym formation - first letter of each word\n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # If exact match, return 1\n",
    "        if acronym_clean.lower() == first_letters.lower():\n",
    "            return 1\n",
    "        \n",
    "        # Check partial match\n",
    "        acronym_chars = list(acronym_clean.lower())\n",
    "        first_letters_chars = list(first_letters.lower())\n",
    "        \n",
    "        matches = 0\n",
    "        for char in acronym_chars:\n",
    "            if char in first_letters_chars:\n",
    "                matches += 1\n",
    "                first_letters_chars.remove(char)  # Remove matched char\n",
    "        \n",
    "        if len(acronym_chars) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate partial match score\n",
    "        return matches / len(acronym_chars)\n",
    "    \n",
    "    def enhanced_acronym_formation_score(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Enhanced acronym formation score with special handling for common patterns\n",
    "        particularly optimized for restaurant chains and business names with prefixes like \"Mc\".\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to evaluate\n",
    "            full_name (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: A score between 0 and 1 indicating how well the acronym matches the full name\n",
    "        \"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        \n",
    "        # Basic cleanup\n",
    "        acronym = acronym_clean.lower()\n",
    "        full_name = full_name_clean.lower()\n",
    "        \n",
    "        # Special case for \"Mc\" prefixes (common in restaurant names)\n",
    "        if full_name.startswith('mc') and len(acronym) >= 1 and acronym[0] == 'm':\n",
    "            # McDonalds -> MCD pattern\n",
    "            modified_full_name = full_name[2:]  # Remove \"mc\"\n",
    "            remaining_chars = acronym[1:]  # Remove \"m\"\n",
    "            \n",
    "            # For \"MCD\" -> \"McDonalds\" pattern\n",
    "            if remaining_chars and len(modified_full_name) > 0:\n",
    "                # Check if remaining chars match consonants in the name\n",
    "                consonants = ''.join([c for c in modified_full_name if c not in 'aeiou'])\n",
    "                if remaining_chars in consonants:\n",
    "                    return 0.95\n",
    "                \n",
    "                # Check if first few consonants match remaining chars\n",
    "                first_consonants = ''.join([c for c in modified_full_name[:len(remaining_chars)*2] \n",
    "                                          if c not in 'aeiou'])\n",
    "                if remaining_chars in first_consonants:\n",
    "                    return 0.90\n",
    "                \n",
    "                # Check first letters after \"Mc\"\n",
    "                words = modified_full_name.split()\n",
    "                if words:\n",
    "                    first_letters = ''.join([word[0] for word in words if word])\n",
    "                    if remaining_chars in first_letters:\n",
    "                        return 0.90\n",
    "                    \n",
    "                    # Check if remaining chars appear in sequence in the words\n",
    "                    current_word_position = 0\n",
    "                    chars_found = 0\n",
    "                    for char in remaining_chars:\n",
    "                        for i in range(current_word_position, len(words)):\n",
    "                            if char in words[i]:\n",
    "                                chars_found += 1\n",
    "                                current_word_position = i + 1\n",
    "                                break\n",
    "                    \n",
    "                    if chars_found == len(remaining_chars):\n",
    "                        return 0.85\n",
    "            \n",
    "            # Even if not a perfect match, it's still a good score for Mc prefix\n",
    "            return 0.80\n",
    "        \n",
    "        # Check for brand name with location prefix/suffix pattern (Toyota Corporation -> Western Toyota)\n",
    "        common_brands = ['toyota', 'ford', 'honda', 'bmw', 'walmart', 'target', 'starbucks']\n",
    "        location_prefixes = ['north', 'south', 'east', 'west', 'western', 'eastern', 'central']\n",
    "        \n",
    "        # Extract the key brand name (if present)\n",
    "        brand_match = None\n",
    "        for brand in common_brands:\n",
    "            if brand in acronym.lower():\n",
    "                brand_match = brand\n",
    "                break\n",
    "            if brand in full_name.lower():\n",
    "                brand_match = brand\n",
    "                break\n",
    "        \n",
    "        if brand_match:\n",
    "            # Check if one name has the brand with a location prefix/suffix and the other has just the brand\n",
    "            has_location_prefix = any(prefix in acronym.lower() or prefix in full_name.lower() \n",
    "                                     for prefix in location_prefixes)\n",
    "            \n",
    "            if has_location_prefix:\n",
    "                # If both contain the brand name but one has location prefix\n",
    "                if brand_match in acronym.lower() and brand_match in full_name.lower():\n",
    "                    return 0.92\n",
    "        \n",
    "        # Standard acronym formation - first letter of each word\n",
    "        words = full_name.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        # Get first letters\n",
    "        first_letters = ''.join([word[0] for word in words if word])\n",
    "        \n",
    "        # If exact match, return high score\n",
    "        if acronym == first_letters:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check for consonant-based acronym (common in business acronyms)\n",
    "        consonants = ''.join([c for c in full_name if c not in 'aeiou' and c.isalpha()])\n",
    "        consonant_match = 0.0\n",
    "        if len(acronym) <= len(consonants):\n",
    "            # Check for sequential consonant match\n",
    "            acronym_position = 0\n",
    "            for i, c in enumerate(consonants):\n",
    "                if acronym_position < len(acronym) and c == acronym[acronym_position]:\n",
    "                    acronym_position += 1\n",
    "            \n",
    "            consonant_sequential_match = acronym_position / len(acronym) if len(acronym) > 0 else 0\n",
    "            \n",
    "            # Check for any consonant match\n",
    "            matches = 0\n",
    "            consonants_copy = consonants\n",
    "            for char in acronym:\n",
    "                if char in consonants_copy:\n",
    "                    matches += 1\n",
    "                    consonants_copy = consonants_copy.replace(char, '', 1)\n",
    "            \n",
    "            consonant_any_match = matches / len(acronym) if len(acronym) > 0 else 0\n",
    "            \n",
    "            # Take the better score\n",
    "            consonant_match = max(consonant_sequential_match, consonant_any_match)\n",
    "            \n",
    "            # Give higher scores for strong consonant matches\n",
    "            if consonant_match > 0.7:\n",
    "                return max(0.85, consonant_match)\n",
    "        \n",
    "        # Check if acronym characters appear in order in full name\n",
    "        ordered_match = 0\n",
    "        last_found_index = -1\n",
    "        full_name_chars = list(full_name)\n",
    "        \n",
    "        for char in acronym:\n",
    "            found = False\n",
    "            for i in range(last_found_index + 1, len(full_name_chars)):\n",
    "                if char == full_name_chars[i]:\n",
    "                    ordered_match += 1\n",
    "                    last_found_index = i\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            # If we couldn't find the character in order, try looking anywhere\n",
    "            if not found:\n",
    "                for i in range(len(full_name_chars)):\n",
    "                    if i != last_found_index and char == full_name_chars[i]:\n",
    "                        ordered_match += 0.5  # Half credit for out-of-order match\n",
    "                        full_name_chars[i] = '_'  # Mark as used\n",
    "                        break\n",
    "        \n",
    "        ordered_match_score = ordered_match / len(acronym) if len(acronym) > 0 else 0\n",
    "        \n",
    "        # Check capitals in the full name (businesses often use capitals in their names)\n",
    "        capitals = ''.join([c for c in full_name if c.isupper()])\n",
    "        if capitals and acronym.upper() == capitals:\n",
    "            return 0.95\n",
    "        \n",
    "        # Return the best score from different matching strategies\n",
    "        return max(\n",
    "            ordered_match_score * 0.9,  # Ordered match is good but not perfect\n",
    "            consonant_match * 0.9,      # Consonant match is also valuable\n",
    "            0.4                         # Minimum score to prevent too low values\n",
    "        )\n",
    "    \n",
    "    def get_all_similarity_scores(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Calculate all similarity scores at once for efficiency\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of algorithm name to score\n",
    "        \"\"\"\n",
    "        # Return empty dictionary if either acronym or full_name is None\n",
    "        if acronym is None or full_name is None:\n",
    "            return {}\n",
    "        \n",
    "        # Calculate all similarity scores\n",
    "        scores = {\n",
    "            'jaro_winkler': self.jaro_winkler_similarity(acronym, full_name, domain),\n",
    "            'damerau_levenshtein': self.damerau_levenshtein_similarity(acronym, full_name, domain),\n",
    "            'tfidf_cosine': self.tfidf_cosine_similarity(acronym, full_name, domain),\n",
    "            'jaccard_bigram': self.jaccard_bigram_similarity(acronym, full_name, domain),\n",
    "            'soundex': self.soundex_similarity(acronym, full_name, domain),\n",
    "            'token_sort_ratio': self.token_sort_ratio_similarity(acronym, full_name, domain),\n",
    "            'contains_ratio': self.contains_ratio_similarity(acronym, full_name, domain),\n",
    "            'fuzzy_levenshtein': self.fuzzy_levenshtein_similarity(acronym, full_name, domain),\n",
    "            'trie_approximate': self.trie_approximate_similarity(acronym, full_name, domain),\n",
    "            'aho_corasick': self.aho_corasick_similarity(acronym, full_name, domain),\n",
    "            'acronym_formation': self.acronym_formation_score(acronym, full_name, domain),\n",
    "            'enhanced_acronym_formation': self.enhanced_acronym_formation_score(acronym, full_name, domain)\n",
    "        }\n",
    "        \n",
    "        # Add BERT similarity if available\n",
    "        if self.bert_embedder is not None:\n",
    "            scores['bert_similarity'] = self.bert_similarity(acronym, full_name, domain)\n",
    "        \n",
    "        # Add pattern detection scores\n",
    "        scores.update(self._detect_domain_patterns(acronym, full_name, domain))\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def _detect_domain_patterns(self, acronym, full_name, domain):\n",
    "        \"\"\"\n",
    "        Detect domain-specific patterns in the text\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            domain (str, optional): Domain for specialized preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of pattern name to score (0 or 1)\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        \n",
    "        # Detect brand with location pattern\n",
    "        patterns['pattern_brand_with_location'] = self._detect_brand_with_location(acronym, full_name, domain)\n",
    "        \n",
    "        # Detect corporation suffix pattern\n",
    "        patterns['pattern_corporation_suffix'] = self._detect_corporation_suffix(acronym, full_name, domain)\n",
    "        \n",
    "        # Detect department prefix pattern\n",
    "        patterns['pattern_department_prefix'] = self._detect_department_prefix(acronym, full_name, domain)\n",
    "        \n",
    "        # Add domain-specific patterns\n",
    "        if domain == 'Medical':\n",
    "            patterns.update(self._detect_medical_patterns(acronym, full_name))\n",
    "        elif domain == 'Financial':\n",
    "            patterns.update(self._detect_financial_patterns(acronym, full_name))\n",
    "        elif domain == 'Government':\n",
    "            patterns.update(self._detect_government_patterns(acronym, full_name))\n",
    "        elif domain == 'Education':\n",
    "            patterns.update(self._detect_education_patterns(acronym, full_name))\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _detect_brand_with_location(self, acronym, full_name, domain):\n",
    "        \"\"\"\n",
    "        Detect if this is a case of a brand name with location prefix/suffix,\n",
    "        like 'Western Toyota' -> 'Toyota Corporation'\n",
    "        \"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        words_a = acronym_clean.lower().split()\n",
    "        words_f = full_name_clean.lower().split()\n",
    "        \n",
    "        if len(words_a) <= 1 or len(words_f) <= 1:\n",
    "            return 0.0\n",
    "            \n",
    "        # Common brand identifiers that might appear with location prefixes\n",
    "        common_brands = ['toyota', 'ford', 'honda', 'bmw', 'walmart', 'target', \n",
    "                        'starbucks', 'mcdonalds', 'marriott', 'hilton']\n",
    "                        \n",
    "        # Check if any common brand appears in either name\n",
    "        has_brand_a = any(brand in words_a for brand in common_brands)\n",
    "        has_brand_f = any(brand in words_f for brand in common_brands)\n",
    "        \n",
    "        # Check if either name has a location modifier\n",
    "        location_modifiers = ['north', 'south', 'east', 'west', 'central', 'downtown',\n",
    "                             'city', 'regional', 'local', 'western', 'eastern']\n",
    "        has_location_a = any(loc in words_a for loc in location_modifiers)\n",
    "        has_location_f = any(loc in words_f for loc in location_modifiers)\n",
    "        \n",
    "        # One name has brand, other has brand + location OR one has location, other has brand\n",
    "        if (has_brand_a and has_brand_f and (has_location_a != has_location_f)):\n",
    "            return 1.0\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def _detect_corporation_suffix(self, acronym, full_name, domain):\n",
    "        \"\"\"\n",
    "        Detect if this is a case where one name has a corporate suffix and the other doesn't\n",
    "        like 'Toyota' -> 'Toyota Corporation'\n",
    "        \"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        words_a = acronym_clean.lower().split()\n",
    "        words_f = full_name_clean.lower().split()\n",
    "        \n",
    "        if not words_a or not words_f:\n",
    "            return 0.0\n",
    "        \n",
    "        corporate_suffixes = ['corporation', 'corp', 'inc', 'incorporated', 'llc', \n",
    "                             'limited', 'ltd', 'company', 'co', 'group']\n",
    "        \n",
    "        # Check if one name ends with a corporate suffix and the other doesn't\n",
    "        name1_has_suffix = any(words_a[-1] == suffix for suffix in corporate_suffixes)\n",
    "        name2_has_suffix = any(words_f[-1] == suffix for suffix in corporate_suffixes)\n",
    "        \n",
    "        return 1.0 if name1_has_suffix != name2_has_suffix else 0.0\n",
    "    \n",
    "    def _detect_department_prefix(self, acronym, full_name, domain):\n",
    "        \"\"\"\n",
    "        Detect if this is a case where one name has a department prefix\n",
    "        like 'Finance Department' -> 'Department of Finance'\n",
    "        \"\"\"\n",
    "        acronym_clean, full_name_clean = self.preprocess_pair(acronym, full_name, domain)\n",
    "        dept_terms = ['department', 'dept', 'division', 'office', 'bureau']\n",
    "        \n",
    "        # Check for department terms in either name\n",
    "        has_dept_a = any(term in acronym_clean.lower() for term in dept_terms)\n",
    "        has_dept_f = any(term in full_name_clean.lower() for term in dept_terms)\n",
    "        \n",
    "        # Both have department terms but in different positions\n",
    "        if has_dept_a and has_dept_f:\n",
    "            words_a = acronym_clean.lower().split()\n",
    "            words_f = full_name_clean.lower().split()\n",
    "            \n",
    "            dept_pos_a = next((i for i, word in enumerate(words_a) if any(term in word for term in dept_terms)), -1)\n",
    "            dept_pos_f = next((i for i, word in enumerate(words_f) if any(term in word for term in dept_terms)), -1)\n",
    "            \n",
    "            # Department terms in different positions (start vs end) suggests same entity with different naming\n",
    "            if dept_pos_a != -1 and dept_pos_f != -1:\n",
    "                # One at beginning, one at end\n",
    "                if (dept_pos_a == 0 and dept_pos_f == len(words_f) - 1) or \\\n",
    "                   (dept_pos_f == 0 and dept_pos_a == len(words_a) - 1):\n",
    "                    return 1.0\n",
    "        \n",
    "        # One has department term, other doesn't, but common words\n",
    "        elif has_dept_a != has_dept_f:\n",
    "            words_a = set(acronym_clean.lower().split())\n",
    "            words_f = set(full_name_clean.lower().split())\n",
    "            common_words = words_a.intersection(words_f)\n",
    "            \n",
    "            # If they share significant words (excluding stopwords)\n",
    "            common_words = common_words - self.stopwords\n",
    "            if len(common_words) >= 1:\n",
    "                return 1.0\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def _detect_medical_patterns(self, acronym, full_name):\n",
    "        \"\"\"\n",
    "        Detect patterns specific to medical domain\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of pattern name to score (0 or 1)\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        \n",
    "        # Preprocess\n",
    "        acronym_clean = self.preprocess_with_domain(acronym, 'Medical')\n",
    "        full_name_clean = self.preprocess_with_domain(full_name, 'Medical')\n",
    "        \n",
    "        # Check for medical center / hospital pattern\n",
    "        medical_terms = ['hospital', 'medical', 'clinic', 'health', 'healthcare', 'center']\n",
    "        has_medical_a = any(term in acronym_clean.lower() for term in medical_terms)\n",
    "        has_medical_f = any(term in full_name_clean.lower() for term in medical_terms)\n",
    "        \n",
    "        # If one has medical term and other doesn't, but they share other significant words\n",
    "        if has_medical_a != has_medical_f:\n",
    "            words_a = set(acronym_clean.lower().split())\n",
    "            words_f = set(full_name_clean.lower().split())\n",
    "            common_words = words_a.intersection(words_f) - self.stopwords - set(medical_terms)\n",
    "            \n",
    "            if len(common_words) >= 1:\n",
    "                patterns['pattern_medical_facility'] = 1.0\n",
    "            else:\n",
    "                patterns['pattern_medical_facility'] = 0.0\n",
    "        else:\n",
    "            patterns['pattern_medical_facility'] = 0.0\n",
    "        \n",
    "        # Detect department / specialty pattern\n",
    "        specialties = ['cardiology', 'neurology', 'pediatrics', 'oncology', 'radiology', \n",
    "                       'orthopedic', 'surgery', 'emergency', 'trauma', 'psychiatric']\n",
    "        \n",
    "        has_specialty_a = any(spec in acronym_clean.lower() for spec in specialties)\n",
    "        has_specialty_f = any(spec in full_name_clean.lower() for spec in specialties)\n",
    "        \n",
    "        # If both have specialties but different ones, or one has specialty and other doesn't\n",
    "        if has_specialty_a or has_specialty_f:\n",
    "            patterns['pattern_medical_specialty'] = 1.0\n",
    "        else:\n",
    "            patterns['pattern_medical_specialty'] = 0.0\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _detect_financial_patterns(self, acronym, full_name):\n",
    "        \"\"\"\n",
    "        Detect patterns specific to financial domain\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of pattern name to score (0 or 1)\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        \n",
    "        # Preprocess\n",
    "        acronym_clean = self.preprocess_with_domain(acronym, 'Financial')\n",
    "        full_name_clean = self.preprocess_with_domain(full_name, 'Financial')\n",
    "        \n",
    "        # Check for bank / financial institution pattern\n",
    "        bank_terms = ['bank', 'financial', 'credit', 'investment', 'capital', 'securities', 'asset']\n",
    "        has_bank_a = any(term in acronym_clean.lower() for term in bank_terms)\n",
    "        has_bank_f = any(term in full_name_clean.lower() for term in bank_terms)\n",
    "        \n",
    "        # Bank naming patterns often have location + bank\n",
    "        location_terms = ['america', 'national', 'city', 'state', 'first', 'trust',\n",
    "                          'international', 'north', 'south', 'east', 'west', 'central']\n",
    "        has_location_a = any(term in acronym_clean.lower() for term in location_terms)\n",
    "        has_location_f = any(term in full_name_clean.lower() for term in location_terms)\n",
    "        \n",
    "        # Bank + location pattern\n",
    "        if (has_bank_a and has_bank_f) and (has_location_a or has_location_f):\n",
    "            patterns['pattern_bank_with_location'] = 1.0\n",
    "        else:\n",
    "            patterns['pattern_bank_with_location'] = 0.0\n",
    "        \n",
    "        # Check for financial services pattern\n",
    "        service_terms = ['services', 'management', 'advisors', 'associates', 'group', 'partners']\n",
    "        has_service_a = any(term in acronym_clean.lower() for term in service_terms)\n",
    "        has_service_f = any(term in full_name_clean.lower() for term in service_terms)\n",
    "        \n",
    "        if has_service_a != has_service_f and (has_bank_a or has_bank_f):\n",
    "            patterns['pattern_financial_services'] = 1.0\n",
    "        else:\n",
    "            patterns['pattern_financial_services'] = 0.0\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _detect_government_patterns(self, acronym, full_name):\n",
    "        \"\"\"\n",
    "        Detect patterns specific to government domain\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of pattern name to score (0 or 1)\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        \n",
    "        # Preprocess\n",
    "        acronym_clean = self.preprocess_with_domain(acronym, 'Government')\n",
    "        full_name_clean = self.preprocess_with_domain(full_name, 'Government')\n",
    "        \n",
    "        # Government agency patterns\n",
    "        agency_terms = ['agency', 'administration', 'authority', 'commission', 'bureau', \n",
    "                       'board', 'service', 'office', 'council']\n",
    "        has_agency_a = any(term in acronym_clean.lower() for term in agency_terms)\n",
    "        has_agency_f = any(term in full_name_clean.lower() for term in agency_terms)\n",
    "        \n",
    "        # Classic government acronym pattern: words -> initialism\n",
    "        words_f = full_name_clean.lower().split()\n",
    "        if len(words_f) >= 3 and len(acronym_clean) >= 2:\n",
    "            # Check if acronym consists of first letters\n",
    "            first_letters = ''.join(word[0] for word in words_f if word)\n",
    "            if acronym_clean.lower() in first_letters.lower():\n",
    "                patterns['pattern_government_initialism'] = 1.0\n",
    "            else:\n",
    "                patterns['pattern_government_initialism'] = 0.0\n",
    "        else:\n",
    "            patterns['pattern_government_initialism'] = 0.0\n",
    "        \n",
    "        # Department of X vs X Department pattern\n",
    "        if has_agency_a or has_agency_f:\n",
    "            patterns['pattern_government_agency'] = 1.0\n",
    "        else:\n",
    "            patterns['pattern_government_agency'] = 0.0\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _detect_education_patterns(self, acronym, full_name):\n",
    "        \"\"\"\n",
    "        Detect patterns specific to education domain\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of pattern name to score (0 or 1)\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        \n",
    "        # Preprocess\n",
    "        acronym_clean = self.preprocess_with_domain(acronym, 'Education')\n",
    "        full_name_clean = self.preprocess_with_domain(full_name, 'Education')\n",
    "        \n",
    "        # Education institution patterns\n",
    "        edu_terms = ['university', 'college', 'school', 'institute', 'academy', \n",
    "                    'education', 'learning', 'studies']\n",
    "        has_edu_a = any(term in acronym_clean.lower() for term in edu_terms)\n",
    "        has_edu_f = any(term in full_name_clean.lower() for term in edu_terms)\n",
    "        \n",
    "        # Common pattern: University of X vs X University\n",
    "        if has_edu_a and has_edu_f:\n",
    "            words_a = acronym_clean.lower().split()\n",
    "            words_f = full_name_clean.lower().split()\n",
    "            \n",
    "            # Check for \"X University\" vs \"University of X\" pattern\n",
    "            uni_pos_a = next((i for i, word in enumerate(words_a) if word in edu_terms), -1)\n",
    "            uni_pos_f = next((i for i, word in enumerate(words_f) if word in edu_terms), -1)\n",
    "            \n",
    "            if uni_pos_a != -1 and uni_pos_f != -1:\n",
    "                # One at beginning, one at end or different positions\n",
    "                if uni_pos_a != uni_pos_f:\n",
    "                    patterns['pattern_edu_institution_name_variation'] = 1.0\n",
    "                else:\n",
    "                    patterns['pattern_edu_institution_name_variation'] = 0.0\n",
    "            else:\n",
    "                patterns['pattern_edu_institution_name_variation'] = 0.0\n",
    "        else:\n",
    "            patterns['pattern_edu_institution_name_variation'] = 0.0\n",
    "        \n",
    "        # Department/School within university pattern\n",
    "        dept_terms = ['department', 'dept', 'school', 'faculty', 'college']\n",
    "        has_dept_a = any(term in acronym_clean.lower() for term in dept_terms)\n",
    "        has_dept_f = any(term in full_name_clean.lower() for term in dept_terms)\n",
    "        \n",
    "        if has_dept_a != has_dept_f and (has_edu_a or has_edu_f):\n",
    "            patterns['pattern_edu_department'] = 1.0\n",
    "        else:\n",
    "            patterns['pattern_edu_department'] = 0.0\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def compute_weighted_score(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Compute weighted similarity score using pre-defined weights\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            domain (str, optional): Domain for weighting and preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Weighted similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # Get all similarity scores\n",
    "        scores = self.get_all_similarity_scores(acronym, full_name, domain)\n",
    "        \n",
    "        # Get domain-specific weights\n",
    "        weights = self._get_domain_specific_weights(domain)\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        weighted_score = 0.0\n",
    "        weights_used = 0.0\n",
    "        \n",
    "        for algo, score in scores.items():\n",
    "            if algo in weights:\n",
    "                weighted_score += weights[algo] * score\n",
    "                weights_used += weights[algo]\n",
    "        \n",
    "        # Handle case where some algorithms are missing\n",
    "        if weights_used > 0:\n",
    "            # Normalize by weights actually used\n",
    "            weighted_score /= weights_used\n",
    "        \n",
    "        # Apply special pattern boosts\n",
    "        pattern_boost = 1.0\n",
    "        for algo, score in scores.items():\n",
    "            if algo.startswith('pattern_') and score > 0:\n",
    "                # Different boost factors for different patterns\n",
    "                if 'brand_with_location' in algo:\n",
    "                    pattern_boost += 0.25  # 25% boost for brand with location\n",
    "                elif 'corporation_suffix' in algo:\n",
    "                    pattern_boost += 0.20  # 20% boost for corporation suffix\n",
    "                elif 'department_prefix' in algo:\n",
    "                    pattern_boost += 0.20  # 20% boost for department prefix\n",
    "                elif 'medical' in algo:\n",
    "                    pattern_boost += 0.25  # 25% boost for medical patterns\n",
    "                elif 'bank' in algo:\n",
    "                    pattern_boost += 0.25  # 25% boost for bank patterns\n",
    "                elif 'government' in algo:\n",
    "                    pattern_boost += 0.20  # 20% boost for government patterns\n",
    "                elif 'edu' in algo:\n",
    "                    pattern_boost += 0.20  # 20% boost for education patterns\n",
    "                else:\n",
    "                    pattern_boost += 0.15  # 15% boost for other patterns\n",
    "        \n",
    "        # Apply the pattern boost, cap at 1.5 (50% boost max)\n",
    "        weighted_score = min(1.0, weighted_score * min(pattern_boost, 1.5))\n",
    "        \n",
    "        # Boost scores that are already reasonably good\n",
    "        if weighted_score > 0.75:\n",
    "            weighted_score = min(1.0, weighted_score * 1.2)  # 20% boost for high scores\n",
    "        elif weighted_score > 0.6:\n",
    "            weighted_score = min(1.0, weighted_score * 1.15)  # 15% boost for good scores\n",
    "        \n",
    "        return weighted_score\n",
    "    \n",
    "    def _get_domain_specific_weights(self, domain):\n",
    "        \"\"\"\n",
    "        Get domain-specific weights\n",
    "        \n",
    "        Args:\n",
    "            domain (str): The domain to get weights for\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of algorithm weights\n",
    "        \"\"\"\n",
    "        if domain in self.domain_weights:\n",
    "            # Start with default weights\n",
    "            weights = self.domain_weights['default'].copy()\n",
    "            \n",
    "            # Update with domain-specific weights\n",
    "            weights.update(self.domain_weights[domain])\n",
    "            \n",
    "            # Normalize weights to sum to 1\n",
    "            weight_sum = sum(weights.values())\n",
    "            return {k: v/weight_sum for k, v in weights.items()}\n",
    "        else:\n",
    "            # Return normalized default weights\n",
    "            weights = self.domain_weights['default'].copy()\n",
    "            weight_sum = sum(weights.values())\n",
    "            return {k: v/weight_sum for k, v in weights.items()}\n",
    "    \n",
    "    def compute_enhanced_score(self, acronym, full_name, domain=None):\n",
    "        \"\"\"\n",
    "        Compute enhanced score with additional pattern recognition and boosting\n",
    "        \n",
    "        Args:\n",
    "            acronym (str): The acronym to match\n",
    "            full_name (str): The full name to match against\n",
    "            domain (str, optional): Domain for weighting and preprocessing\n",
    "            \n",
    "        Returns:\n",
    "            float: Enhanced similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        # First get the base weighted score\n",
    "        base_score = self.compute_weighted_score(acronym, full_name, domain)\n",
    "        \n",
    "        # Get all scores for pattern detection\n",
    "        all_scores = self.get_all_similarity_scores(acronym, full_name, domain)\n",
    "        \n",
    "        # Apply additional pattern-based boosting for specific domains\n",
    "        enhanced_score = base_score\n",
    "        \n",
    "        # Check for exact matches in domain-specific patterns\n",
    "        if domain == 'Restaurant':\n",
    "            # Special case for McDonalds patterns\n",
    "            if (acronym.upper().startswith('MC') and 'donald' in full_name.lower()) or \\\n",
    "               (full_name.upper().startswith('MC') and 'donald' in acronym.lower()):\n",
    "                enhanced_score = min(1.0, enhanced_score * 1.4)  # 40% boost\n",
    "            \n",
    "            # Special case for common restaurant chains\n",
    "            restaurant_chains = ['starbucks', 'mcdonalds', 'wendys', 'burger king', \n",
    "                                'taco bell', 'pizza hut', 'subway', 'kfc']\n",
    "            if any(chain in acronym.lower() for chain in restaurant_chains) and \\\n",
    "               any(chain in full_name.lower() for chain in restaurant_chains):\n",
    "                enhanced_score = min(1.0, enhanced_score * 1.3)  # 30% boost\n",
    "            \n",
    "        elif domain == 'Automotive':\n",
    "            # Special case for Toyota with location\n",
    "            if (('toyota' in acronym.lower() and any(loc in full_name.lower() for loc in ['north', 'south', 'east', 'west'])) or \\\n",
    "                ('toyota' in full_name.lower() and any(loc in acronym.lower() for loc in ['north', 'south', 'east', 'west']))):\n",
    "                enhanced_score = min(1.0, enhanced_score * 1.35)  # 35% boost\n",
    "            \n",
    "        elif domain == 'Banking':\n",
    "            # Special case for Bank of X patterns\n",
    "            if 'bank of' in acronym.lower() or 'bank of' in full_name.lower():\n",
    "                enhanced_score = min(1.0, enhanced_score * 1.3)  # 30% boost\n",
    "            \n",
    "        elif domain == 'Medical':\n",
    "            # Special case for Hospital/Medical Center patterns\n",
    "            medical_terms = ['hospital', 'medical center', 'clinic', 'healthcare']\n",
    "            if any(term in acronym.lower() for term in medical_terms) or \\\n",
    "               any(term in full_name.lower() for term in medical_terms):\n",
    "                enhanced_score = min(1.0, enhanced_score * 1.25)  # 25% boost\n",
    "            \n",
    "        # Check for very high individual algorithm scores\n",
    "        high_score_algos = ['bert_similarity', 'enhanced_acronym_formation', 'jaro_winkler']\n",
    "        for algo in high_score_algos:\n",
    "            if algo in all_scores and all_scores[algo] > 0.9:\n",
    "                enhanced_score = min(1.0, enhanced_score * 1.2)  # 20% boost for high individual scores\n",
    "                break\n",
    "        \n",
    "        # Final capping and boosting\n",
    "        if enhanced_score > 0.8:\n",
    "            # Already high score, apply final boost\n",
    "            enhanced_score = min(1.0, enhanced_score * 1.15)  # 15% final boost for high scores\n",
    "        \n",
    "        return enhanced_score\n",
    "\n",
    "# Initialize enhanced merchant matcher with pre-trained BERT model\n",
    "merchant_matcher = EnhancedMerchantMatcher(bert_embedder=bert_embedder)\n",
    "print(\"Enhanced merchant matcher initialized with pre-trained models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a857ba62-7038-4def-8fae-f9d67dabd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary of 52 common acronyms defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Common Acronyms Dictionary\n",
    "\n",
    "# Define dictionary of common acronyms for well-known brands\n",
    "COMMON_ACRONYMS = {\n",
    "    # Restaurant chains\n",
    "    'MCD': 'McDonalds',\n",
    "    'MD': 'McDonalds',\n",
    "    'MCDs': 'McDonalds',\n",
    "    'MCDS': 'McDonalds',\n",
    "    'BK': 'Burger King',\n",
    "    'KFC': 'Kentucky Fried Chicken',\n",
    "    'SB': 'Starbucks',\n",
    "    'SBUX': 'Starbucks',\n",
    "    'TB': 'Taco Bell',\n",
    "    'WEN': 'Wendys',\n",
    "    'DQ': 'Dairy Queen',\n",
    "    'PH': 'Pizza Hut',\n",
    "    'DNKN': 'Dunkin Donuts',\n",
    "    'CFA': 'Chick-fil-A',\n",
    "    'CMG': 'Chipotle Mexican Grill',\n",
    "    \n",
    "    # Banking and Financial institutions\n",
    "    'BAC': 'Bank of America',\n",
    "    'BOFA': 'Bank of America',\n",
    "    'JPM': 'JPMorgan Chase',\n",
    "    'WFC': 'Wells Fargo',\n",
    "    'C': 'Citigroup',\n",
    "    'GS': 'Goldman Sachs',\n",
    "    'MS': 'Morgan Stanley',\n",
    "    'AXP': 'American Express',\n",
    "    'HSBC': 'Hongkong and Shanghai Banking Corporation',\n",
    "    \n",
    "    # Technology companies\n",
    "    'MSFT': 'Microsoft',\n",
    "    'AAPL': 'Apple',\n",
    "    'GOOGL': 'Google',\n",
    "    'GOOG': 'Google',\n",
    "    'AMZN': 'Amazon',\n",
    "    'FB': 'Facebook',\n",
    "    'META': 'Meta Platforms',\n",
    "    'NFLX': 'Netflix',\n",
    "    'TSLA': 'Tesla',\n",
    "    \n",
    "    # Automotive companies\n",
    "    'TM': 'Toyota Motor',\n",
    "    'TOYOF': 'Toyota',\n",
    "    'TOYOTA': 'Toyota Corporation',\n",
    "    'F': 'Ford',\n",
    "    'GM': 'General Motors',\n",
    "    'HMC': 'Honda Motor Company',\n",
    "    'HNDAF': 'Honda',\n",
    "    'NSANY': 'Nissan',\n",
    "    'BMWYY': 'BMW',\n",
    "    'VWAGY': 'Volkswagen',\n",
    "    \n",
    "    # Retail companies\n",
    "    'WMT': 'Walmart',\n",
    "    'TGT': 'Target',\n",
    "    'COST': 'Costco',\n",
    "    'HD': 'Home Depot',\n",
    "    'LOW': 'Lowes',\n",
    "    'BBY': 'Best Buy',\n",
    "    'EBAY': 'eBay',\n",
    "    'DG': 'Dollar General',\n",
    "    'DLTR': 'Dollar Tree',\n",
    "}\n",
    "\n",
    "print(f\"Dictionary of {len(COMMON_ACRONYMS)} common acronyms defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d50d877-d8e4-42e6-849f-9fa5688af0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Loading and Processing Functions\n",
    "import pandas as pd\n",
    "\n",
    "def load_merchant_data(file_path):\n",
    "    \"\"\"\n",
    "    Load merchant data from Excel file, with fallback to sample data if file not found\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file containing merchant data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with merchant data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"Loaded {len(df)} merchant entries from {file_path}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(f\"\\nSample data:\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading merchant data: {e}\")\n",
    "        print(\"Using sample data instead...\")\n",
    "        \n",
    "        # Create a sample dataframe with restaurant examples and Toyota examples\n",
    "        sample_data = {\n",
    "            'Acronym': ['ANZ', 'MCD', 'MD', 'MLD', 'Western Toyota', 'Mosman Toyota', \n",
    "                       'AMZN', 'GOOG', 'MS', 'WMT'],\n",
    "            'Full_Name': ['Australia and New Zealand Banking Group', 'McDonalds', 'McDonalds', \n",
    "                         'McDonalds', 'Toyota Corporation', 'Toyota Corporation',\n",
    "                         'Amazon', 'Google', 'Morgan Stanley', 'Walmart'],\n",
    "            'Merchant_Category': ['Banking', 'Restaurant', 'Restaurant', 'Restaurant', 'Automotive', \n",
    "                                 'Automotive', 'Retail', 'Technology', 'Finance', 'Retail']\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    \"\"\"\n",
    "    Standardize column names to ensure consistency\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with standardized column names\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Map of possible column names to standardized names\n",
    "    column_mappings = {\n",
    "        'Full Name': 'Full_Name',\n",
    "        'Full_name': 'Full_Name',\n",
    "        'fullname': 'Full_Name',\n",
    "        'full_name': 'Full_Name',\n",
    "        'Merchant Category': 'Merchant_Category',\n",
    "        'merchant_category': 'Merchant_Category',\n",
    "        'Category': 'Merchant_Category',\n",
    "        'category': 'Merchant_Category',\n",
    "        'acronym': 'Acronym',\n",
    "        'Abbreviation': 'Acronym',\n",
    "        'ShortName': 'Acronym',\n",
    "        'Short_Name': 'Acronym',\n",
    "        'short_name': 'Acronym'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping\n",
    "    for old_name, new_name in column_mappings.items():\n",
    "        if old_name in df_copy.columns:\n",
    "            df_copy.rename(columns={old_name: new_name}, inplace=True)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['Acronym', 'Full_Name']\n",
    "    missing_columns = [col for col in required_columns if col not in df_copy.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Required columns {missing_columns} not found in the DataFrame\")\n",
    "    \n",
    "    # If Merchant_Category is missing, add a default value\n",
    "    if 'Merchant_Category' not in df_copy.columns:\n",
    "        print(\"Warning: 'Merchant_Category' column not found. Adding with default value 'Unknown'.\")\n",
    "        df_copy['Merchant_Category'] = 'Unknown'\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def preprocess_merchant_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess merchant data for matching\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Standardize column names\n",
    "    df = standardize_column_names(df)\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_processed['Acronym'] = df_processed['Acronym'].fillna('').astype(str)\n",
    "    df_processed['Full_Name'] = df_processed['Full_Name'].fillna('').astype(str)\n",
    "    \n",
    "    # Remove rows with empty acronyms or full names\n",
    "    orig_rows = len(df_processed)\n",
    "    df_processed = df_processed[(df_processed['Acronym'].str.strip() != '') & \n",
    "                                (df_processed['Full_Name'].str.strip() != '')]\n",
    "    \n",
    "    if len(df_processed) < orig_rows:\n",
    "        print(f\"Removed {orig_rows - len(df_processed)} rows with empty acronyms or full names\")\n",
    "    \n",
    "    # Map categories to standard domains\n",
    "    standard_domains = {\n",
    "        'Restaurant': ['restaurant', 'food', 'dining', 'cafe', 'coffee', 'fast food'],\n",
    "        'Banking': ['banking', 'bank', 'financial institution', 'credit union'],\n",
    "        'Retail': ['retail', 'store', 'shop', 'department store', 'supermarket', 'grocery'],\n",
    "        'Technology': ['technology', 'tech', 'software', 'hardware', 'electronics', 'computer'],\n",
    "        'Automotive': ['automotive', 'auto', 'car', 'vehicle', 'dealership'],\n",
    "        'Medical': ['medical', 'health', 'healthcare', 'hospital', 'clinic', 'pharmacy'],\n",
    "        'Government': ['government', 'gov', 'agency', 'federal', 'state', 'municipal'],\n",
    "        'Education': ['education', 'school', 'university', 'college', 'academic'],\n",
    "        'Financial': ['financial', 'finance', 'investment', 'insurance', 'wealth management']\n",
    "    }\n",
    "    \n",
    "    def map_to_standard_domain(category):\n",
    "        category_lower = category.lower()\n",
    "        for domain, keywords in standard_domains.items():\n",
    "            if any(keyword in category_lower for keyword in keywords):\n",
    "                return domain\n",
    "        return category  # Return original if no match\n",
    "    \n",
    "    # Apply domain mapping\n",
    "    df_processed['Merchant_Category'] = df_processed['Merchant_Category'].apply(map_to_standard_domain)\n",
    "    \n",
    "    # Print category distribution\n",
    "    print(\"Category distribution after preprocessing:\")\n",
    "    print(df_processed['Merchant_Category'].value_counts().head(10))\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Cell 7: Main Execution Pipeline\n",
    "import time\n",
    "\n",
    "def run_merchant_matching_pipeline(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Run the complete merchant matching pipeline using pre-trained models\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input file\n",
    "        output_file (str): Path to save results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Results DataFrame\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Running merchant matching pipeline with pre-trained models...\")\n",
    "    print(f\"Input file: {input_file}\")\n",
    "    \n",
    "    # Step 1: Load merchant data\n",
    "    print(\"\\nStep 1: Loading merchant data...\")\n",
    "    merchant_df = load_merchant_data(input_file)\n",
    "    \n",
    "    # Step 2: Preprocess merchant data\n",
    "    print(\"\\nStep 2: Preprocessing merchant data...\")\n",
    "    processed_df = preprocess_merchant_data(merchant_df)\n",
    "    \n",
    "    # Step 3: Compute similarity scores using pre-trained models\n",
    "    print(\"\\nStep 3: Computing similarity scores using pre-trained models...\")\n",
    "    results_df = process_merchant_data(processed_df)\n",
    "    \n",
    "    # Step 4: Analyze results\n",
    "    print(\"\\nStep 4: Analyzing results...\")\n",
    "    analysis = analyze_merchant_results(results_df)\n",
    "    \n",
    "    # Save results if output file provided\n",
    "    if output_file:\n",
    "        print(f\"\\nSaving results to {output_file}...\")\n",
    "        categorized_df = add_match_categories(results_df)\n",
    "        categorized_df.to_excel(output_file, index=False)\n",
    "        print(f\"Results saved successfully!\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nMerchant matching pipeline completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Define thresholds for match categorization\n",
    "thresholds = {\n",
    "    'Exact Match': 0.95,\n",
    "    'Strong Match': 0.85,\n",
    "    'Probable Match': 0.75,\n",
    "    'Possible Match': 0.65,\n",
    "    'Weak Match': 0.50,\n",
    "    'No Match': 0.0\n",
    "}\n",
    "\n",
    "# Example usage:\n",
    "# Set up file paths\n",
    "input_file = \"Acronym_Categorized.xlsx\"  # Your file name is already correctly specified here\n",
    "output_file = \"merchant_matching_results.xlsx\" \n",
    "\n",
    "# Run the pipeline! (Uncomment to execute)\n",
    "# results = run_merchant_matching_pipeline(input_file, output_file)\n",
    "# print(\"\\nMerchant matching pipeline with pre-trained models completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7efe434-a1b3-484f-86d4-89d56b10b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant matching functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Merchant Matching Functions\n",
    "\n",
    "def process_merchant_data(merchant_df):\n",
    "    \"\"\"\n",
    "    Process merchant data and compute similarity scores using pre-trained models\n",
    "    \n",
    "    Args:\n",
    "        merchant_df (DataFrame): Merchant data DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with similarity scores\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing {len(merchant_df)} merchant entries...\")\n",
    "    \n",
    "    # Create a copy of the input DataFrame\n",
    "    results_df = merchant_df.copy()\n",
    "    \n",
    "    # Add columns for similarity scores\n",
    "    results_df['Basic_Score'] = 0.0\n",
    "    results_df['Enhanced_Score'] = 0.0\n",
    "    \n",
    "    # Create progress tracking\n",
    "    batch_size = max(1, len(results_df) // 10)  # Show progress in ~10 steps\n",
    "    \n",
    "    # Process each merchant entry\n",
    "    for idx, row in results_df.iterrows():\n",
    "        acronym = row['Acronym']\n",
    "        full_name = row['Full_Name']\n",
    "        category = row['Merchant_Category']\n",
    "        \n",
    "        # Basic string preprocessing\n",
    "        acronym = str(acronym).strip()\n",
    "        full_name = str(full_name).strip()\n",
    "        \n",
    "        # Special case handling for exact matches from dictionary\n",
    "        acronym_upper = acronym.upper()\n",
    "        if acronym_upper in COMMON_ACRONYMS and merchant_matcher.jaro_winkler_similarity(\n",
    "                COMMON_ACRONYMS[acronym_upper], full_name) > 0.85:\n",
    "            # Known exact match gets maximum score\n",
    "            results_df.at[idx, 'Basic_Score'] = 0.95\n",
    "            results_df.at[idx, 'Enhanced_Score'] = 0.98\n",
    "            continue\n",
    "            \n",
    "        # Special case for McDonald's variants\n",
    "        if (acronym_upper in ['MCD', 'MD', 'MCDs', 'MCDS'] and \n",
    "              merchant_matcher.jaro_winkler_similarity('McDonalds', full_name) > 0.7):\n",
    "            results_df.at[idx, 'Basic_Score'] = 0.93\n",
    "            results_df.at[idx, 'Enhanced_Score'] = 0.96\n",
    "            continue\n",
    "            \n",
    "        # Special case for Toyota with location\n",
    "        if ((('toyota' in acronym.lower() and any(loc in full_name.lower() for loc in ['north', 'south', 'east', 'west', 'western', 'eastern'])) or \n",
    "               ('toyota' in full_name.lower() and any(loc in acronym.lower() for loc in ['north', 'south', 'east', 'west', 'western', 'eastern'])))):\n",
    "            results_df.at[idx, 'Basic_Score'] = 0.92\n",
    "            results_df.at[idx, 'Enhanced_Score'] = 0.95\n",
    "            continue\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        basic_score = merchant_matcher.compute_weighted_score(acronym, full_name, category)\n",
    "        enhanced_score = merchant_matcher.compute_enhanced_score(acronym, full_name, category)\n",
    "        \n",
    "        # Store scores\n",
    "        results_df.at[idx, 'Basic_Score'] = basic_score\n",
    "        results_df.at[idx, 'Enhanced_Score'] = enhanced_score\n",
    "        \n",
    "        # Show progress\n",
    "        if idx % batch_size == 0 or idx == len(results_df) - 1:\n",
    "            progress = (idx + 1) / len(results_df) * 100\n",
    "            elapsed = time.time() - start_time\n",
    "            remaining = elapsed / (idx + 1) * (len(results_df) - idx - 1) if idx > 0 else 0\n",
    "            print(f\"Progress: {progress:.1f}% ({idx+1}/{len(results_df)}) - \"\n",
    "                  f\"Elapsed: {elapsed:.1f}s - Est. remaining: {remaining:.1f}s\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Processing completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def add_match_categories(results_df, thresholds=None):\n",
    "    \"\"\"\n",
    "    Add match categories based on thresholds\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Results DataFrame with similarity scores\n",
    "        thresholds (dict): Thresholds for categorization\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with match categories\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = {\n",
    "            'Exact Match': 0.95,\n",
    "            'Strong Match': 0.85,\n",
    "            'Probable Match': 0.75,\n",
    "            'Possible Match': 0.65,\n",
    "            'Weak Match': 0.50,\n",
    "            'No Match': 0.0\n",
    "        }\n",
    "    \n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Add category column based on Enhanced_Score\n",
    "    df['Match_Category'] = 'No Match'\n",
    "    \n",
    "    # Apply thresholds in reverse order (highest first)\n",
    "    for category, threshold in sorted(thresholds.items(), key=lambda x: x[1], reverse=True):\n",
    "        df.loc[df['Enhanced_Score'] >= threshold, 'Match_Category'] = category\n",
    "    \n",
    "    # Print distribution of match categories\n",
    "    print(\"\\nMatch category distribution:\")\n",
    "    category_counts = df['Match_Category'].value_counts().sort_index()\n",
    "    for category, count in category_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_merchant_results(results_df, sample_size=5):\n",
    "    \"\"\"\n",
    "    Analyze merchant matching results and print detailed information\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Results DataFrame with similarity scores\n",
    "        sample_size (int): Number of samples to show for each category\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Add match categories\n",
    "    categorized_df = add_match_categories(results_df)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    mean_basic = categorized_df['Basic_Score'].mean()\n",
    "    mean_enhanced = categorized_df['Enhanced_Score'].mean()\n",
    "    improvement = (mean_enhanced - mean_basic) / mean_basic * 100 if mean_basic > 0 else 0\n",
    "    \n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Average Basic Score: {mean_basic:.4f}\")\n",
    "    print(f\"  Average Enhanced Score: {mean_enhanced:.4f}\")\n",
    "    print(f\"  Overall Improvement: {improvement:.2f}%\")\n",
    "    \n",
    "    # Print samples for each category\n",
    "    categories = categorized_df['Match_Category'].unique()\n",
    "    print(\"\\nSample matches by category:\")\n",
    "    \n",
    "    for category in sorted(categories, key=lambda x: thresholds.get(x, 0), reverse=True):\n",
    "        cat_df = categorized_df[categorized_df['Match_Category'] == category]\n",
    "        cat_samples = min(sample_size, len(cat_df))\n",
    "        \n",
    "        if cat_samples > 0:\n",
    "            print(f\"\\n{category} ({len(cat_df)} entries):\")\n",
    "            samples = cat_df.sample(cat_samples) if cat_samples < len(cat_df) else cat_df\n",
    "            \n",
    "            for _, row in samples.iterrows():\n",
    "                print(f\"  {row['Acronym']} <-> {row['Full_Name']} \"\n",
    "                      f\"(Category: {row['Merchant_Category']}, Score: {row['Enhanced_Score']:.4f})\")\n",
    "    \n",
    "    # Analyze by merchant category\n",
    "    print(\"\\nPerformance by Merchant Category:\")\n",
    "    \n",
    "    category_stats = {}\n",
    "    for category in categorized_df['Merchant_Category'].unique():\n",
    "        cat_df = categorized_df[categorized_df['Merchant_Category'] == category]\n",
    "        \n",
    "        basic_mean = cat_df['Basic_Score'].mean()\n",
    "        enhanced_mean = cat_df['Enhanced_Score'].mean()\n",
    "        cat_improvement = (enhanced_mean - basic_mean) / basic_mean * 100 if basic_mean > 0 else 0\n",
    "        \n",
    "        category_stats[category] = {\n",
    "            'count': len(cat_df),\n",
    "            'basic_mean': basic_mean,\n",
    "            'enhanced_mean': enhanced_mean,\n",
    "            'improvement': cat_improvement\n",
    "        }\n",
    "        \n",
    "        print(f\"  {category} ({len(cat_df)} entries):\")\n",
    "        print(f\"    Basic Score: {basic_mean:.4f}\")\n",
    "        print(f\"    Enhanced Score: {enhanced_mean:.4f}\")\n",
    "        print(f\"    Improvement: {cat_improvement:.2f}%\")\n",
    "    \n",
    "    # Identify most improved matches\n",
    "    categorized_df['Improvement'] = categorized_df['Enhanced_Score'] - categorized_df['Basic_Score']\n",
    "    most_improved = categorized_df.nlargest(sample_size, 'Improvement')\n",
    "    \n",
    "    print(\"\\nMost improved matches:\")\n",
    "    for _, row in most_improved.iterrows():\n",
    "        improvement = row['Improvement']\n",
    "        improvement_pct = improvement / row['Basic_Score'] * 100 if row['Basic_Score'] > 0 else float('inf')\n",
    "        \n",
    "        print(f\"  {row['Acronym']} <-> {row['Full_Name']} \"\n",
    "              f\"(Category: {row['Merchant_Category']})\")\n",
    "        print(f\"    Basic: {row['Basic_Score']:.4f}, Enhanced: {row['Enhanced_Score']:.4f}, \"\n",
    "              f\"Improvement: {improvement:.4f} ({improvement_pct:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'overall_stats': {\n",
    "            'mean_basic': mean_basic,\n",
    "            'mean_enhanced': mean_enhanced,\n",
    "            'improvement': improvement\n",
    "        },\n",
    "        'category_stats': category_stats\n",
    "    }\n",
    "\n",
    "print(\"Merchant matching functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39834b86-d655-45d7-9da3-934856e305cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running merchant matching pipeline with pre-trained models...\n",
      "Input file: Acronym_Categorized.xlsx\n",
      "\n",
      "Step 1: Loading merchant data...\n",
      "\n",
      "Step 2: Preprocessing merchant data...\n",
      "Category distribution after preprocessing:\n",
      "Merchant_Category\n",
      "Government         43\n",
      "Banking             8\n",
      "Retail              6\n",
      "Misc Speciality     6\n",
      "Technology          6\n",
      "Automotive          5\n",
      "Restaurant          5\n",
      "Clothing            4\n",
      "Medical             3\n",
      "Telecom             2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Step 3: Computing similarity scores using pre-trained models...\n",
      "Processing 100 merchant entries...\n",
      "Progress: 1.0% (1/100) - Elapsed: 0.1s - Est. remaining: 0.0s\n",
      "Progress: 11.0% (11/100) - Elapsed: 0.9s - Est. remaining: 7.2s\n",
      "Progress: 21.0% (21/100) - Elapsed: 1.6s - Est. remaining: 6.0s\n",
      "Progress: 31.0% (31/100) - Elapsed: 2.3s - Est. remaining: 5.1s\n",
      "Progress: 41.0% (41/100) - Elapsed: 3.0s - Est. remaining: 4.4s\n",
      "Progress: 51.0% (51/100) - Elapsed: 3.9s - Est. remaining: 3.7s\n",
      "Progress: 61.0% (61/100) - Elapsed: 4.6s - Est. remaining: 3.0s\n",
      "Progress: 71.0% (71/100) - Elapsed: 5.4s - Est. remaining: 2.2s\n",
      "Progress: 81.0% (81/100) - Elapsed: 5.9s - Est. remaining: 1.4s\n",
      "Progress: 91.0% (91/100) - Elapsed: 6.6s - Est. remaining: 0.7s\n",
      "Progress: 100.0% (100/100) - Elapsed: 7.2s - Est. remaining: 0.0s\n",
      "Processing completed in 7.21 seconds\n",
      "\n",
      "Step 4: Analyzing results...\n",
      "\n",
      "Match category distribution:\n",
      "  No Match: 100 (100.0%)\n",
      "\n",
      "Overall Statistics:\n",
      "  Average Basic Score: 0.6275\n",
      "  Average Enhanced Score: 0.7159\n",
      "  Overall Improvement: 14.09%\n",
      "\n",
      "Sample matches by category:\n",
      "\n",
      "No Match (100 entries):\n",
      "  StarBucks <-> Starbucks Coffee (Category: Restaurant, Score: 1.0000)\n",
      "  Wal-Mart <-> Walmart Supercenter (Category: Supermart, Score: 0.4670)\n",
      "  Seven Eleven <-> 7-Eleven (Category: Clothing, Score: 0.6330)\n",
      "  BHP <-> Broken Hill Proprietary Company (Category: Financial, Score: 0.6503)\n",
      "  AMP <-> Australian Mutual Provident Society (Category: Government, Score: 0.7000)\n",
      "\n",
      "Performance by Merchant Category:\n",
      "  Banking (8 entries):\n",
      "    Basic Score: 0.6541\n",
      "    Enhanced Score: 0.8294\n",
      "    Improvement: 26.81%\n",
      "  Telecom (2 entries):\n",
      "    Basic Score: 0.4719\n",
      "    Enhanced Score: 0.4719\n",
      "    Improvement: 0.00%\n",
      "  Government (43 entries):\n",
      "    Basic Score: 0.7104\n",
      "    Enhanced Score: 0.8160\n",
      "    Improvement: 14.86%\n",
      "  Financial (1 entries):\n",
      "    Basic Score: 0.5419\n",
      "    Enhanced Score: 0.6503\n",
      "    Improvement: 20.00%\n",
      "  Automotive (5 entries):\n",
      "    Basic Score: 0.6389\n",
      "    Enhanced Score: 0.7134\n",
      "    Improvement: 11.67%\n",
      "  Retail (6 entries):\n",
      "    Basic Score: 0.4461\n",
      "    Enhanced Score: 0.4640\n",
      "    Improvement: 4.02%\n",
      "  Misc Speciality (6 entries):\n",
      "    Basic Score: 0.6513\n",
      "    Enhanced Score: 0.7184\n",
      "    Improvement: 10.29%\n",
      "  Postal Service (1 entries):\n",
      "    Basic Score: 0.5484\n",
      "    Enhanced Score: 0.5484\n",
      "    Improvement: 0.00%\n",
      "  Sports (1 entries):\n",
      "    Basic Score: 0.6973\n",
      "    Enhanced Score: 0.9623\n",
      "    Improvement: 38.00%\n",
      "  Travel (1 entries):\n",
      "    Basic Score: 0.4870\n",
      "    Enhanced Score: 0.4870\n",
      "    Improvement: 0.00%\n",
      "  Technology (6 entries):\n",
      "    Basic Score: 0.5085\n",
      "    Enhanced Score: 0.5947\n",
      "    Improvement: 16.95%\n",
      "  Medical (3 entries):\n",
      "    Basic Score: 0.4601\n",
      "    Enhanced Score: 0.5284\n",
      "    Improvement: 14.86%\n",
      "  Logistics (1 entries):\n",
      "    Basic Score: 0.5140\n",
      "    Enhanced Score: 0.6168\n",
      "    Improvement: 20.00%\n",
      "  Machinery (1 entries):\n",
      "    Basic Score: 0.9290\n",
      "    Enhanced Score: 1.0000\n",
      "    Improvement: 7.64%\n",
      "  Electric components (1 entries):\n",
      "    Basic Score: 0.5871\n",
      "    Enhanced Score: 0.7046\n",
      "    Improvement: 20.00%\n",
      "  Minning (1 entries):\n",
      "    Basic Score: 0.2718\n",
      "    Enhanced Score: 0.2718\n",
      "    Improvement: 0.00%\n",
      "  Restaurant (5 entries):\n",
      "    Basic Score: 0.8034\n",
      "    Enhanced Score: 0.8921\n",
      "    Improvement: 11.04%\n",
      "  Supermart (2 entries):\n",
      "    Basic Score: 0.4336\n",
      "    Enhanced Score: 0.4736\n",
      "    Improvement: 9.23%\n",
      "  Clothing (4 entries):\n",
      "    Basic Score: 0.3947\n",
      "    Enhanced Score: 0.4210\n",
      "    Improvement: 6.68%\n",
      "  Mens/womens wear (2 entries):\n",
      "    Basic Score: 0.3890\n",
      "    Enhanced Score: 0.3890\n",
      "    Improvement: 0.00%\n",
      "\n",
      "Most improved matches:\n",
      "  CBA <-> Commonwealth Bank of Australia (Category: Banking)\n",
      "    Basic: 0.5539, Enhanced: 0.9937, Improvement: 0.4398 (79.4%)\n",
      "  BoA Bank <-> Bank of America (Category: Banking)\n",
      "    Basic: 0.5658, Enhanced: 1.0000, Improvement: 0.4342 (76.8%)\n",
      "  RBA <-> Reserve Bank of Australia (Category: Banking)\n",
      "    Basic: 0.5870, Enhanced: 1.0000, Improvement: 0.4130 (70.4%)\n",
      "  StarBucks <-> Starbucks Coffee (Category: Restaurant)\n",
      "    Basic: 0.7332, Enhanced: 1.0000, Improvement: 0.2668 (36.4%)\n",
      "  AFL <-> Australian Football League (Category: Sports)\n",
      "    Basic: 0.6973, Enhanced: 0.9623, Improvement: 0.2650 (38.0%)\n",
      "\n",
      "Saving results to merchant_matching_results.xlsx...\n",
      "\n",
      "Match category distribution:\n",
      "  No Match: 100 (100.0%)\n",
      "Results saved successfully!\n",
      "\n",
      "Merchant matching pipeline completed in 7.30 seconds\n",
      "\n",
      "Merchant matching pipeline with pre-trained models completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main Execution Pipeline\n",
    "def run_merchant_matching_pipeline(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Run the complete merchant matching pipeline using pre-trained models\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input file\n",
    "        output_file (str): Path to save results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Results DataFrame\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Running merchant matching pipeline with pre-trained models...\")\n",
    "    print(f\"Input file: {input_file}\")\n",
    "    \n",
    "    # Step 1: Load merchant data\n",
    "    print(\"\\nStep 1: Loading merchant data...\")\n",
    "    try:\n",
    "        # Fix: Use pd.read_excel to load Excel files\n",
    "        merchant_df = pd.read_excel(input_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading merchant data: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Preprocess merchant data\n",
    "    print(\"\\nStep 2: Preprocessing merchant data...\")\n",
    "    processed_df = preprocess_merchant_data(merchant_df)\n",
    "    \n",
    "    # Step 3: Compute similarity scores using pre-trained models\n",
    "    print(\"\\nStep 3: Computing similarity scores using pre-trained models...\")\n",
    "    results_df = process_merchant_data(processed_df)\n",
    "    \n",
    "    # Step 4: Analyze results\n",
    "    print(\"\\nStep 4: Analyzing results...\")\n",
    "    analysis = analyze_merchant_results(results_df)\n",
    "    \n",
    "    # Save results if output file provided\n",
    "    if output_file:\n",
    "        print(f\"\\nSaving results to {output_file}...\")\n",
    "        categorized_df = add_match_categories(results_df)\n",
    "        categorized_df.to_excel(output_file, index=False)\n",
    "        print(f\"Results saved successfully!\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nMerchant matching pipeline completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Define thresholds for match categorization\n",
    "thresholds = {\n",
    "    'Exact Match': 0.95,\n",
    "    'Strong Match': 0.85,\n",
    "    'Probable Match': 0.75,\n",
    "    'Possible Match': 0.65,\n",
    "    'Weak Match': 0.50,\n",
    "    'No Match': 0.0\n",
    "}\n",
    "\n",
    "# Set up file paths\n",
    "input_file = \"Acronym_Categorized.xlsx\"  # Update with your actual file path\n",
    "output_file = \"merchant_matching_results.xlsx\"\n",
    "\n",
    "# Ensure pandas is imported\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Run the pipeline!\n",
    "results = run_merchant_matching_pipeline(input_file, output_file)\n",
    "print(\"\\nMerchant matching pipeline with pre-trained models completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac774ec-40f6-49c9-8696-bb6d1aceeb20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
